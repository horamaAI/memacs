# -*- mode: org -*-
#+title: learning open-data and openAPI
#+SETUPFILE: ~/set-up-files/basic-setups.org


* open data
* APIs and open APIs
** APIs
[2025-05-04 Sun 19:49]
*** APIs security
- general:
  + API token and API keys:
    - API tokens :: an access token, device-specific (for security, eg: API tokens for smartphones won't work for laptops
      or vice versa), and is split into 3 components:
      + the payload :: the unique passkey that an API uses, each API resource demands it
      + the header :: provides token format-related information to APIs and other
	information that assist APIs into knowing what they can expect out of the token. It holds details related to
        user session expiration and permission, and makes out most of the part of an API token, hence is often referred as the "API body"
      + the signature :: included in the payload, and is required by API resources. If missing or incorrect, the caller
        won't be able to access the API's resources
      + modus operandi of API tokens ::
	- using the payload linked to an API token in a hashed format, the username and password information of the
          concerned user are verified, after successful verification, API forwards an asset to end-users browser and
          stored somewhere safe
	- for every user-side query request that API receives, an access token comes with it. The job of the access
          token is to make sure API is accessible for the respective user till the time token is usable
	- depending on the API token authentication process, the process can also use SSO (Single-Sign-on) token, eg:
          using Facebook login details for 3rd party services, such tokens remain active for a limited time and prevent
          creating different login details for different services
    - similarities:
      + both resources aim at improved API security
      + both are unique and string-based texts, though, API keys are less informative (less granularity) as API token
    - differences:
      + every concerned entities such as servers have distinct API keys, shared for every acceptable API request: just
        like every time a user has to provide login passwords and email to access a service, an API key has to be
        provided to access an API
      + API keys are to verify the application, while API tokens are used for user verification
      + API key use limited information, since application verification isn't that extensive a job, but user
        verification is extensive, and will ask for more details, hence, the use of API tokens which store more
        data. Though this can be an issue for API tokens, due to more information verification that ought to be precise
        and complex to handle, for example when used in IoT applications
      + standardization: API tokens outperform API keys standardization-wise, and where standards are introduced as per
        requirement of API applications, API tokens are always of the same kind, regardless of the type of application,
        hence its wider adoption
      + API tokens are more granular, and thus offer better security. If compromised, it won't cause much harm to the
        application, as the API token is revoked immediately when an error or any malicious activity is spotted
      + due to its lesser performance, API key usage is on the decline
    - API Tokens and 0Auth 2.0 ::
      + OAuth :: open-standard authorization protocol allowing users to grant applications access to their information
        on other websites without sharing their passwords
      + OAuth 2.0 promises better API security
      + when external server used to store sensitive user credentials and credit card details for example, OAuth 2.0
        implementation makes API developers and end-users trustworthy for each other. API tokens are part of OAuth 2.0
        as well and are used on the user side. By granting future API users a better hold over user information, API
        tokens prevent repetitive information logging, erroneous entry, and data exploitation
      + SSL is a place in OAuth 2.0 and is responsible for user-side data and privacy protection. In SSL, API tokens
        permit access and deliver instant actions in case of any wrong actions
    - best practices ::
      + prefer OAuth 2.0 API tokens when API tokens are used for a user-side application. It's easy to handle and makes
        continual communication with the resource server involved
      + for APIs to offer a product or service for 3rd party applications, simple API tokens are preferred, same
        for any automation dependent applications
      + the best API token usage approach is to keep all crucial authentication related information in an [[https://swagger.io/docs/specification/v3_0/authentication/bearer-authentication/][Authorization:
        Bearer]], and make sure that the JSON file is used. Also, replace the string based authentication with JWT format
        as it's highly optimized and is compatible with most programming languages
      + better to adopt the best API token authentication practices to safeguard the data they carry, for instance:
        using a strong algorithm for API tokens, verify received requests, and blacklist IP addresses involved in
        sending malicious requests
      + use MFA (multi-factor) for authenticating API tokens, the approach combines two more authentication resources to
        avoid information compromise
  + 
** open APIs
*** General
[2025-06-04 Wed 23:53]
- how open API is amazing ::
*** open APIs standard
* openStack
[2025-04-27 Sun 16:06]
** general
- notes ::
  + OpenStack is a set of software components that provide common services for cloud infrastructure, and has proven as
    production at scale
    - eg: grafana and openstack:
      https://grafana.com/docs/grafana-cloud/monitor-infrastructure/integrations/integration-reference/integration-openstack/
* open containers
[2024-10-11 Fri 23:11]
* json and yaml
[2024-07-14 Sun 20:12]
** introduction: yaml/json format and structure
[2024-08-01 Thu 21:17]
- yaml ::
  + format ::
    - yaml file has start (~---~) and end (~...~) line indicators (just convention)
    - comments :: start with hash symbol (#)
  + structure ::
    - structure of a YAML file is a map or a list, it follows a hierarchy depending on the indentation
    - mapping blocks ::
      + maps allow key-value pairs association
      + yaml maps need to be resolved before it can be closed, and a new map created
        #+begin_example
        # KO!
        akey: some value
          a_wrong_new_map: will generate an error
        # OK!
        akey: # no value to allow creation of a new embedded map
          a_correct_new_map: this one will not generate an error
        #+end_example
      + avoid flat mapping when dealing with lists:
        - eg:
          #+begin_example
          Store: Bakery
            - Sourdough loaf
            - Bagels
          #+end_example
        - yaml is valid, but flat and somehow ambiguous since json counterpart is =={“Store”: “Bakery - Sourdough loaf -
          Bagels”}==
        - furthermore, ==store== mapping block is never closed
        - better version:
          #+begin_example
          Store:
            Bakery
              - Sourdough loaf
              - Bagels
          #+end_example      
    - each key must be unique, order doesn't matter
    - a new map can be created by either increasing the indentation or by resolving previous maps and starting an
      adjacent map
    - lists ::
      + lists include values listed in a specific order
      + a list sequence starts with a dash (-) and a space, while indentation separates it from the parent
      + embedded lists: (json: ~[“Flour”, “Salt”, “Water”, {“Sugar”: [“caster”, “granulated”, “icing”]}]~)
        #+begin_example
        ---
        - Flour
        - Water
        - Salt
        - Sugar:
            - caster
            - granulated
            - icing
        ...
        #+end_example
** jq and yq (json and yaml files parser, processor, and basic format validation)
- json and yaml files parsing and processing ::
  + yaml and json processing with jq ::
    - at time of writing this notes, exists 2 versions of yq: in go, and in python
    - yq ::
      + yq: Command-line YAML processor, a jq wrapper for YAML documents (converts yaml to json)
      + options ::
        - preserve yaml format in output :: option: ==--yaml-output/-y==, to convert json output back to YAML,
	  eg: ~cat input.yml | yq -y .foo.bar~ (by defaults jq output is in json format)
        - insert changes in input :: -i (only available in ==yq== mode. To insert with jq use [[https://www.baeldung.com/linux/jq-add-objects-json-array][–argjson for example]])
    - jq ::
      + options ::
        - translate multiple JSONs/YAMLs into a single array with slurp :: ==-s/--slurp==
    - processing YAMLs and JSONs with jq and yq ::
      + prettify :: ~echo '{"fruit":{"name":"apple","color":"green","price":1.20}}' | jq '.'~
      + used examples :: apt.yaml, fruits.json
        - apt.yaml:
          #+begin_example
          ---
          note: install with debian like apt tool
          apt:
            command: sudo apt install
            pkgs:
              - git
              - tree
              - curl  # likely already installed, but do it anyway
              - screen  # GNU screen, multiplexer
              - g++
              - python
              - yamllint  # yaml file linter
              - ccze  # log files coloriser
              - wget
          dpkg:
            command: ""
            pkgs:
              - ""
          todo:
            - java jdk (basic one: default-jre), valgrind
          ...
          #+end_example
        - fruits.json (see next)
      + json to yaml :: (fruits.json)
        #+begin_example
        $cat << EOF > fruits.json | yq -y
        [
          {
            "name": "apple",
            "color": "green",
            "price": 1.2
          },
          {
            "name": "banana",
            "color": "yellow",
            "price": 0.5
          },
          {
            "name": "kiwi",
            "color": "green",
            "price": 1.25
          }
        ]
        EOF
        - name: apple
          color: green
          price: 1.2
        - name: banana
          color: yellow
          price: 0.5
        - name: kiwi
          color: green
          price: 1.25
        #+end_example
      + accessing properties ::
        - access property values with ==.field== operator: filter operator + property name
        - brackets (==[]==) provide all values from a structure/array
        - ~$jq '.[] | .name' fruits.json~: ~"apple" "banana" "kiwi"~
          + brackets iterate over content of list, pass each object to next filter using a pipe (==|==), and then
            outputs 'name' field content
          + same outcome with: ~$ jq '.[].name' fruits.json~
        - ~$cat configs/basic/apt.yaml | yq '.apt.pkgs[]'~ (or ~yq '.apt.pkgs[]' configs/basic/apt.yaml~)
          + results:
            #+begin_example
            "git"
            "tree"
            "curl"
            "screen"
            "g++"
            "python"
            "yamllint"
            "ccze"
            "wget"          
            #+end_example
        - ~$yq '.apt.command' configs/basic/apt.yaml~ :: ~"sudo apt install"~
        - fetch multiple keys :: use comma as separator: ~yq '.apt.pkgs,.apt' configs/basic/apt.yaml~
          #+begin_example
          [
            "git",
            "tree",
            "curl",
            "screen",
            "g++",
            "python",
            "yamllint",
            "ccze",
            "wget"
          ]
          {
            "command": "sudo apt install",
            "pkgs": [
              "git",
              "tree",
              "acurl",
              "screen",
              "g++",
              "python",
              "yamllint",
              "ccze",
              "wget"
            ]
          }
          #+end_example
        - access by index :: ~$ jq '.[1].name' fruits.json~: "banana"
        - slicing (subarray) ::
          + ~echo '[1,2,3,4,5,6,7,8,9,10]' | jq '.[6:9]'~ : ~[7,8,9]~
          + ~echo '[1,2,3,4,5,6,7,8,9,10]' | jq '.[:6]' | jq '.[-2:]'~ : ~[5,6]~
        - ~$yq '.apt.pkgs[] | select(. == "screen")' configs/basic/apt.yaml~ :: ~screen~
      + merging multiple independent JSON/YAML objects or files with slurp ::
        - eg: ~yq -sy '.' apt.yaml fruits.json~
        - works with mix of json and yaml too, cool!
      + built-in functions ::
        - keys (and 'value') :: ~jq '.[] | keys' fruits.json~ : ~["color", "name", "price"]~ 3 times
        - length ::
          + ~jq '.[].name | length' fruits.json~ : ~5,6,4~, the length of 'name' contents
          + ~jq '.[] | length' fruits.json~ : ~3,3,3~, the number of properties in each object
        - map ::
          + handy when filtering needed to then apply further operations on selected values
          + ~jq 'map(.price+2)' fruits.json~: increase content of property 'price' by 2
        - has ::
          + allows recursive evaluation
          + has and map :: ~jq 'map(has("name"))' fruits.json~: boolean evaluation of objects in list that have property
            'name' (returns 3 times 'true')
            - to get the actual result, use select: ~jq 'select(map(has("name")))' fruits.json~
            - can't however just select without mapping (~jq 'select(has("name"))' fruits.json~), since ~jq
              'has("name")' fruits.json~ alone is evaluating the array itself, not its content
              + => need to pass the content of the array to select: ~jq '.[] | select(has("name"))' fruits.json~
            - so, an even better version to: ~jq 'select(map(has("name")))' fruits.json~,
	      is: ~jq '.[] | select(has("name"))' fruits.json~
          + chaining has and select :: ~jq 'select(map(has("name"))) | .[] | select(.color == "green")' fruits.json~
        - min/max ::
          + ~jq '[.[].price] | max' fruits.json~ : returns the price of the most expensive fruit
        - min_by/max_by ::
          + returns *object* that matches the given min/max condition
          + ~jq 'max_by(.price)' fruits.json~: ~{"name": "kiwi", "color": "green", "price": 1.25}~
        - to_entries[] :: convert each object in the array into an array of key-value pairs
        - select ::
          + for searching for values
          + ~jq '.[] | select(.color=="yellow" and .price>=0.5)' fruits.json~
          + ~jq '.[] | to_entries[] | select(.key | startswith("name")) | .value' fruits.json~ : ~"apple" "banana" "kiwi"~
        - unique ::
          + find unique occurrences of a value, or remove duplicates
          + ~jq 'map(.color) | unique' fruits.json~
        - del ::
          + remove keys/values from the outputs
          + ~jq 'del(.[].name)' fruit.json~
        - recursive operations with '..', 'recurse' and 'walk' ::
          + '..' ("recursive descent") ::
            - for key lookup, same as zero-argument ==recurse== function (see next)
            - ~..a~ does not work; use ~.. | .a~
            - safer to use ~.. | .a?~ to find values of keys in any object, in case for example the function iterates
              over values objects such as strings
            - eg: ~yq '.. | select(.name?) ' fruits.json~, to select any object that has key 'name' (~select(.name)~
              will break when jq tries to apply '.name' to string "apple" for example)
              + indexed element since 
          + 'recurse' ::
            - fitting recursive structures
            - eg:
              + given a json file with recursive elements to search through:
                #+begin_example
                {"name": "/", "children": [
                  {"name": "/bin", "children": [
                    {"name": "/bin/ls", "children": []},
                    {"name": "/bin/sh", "children": []}]},
                  {"name": "/home", "children": [
                    {"name": "/home/stephen", "children": [
                      {"name": "/home/stephen/jq", "children": []}]}]}]}
                #+end_example
              + to extract all filenames, one needs to retrieve .name, .children[].name, .children[].children[].name,
                and so on
              + or can just use recurse instead: ~recurse(.children[]) | .name~
            - can even do cool operations: ~echo 2 | jq 'recurse(. * .; . < 20)'~ returns: ~2 4 16~
          + walk ::
            - to apply operations recursively
            - not 'recurse' that only go through the json/yaml elements
            - eg:
              + ~echo "[[4, 1, 7], [8, 5, 2], [3, 6, 9]]" | jq 'walk(if type == "array" then sort else . end)'~ ,
		returns : ~[[1,4,7],[2,5,8],[3,6,9]]~
              + ~echo "[ { "_a": { "__b": 2 } } ]" | jq 'walk( if type == "object" then with_entries( .key |= sub( "^_+"; "") ) else . end )'~ ,
	      returns: ~[{"a":{"b":2}}]~ (removes underscores)
        - support for regex (with 'test' function) ::
          + tests if an input matches against a given regular expression
          + ~jq '.[] | select(.name|test("^a.")) | .price' fruits.json~
      + working with jq nodes ::
        - use nodes to query and modify yaml/json structures: add/delete/find nodes
        - create/add node ::
          + add new object :: ~yq '. + [{"name": "avocado", "color": "green", "price": "3.5"}]' fruits.json~
            - adds a new object "avocado" to the result
            - the command only outputs the result, if need to update the yaml file, remember to use the ==-iy== option
          + add new property :: ~yq -y '. | .[] | select(.name=="banana") |= . + {"tastes": "amazing"}'  fruits.json | yq -sy~
            - adds a new field/property to the fruit object banana
            - option ==-s== (slurp) in the second part so that the result (a list of independent fruit objects) is translated into an array
            - option ==-y== on both commands to work in full yaml format
        - delete node :: with del function: ~yq 'del(. | .[] | select(.name=="banana"))' fruits.json~
        - retrieving nodes entries :: with function ~to_entries~: ~yq '.[1] | to_entries' fruits.json~ ,
	  returns: ~[{"key": "name","value": "banana"},{"key": "color","value": "yellow"},{"key": "price","value": 0.5}]~
          + for arrays the keys are the indexes: ~yq 'to_entries' fruits.json~ ,
	    returns ~[{"key": 0,"value": {"name": "apple","color": "green","price": 1.2}},...]~
          + pipe returned array to .key/.value filters to retrieve entries' elements ::
	    ~yq '.[1] | to_entries | .[] | .values' fruits.json~,returns: ~banana yellow 0.5~ 
      + data transformation with jq ::
        - pages.json ::
          #+begin_example
          {
            "query": {
              "pages": [
                {
                  "21721040": {
                    "pageid": 21721040,
                    "ns": 0,
                    "title": "Stack Overflow",
                    "extract": "Some interesting text about Stack Overflow"
                  }
                },
                {
                  "21721041": {
                    "pageid": 21721041,
                    "ns": 0,
                    "title": "Baeldung",
                    "extract": "A great place to learn about Java"
                  }
                }
              ]
            }
          }
          #+end_example
        - command :: ~jq '.query.pages | [.[] | map(.) | .[] | {page_title: .title, page_description: .extract}]' pages.json~
          + ~.[] | map(.) | .[]~:
            1. take all content of .query.pages
            2. map it <=> turn pages objects into arrays of their content: in the original structure, the pages are
               objects identified by their ISBN, getting their content leaves out the identifier/key and keeps only the
               content
            3. take content => 1-3: transform original pages with ISBNs as keys into just their content
        - results ::
          #+begin_example
          [
            {
              "page_title": "Stack Overflow",
              "page_description": "Some interesting text about Stack Overflow"
            },
            {
              "page_title": "Baeldung",
              "page_description": "A great place to learn about Java"
            }
          ]
          #+end_example
    - going further ::
      + cookbook :: https://github.com/jqlang/jq/wiki/Cookbook
      + official documentation :: https://jqlang.github.io/jq/manual/
      + cheatsheet :: https://gist.github.com/olih/f7437fb6962fb3ee9fe95bda8d2c8fa4
  + jq/yq and bash scripting ::
    - from bash :: bash -> yq/jq
      + using bash variables ::
        - straightforward way (no option to jq) :: ~ax=xoxo && echo "{'a':'$ax'}" | yq .~
        - with options :: 
          + --args option ::
            - format :: ~jq --arg jq_var ${bash_var} [options...] filter [files ...]~
            - eg ::
              + ~bash_fruit_var=Banana && jq --arg jq_fruit_var $bash_fruit_var '.[] | select(.name == "apple") | .name |= $jq_fruit_var' fruits.json~
              + ~bash_fruit_var=Banana && jq --arg jq_fruit_var $bash_fruit_var '.[] | select(.name == $jq_fruit_var)' fruits.json~
          + other options :: ==$ENV, env, --args, --argjson, --slurpfile, etc.== (checkout [[https://www.baeldung.com/linux/jq-passing-bash-variables][jq and bash with baeldung]])
        - as jq filter field :: (with .[$var] instead of just .$var)
          + ~var=name && jq --arg jq_var $var '.[] | select(.[$jq_var] == "banana")' fruits.json~
        - with variables within yaml/json file ::
          + input :: ~$cat input.yaml~
            #+begin_example
            hostname: ${HOSTNAME}
            user: ${USER}
            shell: ${SHELL}
            #+end_example
          + several possible way ::
            - with bash builtin 'envsubst' ::
              + key-value pair :: ~envsubst < input.yaml~
              + values only :: ~alo=$(yq '.[]' input.yaml | envsubst)~
    - from script :: yq/jq -> bash script
      + build an associative array from a yaml/json :: key: property name, value: well... its corresponding value :-P
        - example ::
          #+begin_example
          declare -A content
          
          while IFS="=" read -r key value;
          do
            echo "process $key - $value"
            content["$key"]=$value;
          done < <(yq '.[] | to_entries | map([.key, .value] | join("=")) | .[]' fruits.yaml)
           
          for key in "${!content[@]}";
          do
            printf "key %s, value %s\n" "$key" "${content[$key]}";
          done
          #+end_example
        - cons :: since key-value pair, will overwrite/replace previous values => will keep the last occurrence
      + using a simple array of size n*2 :: where each line contains 2 elements: the key and its value
        - example ::
          #+begin_example
          declare -a content
          
          while IFS="=" read -r key value;
          do
            # echo "process $key - $value"
            content+=($key $value);
          done < <(yq '.[] | to_entries | map([.key, .value] | join("=")) | .[]' fruits.yaml)

          echo "${content[*]}"
          #+end_example
** data models (mainly yaml schemas, but most properties are json valid as well)
[2024-08-08 Thu 22:09]
- primitive types :: string (for dates and files too), number, integer, boolean, array, object, and ==nullable==
  attribute for potential null values
- numbers/integer ::
  + ranges with ==minimum== and ==maximum== ::
    - ~minimum ≤ value ≤ maximum~, use ~exclusiveMinimum: true~ and ~exclusiveMaximum: true~ to exclude boundaries
    #+begin_example
    # same level, when defining a number/integer with ranges: floating-point between 0 and 50, 0 excluded
    type: number
    minimum: 0
    exclusiveMinimum: true
    maximum: 50
    #+end_example
  + ==multipleOf== :: to define multiple of another number (may be used with floating-point, but can be unreliable due
    to limited precision or floating point math)
    #+begin_example
    type: integer
    multipleOf: 10
    #+end_example
- strings ::
  + ==minLength== and ==maxLength== :: to set range for string's size
    #+begin_example
    type: string
    minLength: 3
    maxLength: 20
    #+end_example
  + ==pattern== :: to define string pattern schema with regex (enclosed in ==^…$==, ==^== for string beginning,
    and ==$== for end)
    #+begin_example
    # social security format
    ssn:
      type: string
      pattern: '^\d{3}-\d{2}-\d{4}$'
    #+end_example
- array ::
  + ==items== :: always required in arrays (unlike with json)
  + ==minItems== and ==maxItems== :: to set range for length of array
  + ==uniqueItems== :: well, to define unique items :-P
    #+begin_example
    type: array
    items:
      type: integer
    uniqueItems: true
    # [1, 2, 3] – valid
    # [1, 1, 3] – not valid
    # [ ] – valid
    #+end_example
  + 2D arrays ::
    #+begin_example
    # [ [1, 2], [3, 4] ]
    type: array
    items:
      type: array
      items:
        type: integer
    #+end_example
  + array of objects ::
    - using items inline schema ::
      #+begin_example
      # [ {"id": 5}, {"id": 8} ]
      type: array
      items:
        type: object
        properties:
          id:
            type: integer
      #+end_example
    - with $ref for items ::
      #+begin_example
      # Array of Pets
      type: array
      items:
        $ref: '#/components/schemas/Pet'
      #+end_example
  + mixed type array ::
    - inline item schema ::
      #+begin_example
      # ["foo", 5, -2, "bar"]
      type: array
      items:
        oneOf:
          - type: string
          - type: integer
      #+end_example
    - reference ::
      #+begin_example
      # Array of Cats and Dogs
      type: array
      items:
        oneOf:
          - $ref: '#/components/schemas/Cat'
          - $ref: '#/components/schemas/Dog'
      #+end_example
  + arbitrary types array :: (=={}== for any-type)
    #+begin_example
    type: array
    items: {}
    # [ "hello", -2, true, [5.7], {"id": 5} ]
    #+end_example
- objects ::
  + objects are collections of property-value
  + ==properties==: used to define object properties
  + ==required==: same as json's ==required==, used to define required properties
    - to remember :: ==required== is an object-level attribute, not a property attribute
    #+begin_example
    type: object
    properties:
      id:
        type: integer
      username:
        type: string
      name:
        type: string
    required:
      - id
      - username
    #+end_example
  + nested objects ::
    - in OpenAPI, objects are usually defined in the global ==components/schemas== section rather than inline in the
      request and response, so, since yaml data models are so powerful, why not adopt the same modelling structure. Who
      knows how much a simple model can be extended in the future
    - ==$ref== can also be used with nested objects
    - example:
      #+begin_example
      components:
        schemas:
          User:
            type: object
            properties:
              id:
                type: integer
              name:
                type: string
              contact_info:
                $ref: '#/components/schemas/ContactInfo'
          ContactInfo:
            type: object
            properties:
              email:
                type: string
                format: email
              phone:
                type: string
      #+end_example
  + free-form objects :: (for arbitrary property/value pairs) use of ==additionalProperties== in the background
    - many forms ::
      #+begin_example
      # either:
      type: object
      # or:
      type: object
      additionalProperties: true
      # or:
      type: object
      additionalProperties: {}
      #+end_example
- ==nullable== ::
  #+begin_example
  id:
    type: number
    nullable: false
  #+end_example
- ==properties== ::
  + ==readOnly==, ==writeOnly==: does exactly what it says to the defined property
    - usecase :: when defining an openAPI with yaml, and wants to define that GET returns more properties than used in POST:
      + the same schema can be used for both GET and POST (or PUT, PATCH)
      + keep unmarked the properties that are common to both GET and POST
      + mark properties that are only returned for a GET as ==readOnly== (with the openAPI generator, ==readOnly==
        properties are included in responses but not in requests)
      + mark properties specific to POST with ==writeOnly== (==writeOnly== properties may be sent in requests but not in
        responses)
      + data model:
        #+begin_example
        type: object
        properties:
          id:
            # Returned by GET, not used in POST/PUT/PATCH
            type: integer
            readOnly: true
          username:
            type: string
          password:
            # Used in POST/PUT/PATCH, not returned by GET
            type: string
            writeOnly: true
        #+end_example
    - ==required== works as expected with ==readOnly== and ==writeOnly==, it affects just the relevant scope: read-only
      required properties apply to responses only, and write-only to requests only
  + ==minProperties== and ==maxProperties== :: to set number of properties
    - handy when used with free-form objects or ==additionalProperties==
    - eg:
      #+begin_example
      # ~{"id": 5, "username": "trillian"}~ matches the schema, ~{"id": 5}~ does not
      type: object
      minProperties: 2
      maxProperties: 10
      #+end_example
- ==oneOf== or ==anyOf== :: to mix values
  #+begin_example
  oneOf:
    - type: string
    - type: integer
  #+end_example
- ==AnyValue== :: (=={}==) to define any type: object, string, number, etc.
  + schema without type matches any data type: numbers, strings, objects, and so on
  + =={}== is shorthand syntax for an arbitrary-type schema
  + eg:
    #+begin_example
    components:
      schemas:
        AnyValue: {}
    #+end_example
  + with description:
    #+begin_example
    components:
      schemas:
        AnyValue:
          description: Can be any value - string, number, boolean, array or object.
    #+end_example
  + restrict any value to be set:
    #+begin_example
    schemas:
      AnyValue:
        anyOf:
          - type: string
          - type: number
          - type: integer
          - type: boolean
          - type: array
            items: {}
          - type: object
    #+end_example
  + any value, including null:
    #+begin_example
    components:
      schemas:
        AnyValue:
          nullable: true
          description: Can be any value, including `null`
    #+end_example
- enums ::
  + eg: defining your own API data model with a sorting parameter feature having two possible values: asc, desc, and
    nullable:
    #+begin_example
    paths:
      /items:
        get:
          parameters:
            - in: query
              name: sort
              description: Sort order
              schema:
                type: string # <--- all enums have to be of the same type (string here), except for null value (null object, not string)
                nullable: true  # <--- enum may be nullable
                enum: # can also just be in array form instead of per line: ~enum: [asc, desc]~
                  - asc
                  - desc
                  - null # <--- got to be explicitly included, "nullable: true" alone is not enough, and without quotes, i.e. null object not "null"
                description: >
                  Sort order:
                   * `asc` - Ascending, from A to Z
                   * `desc` - Descending, from Z to A
    #+end_example
  + reusable enums :: by defining reusable enums in the global ==components== section and reference them via ==$ref== elsewhere:
    #+begin_example
    paths:
      /products:
        get:
          parameters:
          - in: query
            name: color
            required: true
            schema:
              $ref: '#/components/schemas/Color'
          responses:
            '200':
              description: OK
    components:
      schemas:
        Color:
          type: string
          enum:
            - black
            - white
            - red
            - green
            - blue    
    #+end_example
- files ::
  + files defined as strings, but ==format== can be defined
    #+begin_example
    # for binary
    type: string
    format: binary  # binary file contents
    # base64 content
    type: string
    format: byte    # base64-encoded file contents
    #+end_example
- inheritance and polymorphism ::
  + model composition ::
    - use :: when some model schemas share common properties, instead of describing those properties for each schema
      repeatedly, describe schemas as a composition of common property set and schema-specific properties
    - use ==allOf== to combine different properties ::
    - in following example ==ExtendedErrorModel== schema includes its own properties and properties inherited from ==BasicErrorModel==:
      #+begin_example
      components:
        schemas:
          BasicErrorModel:
            type: object
            required:
              - message
              - code
            properties:
              message:
                type: string
              code:
                type: integer
                minimum: 100
                maximum: 600
          ExtendedErrorModel:
            allOf:     # Combines the BasicErrorModel and the inline model
              - $ref: '#/components/schemas/BasicErrorModel'
              - type: object
                required:
                  - rootCause
                properties:
                  rootCause:
                    type: string
      #+end_example
  + polymorphism ::
    - use :: when possible to have different alternatives
    - can use ==oneOf==, or ==anyOf== ::
    - eg: an openAPI with request and responses that can be described by several alternative schemas:
      #+begin_example
      components:
        responses:
          sampleObjectResponse:
            content:
              application/json:
                schema:
                  oneOf:
                    - $ref: '#/components/schemas/simpleObject'
                    - $ref: '#/components/schemas/complexObject'
        …
      components:
        schemas:
          simpleObject:
            …
          complexObject:
            …
        #+end_example
  + discriminator ::
    - use :: to help identify an object type
    - add ==discriminator/propertyName== keyword to the model definitions
      + the discriminator is used with ==anyOf== or ==oneOf== keywords only
      + it's important that all models mentioned below ==anyOf== or ==oneOf== contain the property that the discriminator specifies
    - eg: help an API consumers detect an object type: the discriminator points to ~objectType~ property that contains
      the data type name, and both ~simpleObject~ and ~complexObject~ have (must) the ~objectType~ property
      #+begin_example
      components:
        responses:
          sampleObjectResponse:
            content:
              application/json:
                schema:
                  oneOf:
                    - $ref: '#/components/schemas/simpleObject'
                    - $ref: '#/components/schemas/complexObject'
                  discriminator:
                    propertyName: objectType
        …
        schemas:
          simpleObject:
            type: object
            required:
              - objectType
            properties:
              objectType:
                type: string
            …
          complexObject:
            type: object
            required:
              - objectType
            properties:
              objectType:
                type: string
            …
      #+end_example
    - ==discriminator== can be used by various API consumers, such as code generation tools: they can use discriminator
      to generate program statements that typecast request data to appropriate object type based on the discriminator
      property value
  + mapping type names ::
    - use :: further to discriminating and help identify objects types, can add an additional specialisation and map to
      specific objects
    - as seen previously, with ==discriminator==, the property to which discriminator refers, contains the name of the
      target schema (in example above: ==objectType== property should contain either ==simpleObject==, or
      ==complexObject== string). If the property values do not match the schema names, it's possible to map the values
      to the names. To do this, use the ==discriminator/mapping== keyword
    - use ==discriminator/mapping== keyword :: 
    - eg:
      #+begin_example
      components:
        responses:
          sampleObjectResponse:
            content:
              application/json:
                schema:
                  oneOf:
                    - $ref: '#/components/schemas/Object1'
                    - $ref: '#/components/schemas/Object2'
                    - $ref: 'sysObject.json#/sysObject'
                  discriminator:
                    propertyName: objectType
                    mapping:
                      obj1: '#/components/schemas/Object1'
      		  obj2: '#/components/schemas/Object2'
                      system: 'sysObject.json#/sysObject'
        …
        schemas:
          Object1:
            type: object
            required:
              - objectType
            properties:
              objectType:
                type: string
            …
          Object2:
            type: object
            required:
              - objectType
            properties:
              objectType:
                type: string
            …
      #+end_example
    - in example, all objects are discriminated to ~objectType~ property, and mapped to respective values ~obj1~, ~obj2~, or ~system~
    - ~obj1~ value is mapped to the ~Object1~, ~obj2~ value is mapped to the ~Object2~, and value ~system~ matches the
      ~sysObject~ model that is located in an external file
- all supported json schemas keywords :: checkout: https://swagger.io/docs/specification/data-models/keywords/
** schemas for yaml/json data validation
- notes ::
  + each time a yaml parser reads a value, it guesses the type tag in a process called "tag resolution":
    - eg: the "Norway problem": suppose a yaml parser reads: ~country-list: [DK, NO, SE]~, with no context, or details
      given, it interprets that as: ~{'country-list': ['DK', False, 'SE']}~
    - hence the need for a schema, so that there is no ambiguities in the parser result
  + json/yaml schemas are standards that describe structure, constraints, and data types for a set of json/yaml data
  + json/yaml schemas are "hypermedia-ready", meaning that resources' representation contains links to other resources,
    enabling the client to navigate APIs without knowing URLs for that resource, only the API "entry point" is required
  + json/yaml schema documents are identified by URIs (Uniform Resource Identifiers: whereas URLs specify the location
    of a resource on the internet, a URI can be used to identify any type of resource, not just those on the internet),
    which can be used in HTTP link headers and within schema documents to allow recursive definitions for example
  + pros/cons ::
    - pros ::
      + allows validity of json/yaml structures, constraints, entries' values, etc.
      + json/yaml schemas are leveraged by many IDEs to provide json/yaml documents auto-completion, error highlighting
        (live validation), documentation on hover, etc. (eg with vscode plugin ==yaml== by redhat:
        https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml)
        - [[https://www.schemastore.org/json/][json schema store]] registry provides a large choice of ready to use schemas to choose from
    - cons ::
      + indentation sensitive
- schemas ::
  + notes ::
    - vocabulary ::
      + "instance" :: the document being validated or described,
      + "schema" :: the document that contains the description
    - most programming languages have builtin types that map yaml data models to objects, but further features/tools
      might be needed to completely integrate them
      + for example, a general purpose validator is needed to ensure that the data is formed as expected: in JavaScript,
        when loading a json document with ==js-yaml==, it only returns a tree of JavaScript objects and values which
        hasn't been validated against any schema
      + some languages like C have no built-in way to represent yaml models, and additional libraries are required to
        define their own types to represent the yaml data: == libyaml== C library for example provides a low-level
        token-based API, but can be inconvenient to use
      + C++ ?
      + validating yaml with yq ::
        - ~yq v -d1 multidoc.yml~ :: validates both document 0 and document 1 (but not document 2)
        - ~yq v -d'*'~ multidoc.yml :: validates all documents in the yaml (==*== is quoted to avoid the CLI from
          processing the wildcard)
    - in many cases a validator supports only some subset of the yaml data model and not all of it, but it's usually
      enough when one knows what he needs and expects
      + also, for the same programming language, there can be different libraries or frameworks providing schema
        validators/generators, code generators, documentation generators, UI generation, json schema processing, etc.,
        so keep in mind that they might have (or miss) some of desired features, or might conflict/mismatch with what's
        provided by other tools for the same schema/model/etc.
      + in other words, know what you need and try to search a bit for the best tools that match the most your needs
    - example of json schema ::
      #+begin_example
      $id: https://example.com/schema.json
      $schema: https://json-schema.org/draft/2020-12/schema
      type: object
      properties:
        country-list:
          type: array
          items: { "type": "string" }
      #+end_example
  + input example used ::
    - product catalogue storing json data, eg:
      #+begin_example
      {
        "productId": 1,
        "productName": "A green door",
        "price": 12.50,
        "tags": [ "home", "green" ]
      }
      #+end_example
  + creating schemas ::
    - required definitions at the beginning of the json schema ::
      + $schema :: specifies which draft of the JSON Schema standard the schema adheres to
      + $id :: sets a unique URI for the schema, that can be used to refer to elements of the schema from inside the
        same document or from external JSON documents
        - reminder :: can be URL or a local reference (prefer relative path instead of absolute)
      + title, description :: state the intent of the schema (doesn't add any constraints to the data being validated)
      + type :: defines the first constraint on the JSON data (on the json document itself)
        - in the product catalogue example considered here, this keyword would specify that the data in the json
          document must be a JSON object (~{"type": "object" }~)
        - but in general ==type== is used to define any type constraint on a property's (key) value :
	  ~{ "type": "string" }~, ~{ "type": "integer" }~, etc.
      + vocabulary ::
        - [[https://json-schema.org/draft/2020-12/json-schema-core#name-keyword-behaviors][schema keywords]] :: $schema, $id, etc.
        - [[https://json-schema.org/draft/2020-12/json-schema-validation#name-a-vocabulary-for-basic-meta][schema annotations]] :: title, description, etc.
        - [[https://json-schema.org/draft/2020-12/json-schema-validation#name-validation-keywords-for-any][validation keywords]] :: properties, type, enum, const, etc.
      + eg ::
        #+begin_example
        {
          "$schema": "https://json-schema.org/draft/2020-12/schema",
          "$id": "https://example.com/product.schema.json",
          "title": "Product",
          "description": "A product in the catalog",
          "type": "object"
        }
        #+end_example
  + defining properties ::
    - ==properties==: validation keyword, that define properties (keys) that are used in yaml/json documents that needs
      to be validated
      + used to define constraint, structure, data types, etc., against which a validator is to validate the data content
      + property productId can also be defined without any annotation: the productId description or title,
	and just ~{"properties": {"productId": {}}~
    - requirements ::
      + value of "properties" MUST be an object, and each value of this object MUST be a valid JSON Schema
    - required properties ::
      + done with object ==required==, eg: ~"required": [ "productId", "productName", "price" ]~ (defined at same level
        as properties)
    - arrays' properties ::
      + define item properties in an array property ::
        - ==uniqueItems== :: enforces uniqueness of items in an array
        - ==minItems== :: for example a product can have tags, which can be a list of strings, and when a tag is
          defined, it has to have at least one tag:
          #+begin_example
           ...
          "tags": {
            "description": "Tags for the product",
            "type": "array",
            "items": {
              "type": "string"
            },
            "minItems": 1
          }
          #+end_example
    - nested structures :: insert object with properties inside a property
      + eg: a product has a dimension with width, length, etc.: ==product/dimensions/{length,width,height}==
        #+begin_example
        {
          "$schema": "https://json-schema.org/draft/2020-12/schema",
          "$id": "https://example.com/product.schema.json",
          "title": "Product",
          ...
          "properties": {
            "productId": {
              "description": "The unique identifier for a product",
              "type": "integer"
            },
            ...
            "dimensions": {
              "type": "object",
              "properties": {
                "length": {
                  "type": "number"
                },
                "width": {
                  "type": "number"
                },
                "height": {
                  "type": "number"
                }
              },
              "required": [ "length", "width", "height" ]
            }
          },
          "required": [ "productId", "productName", "price" ]
        }
        #+end_example
    - add external reference with ==$ref== ::
      + used to reference resources outside of current schema, and allow schema sharing across data structures
      + advantages: easy to share, use, read, and keep up-to-date
      + eg: create new schema (geographical location), and reference it in our product schema
        - geographical location ::
          #+begin_example
          {
            "$id": "https://example.com/geographical-location.schema.json",
            "$schema": "https://json-schema.org/draft/2020-12/schema",
            "title": "Longitude and Latitude",
            "description": "A geographical coordinate on a planet (most commonly Earth).",
            "required": [ "latitude", "longitude" ],
            "type": "object",
            "properties": {
              "latitude": {
                "type": "number",
                "minimum": -90,
                "maximum": 90
              },
              "longitude": {
                "type": "number",
                "minimum": -180,
                "maximum": 180
              }
            }
          }
          #+end_example
        - product schema ::
          #+begin_example
          {
            "$schema": "https://json-schema.org/draft/2020-12/schema",
            "$id": "https://example.com/product.schema.json",
            "title": "Product",
            "description": "A product from Acme's catalog",
            "type": "object",
            "properties": {
              "productId": {
                "description": "The unique identifier for a product",
                "type": "integer"
              },
              ...
              "warehouseLocation": {
                "description": "Coordinates of the warehouse where the product is located.",
                "$ref": "https://example.com/geographical-location.schema.json"
              }
            },
            "required": [ "productId", "productName", "price" ]
          }
                    #+end_example
        - reminder :: $ref refers a URI, so can also be a local relative reference
    - final complete product schema example ::
      + product catalogue schema ::
        #+begin_example
        {
          "$schema": "https://json-schema.org/draft/2020-12/schema",
          "$id": "https://example.com/product.schema.json",
          "title": "Product",
          "description": "A product from Acme's catalog",
          "type": "object",
          "properties": {
            "productId": {
              "description": "The unique identifier for a product",
              "type": "integer"
            },
            "productName": {
              "description": "Name of the product",
              "type": "string"
            },
            "price": {
              "description": "The price of the product",
              "type": "number",
              "exclusiveMinimum": 0
            },
            "tags": {
              "description": "Tags for the product",
              "type": "array",
              "items": {
                "type": "string"
              },
              "minItems": 1,
              "uniqueItems": true
            },
            "dimensions": {
              "type": "object",
              "properties": {
                "length": {
                  "type": "number"
                },
                "width": {
                  "type": "number"
                },
                "height": {
                  "type": "number"
                }
              },
              "required": [ "length", "width", "height" ]
            },
            "warehouseLocation": {
              "description": "Coordinates of the warehouse where the product is located.",
              "$ref": "https://example.com/geographical-location.schema.json"
            }
          },
          "required": [ "productId", "productName", "price" ]
        }
        #+end_example
      + ==exclusiveMinimum==: to set a minimum acceptable value to a numerical type (0 not included)
        - ==minimum== : to also include 0
  + validating yaml/json against its schema ::
    - 
** schema generation
[2024-08-04 Sun 03:14]
** yaml/json linters
[2024-08-25 Sun 00:11]
- linters: code/data analyser used to flag programming errors, bugs, stylistic errors, suspicious constructs, etc.
- yamllint ::
  + yamllint: much stricter standard than some basic validators (such as ==yq==), and highly customisable: "does not
    only check for syntax validity, but also for weirdnesses like key repetition, cosmetic problems such as lines
    length, trailing spaces, indentation, etc."
  + can configure style using rules in a config file to fit needs and standards (perfect for CI/CD pipelines)
  + yamllint has limits though, such as being slower when substantial amount of yaml files and sets of schemas to
    process
    - exists alternatives (though potentially more complex) able to overcome the issue, such as
      https://github.com/23andMe/Yamale (enforces further rigid rules, and allow parallel yaml processing files and
      further security checking)
    - yamale can also validate yaml files against schemas, which is a plus
- yamale ::
  + tool set for yaml files: linter (much stricter than yamllint), schema validator, parallel yaml files processor
    (perfect when processing lots of files), package importable in python, etc.
- yaml-validator :: handy when working with node.js
** yaml/json schema validator: pajv
- some yaml/json schema validators (all for CLI, [[https://json-schema-everywhere.github.io/yaml][source]]) ::
  - yamale (python, not easy to install on debian, relatively unknown), yaml-validator (JS, only structural validation),
    Rx (JS, Perl, PHP, Ruby, Python, no recent development activity)
- 
* curl
[2024-01-29 Mon 20:38]
** Notes
[2024-01-29 Mon 21:17]
- in every HTTP request, there is a method, sometimes called a verb. The most commonly used ones are GET, POST, HEAD and
  PUT
- normally however you do not specify the method in the command line, but instead the exact method used depends on the specific options you use ::
  + GET is the default,
  + using -d or -F makes it a POST,
  + -I generates a HEAD,
  + and -T sends a PUT.
- some options ::
  + -H :: eg: ~-H "Accept: application/json"~: specifies the HTTP request header, indicating an expected response in
    JSON format
  + -o :: for output file
** examples
- Get a README file from an FTP server :: curl ftp://ftp.example.com/README
- Get a webpage from a server using port 8000 :: curl http://www.example.com:8000/
- Get all terms matching curl from a dictionary :: curl dict://dict.example.com/m:curl
- Get a file from an SSH server using SFTP :: curl -u username sftp://example.com/etc/issue
- Get a file from an SSH server using SCP using a private key (not password-protected) to authenticate :: curl -u
  username: --key ~/.ssh/id_rsa scp://example.com/~/file.txt
** Http methods with curl
[2024-01-29 Mon 20:59]
*** Post
- notes ::
  + to send form data, a browser URL encodes it as a series of name=value pairs separated by ampersand (&) symbols. The
    resulting string is sent as the body of a POST request. To do the same with curl, use the -d (or --data) argument
- simple post :: curl -d 'name=admin&shoesize=12' http://example.com/
- when specifying multiple -d options on the command line, curl concatenates them and insert ampersands in between ::
  curl -d name=admin -d shoesize=12 http://example.com/
- if the amount of data to send is too large for a mere string on the command line, can read it from a filename in
  standard curl style ::
  curl -d @filename http://example.com
- content-type ::
  + POSTing with curl's -d option makes it include a default header that looks like
    ~Content-Type: application/x-www-form-urlencoded~. That is what your typical browser uses for a plain POST.
  + if that header is not good enough for you, you should, of course, replace that and instead provide the correct
    one. Such as if you POST JSON to a server and want to more accurately tell the server about what the content is:
    ~curl -d '{json}' -H 'Content-Type: application/json' https://example.com~
- json :: 
  + curl 7.82.0 introduced the --json option as a new way to send JSON formatted data to HTTP servers using POST. This option works as a shortcut and provides a single option that replaces these three ::
    1. --data [arg]
    2. --header "Content-Type: application/json"
    3. --header "Accept: application/json"
  + the option does not make curl actually understand or know about the JSON data it sends, but makes it easier to send
    it. curl does not touch or parse the data that it sends, so you need to make sure it is valid JSON yourself
  + can use multiple --json options on the same command line. This makes curl concatenate the contents from the options
    and send all data in one go to the server. Note that the concatenation is plain text based and it does not merge the
    JSON objects as per JSON
  + receiving json ::
    - curl itself does not know or understand the contents it sends or receives, including when the server returns JSON
      in its response. Using a separate tool for the purpose of parsing or pretty-printing JSON responses might make
      things easier for you, and one tool in particular that might help you accomplish this is 'jq'. example:
      + send a basic JSON object to a server, and pretty-print the JSON response :: curl --json '{"tool": "curl"}' https://example.com/ | jq
      + send the JSON with jo, print the response with jq :: jo -p name=jo n=17 | curl --json @- https://example.com/ | jq
  + examples ::
    - send a basic JSON object to a server :: curl --json '{"tool": "curl"}' https://example.com/
    - send JSON from a local file :: curl --json @json.txt https://example.com/
    - send JSON passed to curl on stdin :: echo '{"a":"b"}' | curl --json @- https://example.com/
    - send JSON from a file and concatenate a string to the end :: curl --json @json.txt --json ', "end": "true"}' https://example.com/
