# -*- mode: org -*-
#+title: "Other" learnings in general
#+SETUPFILE: ~/set-up-files/basic-setups.org


* cron (crontab, cronjob, etc.)
** Notes
+ cron tool format explainer :: https://crontab.cronhub.io/
+ Simply put, cron is a basic utility available on Unix-based systems. It enables users to schedule tasks to run periodically at a specified date/time, without requiring human intervention.
+ Cron runs as a daemon process (needs to be started once and it keeps running in the background). The process makes use of crontab to read the entries of the schedules and kicks off the tasks.
** Working with crontab
+ A cron schedule is a simple text file located under ==/var/spool/cron/crontabs== on Linux systems.
+ crontab files cannot be edited directly, so they need to be accessed using the crontab command.
+ commands ::
  - to open crontab file :: ==crontab -e==
  - a crontab line is an entry with an expression and a command to run ::
    + eg :: the entry ==* * * * * /usr/local/ispconfig/server/server.sh== runs the mentioned script (server.sh) every single minute.
*** Cron Expression
+ the cron expression consists of 5 fields :: ==<minute> <hour> <day-of-month> <month> <day-of-week> <command>== 
**** Special Characters in Cron Expression
+ ==*== (all) specifies that event should happen for every time unit
+ ==?== (any) is used in the <day-of-month> and <day-of-week> fields to denote the arbitrary value and thus neglect the field value. For example, if we want to fire a script at the 5th of every month, irrespective of what day of the week falls on that date, we specify a ==?== in the <day-of-week> field
+ ==–== (range) determines the value range. For example, "10-11" in the <hour> field means "10th and 11th hours"
+ ==,== (values) specifies multiple values. For example, "MON, WED, FRI" in <day-of-week> field means on the days "Monday, Wednesday and Friday"
+ ==/== (increments) specifies the incremental values. For example, a "5/15" in the <minute> field means at "5, 20, 35 and 50 minutes of an hour"
+ ==L== (last) has different meanings when used in various fields. For example, if it’s applied in the <day-of-month> field, it means last day of the month, i.e. 31st of January and so on as per the calendar month.
  - it can be used with an offset value, like =L-3=, which denotes the "third to last day of the calendar month"
  - in <day-of-week>, it specifies the "last day of a week"
  - it can also be used with another value in <day-of-week>, like ==6L==, which denotes the "last Saturday"
+ ==W== (weekday) determines the weekday (Monday to Friday) nearest to a given day of the month. For example, if we specify "10W" in the <day-of-month> field, it means the "weekday near to 10th of that month". So if "10th" is a Saturday, the job will be triggered on "9th" and if "10th" is a Sunday, it will trigger on "11th". If we specify "1W" in <day-of-month> and if "1st" is Saturday, the job will be triggered on "3rd" which is Monday, it will not jump back to the previous month
+ ==#== specifies the "N-th" occurrence of a weekday of the month (<day-of-week>), for example, "third Friday of the month" can be indicated as "5#3"
**** Examples
+ at 12:00 p.m. (noon) every day :: ==0 12 * * ?==
+ every 15 minutes every day :: ==0/15 0 * * ?==
+ using increments to run the job every odd minute :: ==1/2 0 * * ?==
+ every five minutes starting at 1 p.m. and ending at 1:55 p.m. (cron job reference by default is per hour) and then starting at 6 p.m. and ending at 6:55 p.m., every day :: ==0/5 13,18 * * ?==
+ every minute starting at 1 p.m. and ending at 1:05 p.m. (not default here, but rather range), every day :: ==0-5 13 * * ?==
+ at 1:15 p.m. and 1:45 p.m. every Tuesday in the month of June :: ==15,45 13 ? 6 Tue==
+ at 9:30 a.m. every Monday, Tuesday, Wednesday, Thursday and Friday :: ==30 9 ? * MON-FRI==
+ at 6 p.m. on the third to last day of every month :: ==0 18 L-3 * ?==
+ at 10:30 a.m. on the last Thursday of every month :: ==30 10 ? * 4L==
+ at 10 a.m. on the third Monday of every month :: ==0 10 ? * 1#3==
+ at 12 midnight on every 5th day, starting from the 10th until the end of the month :: ==0 0 10/5 * ?==
**** Cron Special Strings
In addition to the fields specified in the cron expression, there’s also support for some special, predefined values that we can use instead of the fields :
+ ==@reboot== :: run once at the startup
+ ==@yearly== or ==@annualy== :: run once a year
+ ==@monthly== :: run once a month
+ ==@weekly== :: run once a week
+ ==@daily== or ==@midnight== :: run once a day
+ ==@hourly== :: run hourly

  
* Containerization, docker, and kubernetes
[2024-01-21 Sun 20:06]
** Introduction, glossary and notes
- why use containers ::
  + instead of one app per server -> many apps on one server
  + VMWare and hypervisor models technology (that allows for many apps to run on one server) are not solving all the issues, and not cheap :
    - hypervisor models :: several virtual machines (VMs) on same physical hardware (separate and completely independent from each other)
      + slices of VMs are installed on a hypervisor
      + need for own dedicated OS, need pre-configured use of physical hardware => can't be used by another app when not being used by the dedicated app
  + containers ::
    - slices of the OS on which the apps are run
  + pros ::
    - containers are more lightweight, thus more efficient space wise
    - apps share dynamically the physical resources
    - no dedicated OS required on the containers since they're using the physical OS
    - can run on any machine, server, VM, etc.
  + cons ::
    - dependent on the host OS (docker on windows run windows apps, same for linux (still possible to run linux apps on docker windows))
- docker images :: pre-packed application, has everything needed to run single application wrapped into a single bundle, eg: web server running static/dynamic content, database, etc.
- workloads :: application running on kubernetes for example
- cloud-native microservices design :: instead of a monolith, split the application modules into several parts that can communicate
  + monolith/legacy app ::  everything the app does (web, authentication, search, etc.) is provided within a single binary (computer program)
    - cons :: an issue on a single feature requires taking down whole app, fix the module, recompile the whole app, deploy it, etc.; scaling is harder and needs scaling (almost, but quite) the whole app; etc.
  + microservices :: is a way of building, deploying, and managing each application feature as its own small app
    - usually, each microservice is run in its own container
  + pros of microservices :: redeploy just the required module (faster), adapted to cloud-native services
  + cloud-native applications :: built as a set of microservices that run in /Open Container Initiative/ compliant containers
    - cloud-native :: is like a set of capabilities, such as self-healing, auto-scaling, rolling updates, and more.
    - cloud-native doesn't apply only in the cloud, can do cloud-native capabilities on laptop/pc with docker desktop
    - advantages :: scalable, dynamic, loosely coupled via APIs for example, and more importantly, it can run anywhere with k8s
- declarative approach ::
  + describes an application in a configuration file, and use that file to deploy the application
    - in other words, instead of deploying sets of apps and containers with loads of command line action, just list every step in a configuration file, give the file to the containerization platform, and let the platform do the hard work of deployment/management
  + compose file :: generally the declarative approach is done using a yaml configuration file called a "compose file"
- docker hub :: library of pre-existing docker images (need to download and run them in the docker host)
- container runtime ::
  + also called 'container engine': software component that can run containers on a host operating system
  + sits at the heart of any container service such as Docker
  + are daemon (background) processes responsible for managing container creation tasks such as pulling images from repositories, resource and storage allocation, network creation, etc.
- containerd :: container runtime originally developed by Docker
  + docker uses containerd as its runtime for creating containers from images. Essentially, it acts as an interface (API) that allows users to use containerd to perform low-level functionality. Simply put, when you run Docker commands in the terminal, Docker relays those commands to its low-level runtime (Containerd) that carries out all the necessary procedures.
** Docker
[2024-01-21 Sun 20:51]
*** Notes
[2024-01-21 Sun 21:14]
- main improvement of docker :: make running apps inside of containers easy
- two docker version :: community edition and enterprise edition for more features and official support
- process ::
  1. transform code into docker image (build code into docker image)
  2. push image into a registry (docker hub or any other private registry)
  3. start image (run app as a container)
- generated OCI image :: (Open Container Initiative), the generated docker image, which is just standard container content
  + image spec
  + runtime spec
  + distribution spec (registries)
- "images are build time constructs, and containers are runtime constructs" :: jargon to say that containers can be seen as extensions of an image, just as in running state (and the image is on stop state)
- "containerization" :: buzzword to refer to an app running in a container
- container image :: a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings
*** Dockerfile
[2024-02-01 Thu 17:15]
- Dockerfile :: contains all the commands a user could call on the command line to assemble an image (set of build instructions for the app and its dependencies)
- Docker swarm :: lightweight container orchestrator (compared to kubernetes which is more complete, but also more complex)
- some instructions ::
  + ~FROM~ :: initializes a new build stage, and sets the Base Image for subsequent instructions. eg: ~FROM node:current-alpine~: build ongoing image by first grabbing image 'node:current-alpine'
  + ~LABEL~ :: for metadata
  + ~RUN~ :: to execute a command, eg: ~RUN npm install~: will run npm to install the app dependencies
  + ~ENTRYPOINT~ :: to specify default executable, eg: ~ENTRYPOINT ["node", "app.js"]~: will run the command 'node app.js' each time the container gets started
  + ~COPY~ :: to copy :-), eg: ~COPY . /usr/src/app~: copy app code (.) to /usr/src/app *of the container image*
  + ~WORKDIR~ :: to change working directory, eg: ~WORKDIR /usr/src/app~: set working directory context (when run together with ~COPY . /usr/src/app~ command, it just basically set the working directory to where the app was installed, on *the container image*)
*** Declarative method with compose files
[2024-02-02 Fri 09:40]
**** General
- notes ::
  + some often used files in declarative method :: 
    - requirements.txt :: lists dependencies to install (with for example ~RUN pip install -r requirements.txt~ in the dockerfile)
    - Dockerfile :: says how to build the docker images
    - compose.yml :: some instructions example of a docker compose file
      #+begin_src yaml
      # declaring network called 'counter-net', and volume called 'counter-vol'
      # saves time by declaratively doing the configurations, instead of using scripts with long docker network create commands
      networks:
        counter-net:

      volumes:
        counter-vol:

      # under services, defines 2 microservices (thus, a multi-container app), ie. 2 containers: 'web-fe' (for the front-end) and 'redis' (for the back-end and persistence)
      services:
        web-fe:
          # call command docker build, and build a container image using Dockerfile and the app files in the current directory
          build: .
          # set command that will run whenever the app (the docker image) is started
          command: python app.py
          ports:
            # map port 8080 of container to port 5001 of docker host
            - target: 8080
              published: 5001
          # attach container to network 'counter-net'
          networks:
            - counter-net
          # mount volume 'counter-vol' into the container at '/app'
          volumes:
            - type: volume
              source: counter-vol
              target: /app
        # pull redis:alpine image and attach it to network 'counter-net'
        redis:
          image: "redis:alpine"
          networks:
            counter-net:
      #+end_src
    - 'desired state' :: configurations in compose files are called the desired state. From example above, the desire state is:
      + to create a network 'counter-net' and volume (a persistent data stores implemented by the container engine) 'counter-vol',
      + a web container on port 5001 with a shared volume and on network counter-net
      + a redis service on same network
- some docker compose commands ::
  + build all networks, volumes, services, etc., and bring the app up using docker compose command :: docker compose up --detach (~--detach~ to run in background and be able to keep using the current terminal)
    - need to run docker compose build command within the directory containing the 'compose.yml'
  + to stop the "microservices app" (multi-container app), and cleanup volumes :: docker compose down --volumes (volumes usually stay up otherwise in case some data must not be destroyed)
**** Docker swarm, and swarm mode
[2024-02-07 Wed 00:30]
- notes ::
  + swarm mode :: lets one connect multiple docker hosts into a secure highly available cluster
  + clusters :: 
    - a cluster (or a 'swarm') :: is a set of one or more machines, or 'nodes'. A node can be any type of node:  physical, virtual machines, cloud instances, multipass virtual machines, etc.
      + multipass virtual machines :: lightweight VMs that can easily run on windows, linux, mac, etc.
        - multipass VMs are nice tools to create cloud style ubuntu VMs, and they have precanned docker templates (docker environment portainer and related tools)
    - a cluster :: is a bunch of nodes with docker installed, and network connectivity
    - in a cluster :: every node has to be either a worker or a manager
      + managers :: host the control plane (cluster store, scheduler, etc.)
        - managers :: usually between 3 or 5 managers in a cluster, but need to be an odd due to the clusters' majority control principle (if a set of cluster still can communicate in case of "split brain" or "deadlock" (a network issue for example that causes a divide of managers in the cluster) has the majority, then the group stays up and running, otherwise, the group in minority switch to read-only mode, and changes on this minority group of managers is frozen)
      + workers :: where business apps run
        - can run as many worker nodes as apps requires
    - manager and worker nodes can be anything one wants :: on-prem, in the cloud, VMs, physicals, etc. All that matters is that they have docker installed, and they can communicate over reliable networks
      + better to spread nodes over availability zones so that for example when somebody trips over a wire, not all of them are taken out of actions at the same time
- multipass VMs ::
  + when using docker desktop, only one node is available really. If one wants to use different nodes then multipass VMs are a better alternative 
  + some multipass commands ::
    - ~multipass launch docker --name nodename~ :: create a multipass VM named 'nodename' that will be using a docker image
    - ~multipass info nodename~ :: to list infos about a node name (among which its IP address)
    - ~multipass list~ :: list infos of all nodes
    - ~multipass shell nodename~ :: reach multipass node 'nodename' with a shell bash
    - ~multipass delete nodename~ :: delete node 'nodename' (pretty handy to simulate a system failure on a node for example)
      + and then ~multipass purge~ for a deeper clean
- setup the swarm ('initialize' the swarm) ::
  1. create the swarm cluster :: ~docker swarm init --advertise-addr [ip-address]~ (advertise-addr is for configuring the swarm to set which of the multiple interfaces to use for cluster communications)
     - what to look for is the IP address to use for communication with other nodes in the cluster: usually, it will be a private IP in the internal network
  2. join in more *managers* to the cluster: within the node used for the docker swarm initialization, run ~docker swarm join-token manager~, and it will return the command and the token to use
     - to authorize new nodes to join as managers, run 'join-token' command on all of them
     - tip :: when listing nodes in the cluster (with ~docker node ls~), there is an asterisk at the end of the hash to depict the current node in the listed nodes
  3. join in more *workers* (still using join-token command to find the authorization token): ~docker swarm join-token worker~
     - tip :: worker nodes are found within the docker node list (~docker node ls~), with empty data in the "Manager status" column 
  4. need to stop business apps being scheduled to the managers :: basically just keep managers clean, and force business apps to run on workers only: ~docker node update --availability drain <managernodename>~ (to do for all managers)
- swarm services ::
  + swarm helps by providing features for microservices apps management, such as the service object, and the docker stack command
  + swarm service objects :: designed to map directly to individual application microservices.
    - Containers don't have a native way of doing things like scaling, rollouts, rollbacks, etc. For those, advanced tools like swarm service objects are needed: instead of mapping each microservice to individual container specs, they're instead mapped to service objects. And then, it's the service objects that provide more advanced features for microservices managements such as scaling, zero downtime, rolling updates, automatic rollbacks, etc.
    - declarative approach ::
      + notes ::
        - using the imperative way, managing manually all docker commands can be cumbersome, so better to do the service management declaratively through config files
        - in the declarative way, the deployment is done through config files (compose files and Dockerfile)
        - in swarm mode, the multi-container app is sometimes called a 'stack', and the compose file called the 'file describing a stack'
        - stacks on swarm do not support building images on the fly (eg with the compose file seen previously, where building was done using a local dockerfile: ~build: .~) => image need to be created first before the app is deployed (in compose file, use image instead of 'build': ~image: <registry-username>/<name-of-repository>:<name-of-the-image>~)
        - with swarm, building images on the fly is allowed (with dockerfile ~build: .~), which makes sense since shouldn't be building image at deploy time
          + so, first, need to create the service's image before the app is deployed
        - it is recommended to modify the config file (compose.yml file) when making changes to declarative apps  (instead of doing it imperatively through manual docker commands)
          + eg, when needs for scaling down/up: instead of using command ~docker service scale~, better to edit compose.yaml, and resend it to the swarm (thus, guaranteeing to keep config files synched with production environments)
            - resend the updated compose file with ~docker stack deploy -c compose.yaml <app-name>~ again, which will ping the latest compose file over to the swarm, and the reconciliation loop will spot the new desired state.
            - in case of system failure on a node, self-healing will still deploy containers with respect to the desired state (keep the number of containers on the still available nodes)
    - examples:
      1. using the *imperative way*, all docker commands are performed manually through the CLI:
         - ~docker service create --name web -p 8080:8080 --replicas 3 <registry-username>/<name-of-repository>:<name-of-the-image>~
           + create a service with on host-container mapping both on 8080
           + ~--replicas~: to set the number of containers the service is going to deploy and manage (3 here). Docker will spin up 3 identical containers from image '<name-of-the-image>'
           + can list services and their infos with: ~docker service ls~
           + notes ::
             - the command ~docker container ls~ doesn't understand swarm clusters, it's really not asking the cluster for a list of containers, it's just asking the local nodes (result depends on type of node connected on: manager, or workers). So, a ~docker container ls~ on a manager node won't return any service (or any application containers on it), since they're all on the workers
             - to proper way to list the application containers on any node is by using ~docker service ps <name-of-service>~, for example with 'web' service in the example above: ~docker service ps web~
               + ~service ps~ will also display the nodes they're running on
             - with docker swarm it's possible to reach the service from any node in the cluster, just using the service's exposed port. Example: service 'web' can be reached with using the IP of any node in the cluster and the port 8080 (eg: on browser: 192.168.64.20:8080 will access the web app, even though 192.168.64.20 is the IP of a manager node)
             - a docker service defines a single container template, but it let's you deploy multiple containers from it :: in the example above service 'web' defines 3 replicas, which will deploy and manage 3 identical containers: all based on same image, all listening to 8080, and all attached to same network
      2. using the *declarative way*:
         1. compose.yml ::
           #+begin_src yaml
           # Un-comment the 'version' field if you get an 'unsupported Compose file version: 1.0' ERROR
           #version: '3.8'
           networks:
             counter-net:
           
           volumes:
             counter-vol:
           
           services:
             web-fe:
               # with swarm, building images on the fly is not allowed (with dockerfile ~build: .~), which makes sense since shouldn't be building image at deploy time
               # so, first task is to create web-fe image before the app is deployed => run ~docker image build -t <registry-username>/gsd:swarm2023 .~ in a cluster's node before deploying ('.' to build the local directory as the build context)
               # and push the image to a docker registry so that the workers can pull it => ~docker image push <registry-username>/gsd:swarm2023~ (may require logging in with ~docker login~)
               image: <registry-username>/gsd:swarm2023
               command: python app.py
               # specifies the number of replicas of the containers defined by this service ('web-fe') => 10 identical containers based off of image: nigelpoulton/gsd:swarm2023
               deploy:
                 replicas: 10
               ports:
                 - target: 8080
                   published: 5001
               networks:
                 - counter-net
               # mount volume 'counter-vol' into the container at '/app'
               volumes:
                 - type: volume
                   source: counter-vol
                   target: /app
             # pull redis:alpine image and attach it to network 'counter-net'
             redis:
               image: "redis:alpine"
               networks:
                 counter-net:
           #+end_src
         2. when image is built and pushed to a registry, now can deploy using the declarative approach (through docker compose config file): ~docker stack deploy -c compose.yaml <app-name>~ (need to be in folder containing compose.yaml so that compose file can be found)
    - "ingress routing mesh" :: the routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there's no task running on the node. In other word, it's possible to reach the service from any node in the cluster, using the service's exposed port: if service is deployed on port XXXX, then from any IP of a node in the cluster, it's possible to reach the service using the exposed port [IP]:XXXX
      + swarm/ingress routing mesh is a *load balancing* requests across deployed services, meaning the requests will switch between the different deployed replicas (containers)
      + when scaling down, balancing requests will not probe traffic to containers that are deleted, swarm is clever enough to do that
    - swarm service commands ::
      + ~docker service create --name <service-name> -p <target-port>:<host-port> --replicas <number-of-replicas> <registry-username>/<name-of-repository>:<name-of-the-image>~ :: create service and deploy
      + ~docker service ls~ :: list all services and their infos
      + ~docker service ps <service-name>~ :: list specific service's infos
      + ~docker service scale <service-name>=<number-of-containers>~ :: rescale the number of containers for a given service (add/remove containers for the service)
      + ~docker service rm <service-id-1>[ <service-id-n>]* -f~ :: removal containers (~[ <service-id-n>]*~ is just a regex notation to specify that can add as many container ids separated by a space)
        - however, swarm constantly checks if desired state is fulfilled, so if deployed (or scaled) n containers, and removed some with ~docker service rm~, swarm will create new ones to match the desired state since the deleted ones will fail. This renewal process is called ==self-healing== (or reconciliation), which is constantly checking if observed state matches the desired state
        - an alternative is to rescale the number of containers
      + ~docker service rm <service-name>~ :: delete a service
      + ~docker stack deploy -c compose.yaml <app-name>~ :: deploy the 'stack' (the multi-container app), -c to tell where compose file is
      + ~docker stack ls~ :: check stacks infos
      + ~docker stack services <app-name>~ :: get more details about stack's services
      + ~docker stack ps <app-name>~ :: get replicas' infos
      + ~docker stack rm <app-name>~ :: to delete a stack. It will just delete the stack, but still keep the cluster, in case the nodes are still needed (to delete nodes/cluster: ~multipass delete <node-name1>[ <node-name-n>]*~, and ~multipass purge~ for a deeper clean)
*** Some docker commands
- general :: 
  + get help with Docker (can also use –help on all subcommands) :: docker --help
  + start the docker daemon :: docker -d (-d for detached from current terminal)
  + display system-wide information :: docker info
  + view resource usage stats :: docker container stats
  + view volumes services :: docker volume ls
  + note :: by default without specifying the registry url in the complete registry, docker will default to docker hub. So, when using a different registry, use the complete url with the target registry, like: docker.io/<registry-username>/<name-of-repository>:<name-of-the-image>, here docker.io is still docker hub though, lol
- listing ::
  + list all docker containers (running and stopped) :: docker ps --all (or: docker ls -a)
  + list currently running containers :: docker ps 
  + list local images :: docker images
- building image :: build an image (OCI file) from a Dockerfile and a context (the set of files and dependencies located in the specified PATH or URL)
  + build :: docker build -t <image_name>
  + build without the cache :: docker build -t <image_name> . –no-cache
  + build from a docker registry :: ~docker image build -t <registry-username>/<name-of-repository>:<name-of-the-image> .~ // the dot at the end is to specify the files path from which to build the docker image, '-t' is for tag, same as '--tag'
  + build with a different OS architecture (to be able to run it on a different OS architecture) (and push it to a registry) :: ~docker buildx build --platform linux/arm64/v8,linux/amd64 --push --tag <registry-username>/<name-of-repository>:<name-of-the-image> .~
- publish (push) an image to Docker Hub :: docker push <username>/<image_name>
- container state :: 
  + start or stop an existing container :: docker start|stop <container_name> (or <container-id>)
  + remove a stopped container :: docker rm <container_name>
- running ::
  + run a container in the background :: docker run -d <image_name> // the -d is to detach it from the terminal
    - run in attached to terminal mode :: docker container run -it --name <app_alias> alpine sh (-it for interactive; 'alpine' to base the container on the minimal Docker image based on Alpine Linux with a complete package index and only 5 MB in size; and 'sh' to run commands in the sh terminal as the container main process)
  + run a container and publish the container’s port(s) to the host :: docker run -p <host_port>:<container_port> <image_name> // 'host_port' is the local running port, and 'container_port' is the port the app is listening on in the container: any traffic hitting on 'host_port' will be mapped and sent to the 'container_port'
- open a shell inside a running container :: docker exec -it <container_name> sh
- removing images ::
  + delete an Image :: docker rmi <image_name>
  + remove all unused images :: docker image prune
- options ::
  + -d :: detached mode
  + -it :: interactive mode
  + --name :: to give the image an alias
*** Docker Swarm
[2024-02-02 Fri 09:11]

** Kubernetes
[2024-01-21 Sun 20:52]
*** Notes
[2024-01-21 Sun 21:34]
- fun facts ::
  + made by google, written in go
  + kubernetes can sometimes be shortened to 'k8s' (8 for 8 characters between the starting k and the ending s) 
- what, why k8s ::
  + k8s is a platform for managing containerized applications at scale (cloud native microservices apps)
  + k8s can be seen as an OS of the cloud, it just needs some infos about the app, the different services the app is made of, the configs, and it just run it, relieving the person making the request from tedious work: k8s does all the work deciding which node to run services on, how to pull and verify images, start the containers attached to networks, etc.
  + just needs a standard package format (package app as containers), a declarative manifest file, and there it goes
- more efficient than docker swarm
- kubernetes is more higher level than docker
  + whereas with docker things are more low level: build/download/start/stop/delete image, build
  + with kubernetes things are more higher level: scheduling, scaling, healing, updating, etc. (eg: how many containers to run in, which nodes to run them on, know when to scale them up/down, how many instances required to meet demand, etc.)
- takeaways ::
  + kubernetes cluster hosts applications
  + k8s cluster is mainly made of ::
    - 1 or more control plane nodes (brains of cluster, the /managers/: do tasks scheduling, monitoring, responding to events, etc.),
    - and a bunch of workers
  + kubernetes runs workloads by placing containers into 'Pods' (groups of one or more containers) to run on nodes
    - a 'node' may be a virtual or physical machine, depending on the cluster
    - each node is managed by the control plane and contains the services necessary to run Pods
    - a k8s 'Control Plane' is the container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers
  + each node on a cluster is running some kubernetes softwares ('Agents') and a container runtime (docker, containerd, or others)
    - there is a container runtime on every node so that every node can run containers
  + thus, one of the things k8s does is decide which nodes to run stuff on
    - one of the advantage of k8s is that it manage changes in loads, eg: it can override manual configs to balance loads on some other nodes when those set are overused ('increased load', can basically spin up more containers to balance workload even when the configurations are already set)
    - same when a node fails ('failed load'), k8s can use another node to run the workload, which is called 'self-healing'
  + k8s can run anywhere: On-premises (or 'On-prem': installed and runs on computers on the premises of the person/organization using the software, rather than at a remote facility), on server farms, cloud, etc. => k8s is easy to migrate
  + some cloud k8s options: AWS EKS, Azur AKS, Google GKE, etc.
  + apps' services are generally each deployed on its container, and when one of these services needs scaling, then the orchestrator (k8s for example) throw more containers at it (not make it bigger, just more of the same containers are enlisted, and reverse if need scaling down: reduce the enlisted containers)
*** Orchestration
[2024-02-01 Thu 15:05]
- notes ::
  + as of today, the most powerful containerization orchestrator is kubernetes
  + orchestration :: define the apps, how all the parts interact, provision the infrastructure, and then deploy and manage the app
  + key to automation of orchestration :: dependencies management, ordered startups, intelligent scheduling (scheduling services next to each other, or not next to each other (not starting on the same nodes as the others)), etc.
  + "app manifest" ::
    - it is the 'road map', the one that describes the map for the orchestration
    - it is given to the orchestrator (k8s), and then the orchestrator deploys and manages the app
*** Kubernetes architecture
[2024-02-11 Sun 23:57]
**** Intro
- process in a nutshell ::
  1. start with a packaged app ::
     - app is containerized (make it run as a container), wrap it in a pod, and in order to have features like scaling and self-healing, the pod is wrapped in a /deployment/
     - all the packaging process is defined in a k8s yaml file, to describe what the app should look like for k8s (which container to use, which port, what network, how many replicas, etc.)
  2. then, the packaged app (the k8s yaml file) is given to k8s cluster, and sets the cluster as specified in the yaml
**** Control plane nodes
[2024-02-12 Mon 23:36]
- notes ::
  + control plane are essential, they need to stay available, and so, better to run multiple ones for higher availability ("HA", 3 or 5, odd number. More than 5 is possible, but would imply more time for control planes to reach consensus)
  + best to keep them in separate failure domains connected by fast reliable networks (failure domains: regions or components of infrastructure that could fail. Each has its own risks and challenges to architect for. eg: putting control planes in same data center rack, and under same potential risk of failure could risk failures on all of them at once)
  + leader vs followers control planes :: k8s operates an active passive model for the managers, where only one control plane node is making active changes to the cluster at any one time (leader control plane). The others are followers.
    - the followers control planes proxy connections over to the leader so that the leader can make the actions
    - if leader goes down, a new one is elected :: every control plane node runs a bunch of smaller services responsible for individual control plane features, but also, every control plane node runs the full set of features, so any one of them can act as a leader
  + if one is building k8s himself, then he gets to choose the number and size of control planes nodes. But, on hosted k8s platform, the control plane is hidden and the cloud provider manages all of that
    - hosted k8s :: where cloud provider runs k8s for the user as a service => the user gets API endpoints, but the internal mechanisms (builds, upgrades, performances, high availability (HA), etc.) are handled by the cloud behind the scene
      + kind of a drawback since control plane nodes are hidden, and might be that workers are being run on managers, which is not recommended
  + main services making up the control planes ::
    - API server :: ==kube-apiserver==: gateway to the cluster, the front-end to the control plane. It is the only component the user gets to interact directly with
      + main entry for commands and queries (for example with the command line tool: ==kubectl==)
      + same goes for worker nodes and applications. If any needs to communicate with anything on the control plane, it does it through ==kube-apiserver==. Same for other control plane services, they talk to each other via the API server
      + exposes a RESTful API over a security port, and consumes JSON and yaml
        - in the case of users deploying and managing applications, a yaml file describing the need is sent (for example), the API server authenticates, authorizes, and validates the request. If all is good, it instructs other managers to deploy it and manage it
    - cluster store ::
      + only persistent component of entire control plane, persists cluster state and configs (including configs and states of all apps)
      + based on etcd NoSQL database (so far), and automatically distributed throughout all control plane nodes
        - possible to swap it with something else, or run etcd bit on dedicated nodes (advanced use though)
      + critical to cluster operations, and on large busy clusters can be the main performance bottleneck (distribute database at scale can be hard on etcd)
      + have recovery plans in place (best to regularly test them)
    - controller manager :: ==kube-controller-manager==
      + kind of a controller of controllers (node controller, deployment controller, endpoints controller, namespace controller, etc.)
      + runs as a reconciliation loop ("watch loop") :: watches the bits of the cluster it is responsible for, and looks for changes. Checks if the observed states match the desired state
    - scheduler :: ==kube-scheduler==
      + watches API server for new task and assigns them to nodes (tree hiding the forest, it does more: (anti-)affinity, constraints, taints, resource management, etc.)
  + ex: run kubectl to deploy a new application
    1. kubectl query sent to API server, which does authentication, authorization, etc.
    2. the desired state gets written to the cluster store
    3. the scheduler farms the work out to worker nodes
    4. various controllers sit in watch loops, observing the state of cluster, and making sure the desired is matched
**** Worker nodes
[2024-02-12 Mon 23:36]
- notes ::
  + 3 important worker components ::
    - kubelet ::
      + main k8s agent running on *every* cluster node (worker and control plane nodes)
    - container runtime ::
    - kube proxy :: 
**** Pods
[2024-02-12 Mon 23:37]
**** Services
[2024-02-12 Mon 23:37]
**** Deployment
[2024-02-12 Mon 23:37]
**** K8s API and API server
[2024-02-12 Mon 23:38]
*** Getting k8s
[2024-02-12 Mon 23:41]
*** Some kubernetes commands
** Tips and tricks
[2024-01-22 Mon 20:45]
*** Preparing containerization
- tools ::
  + docker desktop :: development docker and kubernetes environment (or their cloud counterparts: GKE (Google Kubernetes Engine) for google cloud, AKS (Elastic Kubernetes Service) for AWS, etc.)
  + checkout for :: tools for monitoring, logging, etc.
  + containers and cloud native :: can live alongside VMs within the same app, so don't hesitate to probe into the most appropriate direction for your business
- suitable workloads ::
- tips ::
  + try to setup a 'research and development "SWAT" team' to try new technologies on the cloud, and let them be ambassadors on the whole company when technology has gain enough momentum; advice: get devs and ops talking, get management talking, and then get doing!!
*** Entreprise oriented vs startup oriented (how appropriate is the design for the production readiness)
[2024-01-27 Sat 23:45]
* curl
[2024-01-29 Mon 20:38]
** Notes
[2024-01-29 Mon 21:17]
- in every HTTP request, there is a method, sometimes called a verb. The most commonly used ones are GET, POST, HEAD and PUT
- normally however you do not specify the method in the command line, but instead the exact method used depends on the specific options you use ::
  + GET is the default,
  + using -d or -F makes it a POST,
  + -I generates a HEAD,
  + and -T sends a PUT.
- some options ::
  + -H :: eg: ~-H "Accept: application/json"~: specifies the HTTP request header, indicating an expected response in JSON format
  + -o :: for output file
** examples
- Get a README file from an FTP server :: curl ftp://ftp.example.com/README
- Get a webpage from a server using port 8000 :: curl http://www.example.com:8000/
- Get all terms matching curl from a dictionary :: curl dict://dict.example.com/m:curl
- Get a file from an SSH server using SFTP :: curl -u username sftp://example.com/etc/issue
- Get a file from an SSH server using SCP using a private key (not password-protected) to authenticate :: curl -u username: --key ~/.ssh/id_rsa scp://example.com/~/file.txt
** Http methods with curl
[2024-01-29 Mon 20:59]
*** Post
- notes ::
  + to send form data, a browser URL encodes it as a series of name=value pairs separated by ampersand (&) symbols. The resulting string is sent as the body of a POST request. To do the same with curl, use the -d (or --data) argument
- simple post :: curl -d 'name=admin&shoesize=12' http://example.com/
- when specifying multiple -d options on the command line, curl concatenates them and insert ampersands in between :: curl -d name=admin -d shoesize=12 http://example.com/
- if the amount of data to send is too large for a mere string on the command line, can read it from a filename in standard curl style :: curl -d @filename http://example.com
- content-type ::
  + POSTing with curl's -d option makes it include a default header that looks like ~Content-Type: application/x-www-form-urlencoded~. That is what your typical browser uses for a plain POST.
  + if that header is not good enough for you, you should, of course, replace that and instead provide the correct one. Such as if you POST JSON to a server and want to more accurately tell the server about what the content is: ~curl -d '{json}' -H 'Content-Type: application/json' https://example.com~
- json :: 
  + curl 7.82.0 introduced the --json option as a new way to send JSON formatted data to HTTP servers using POST. This option works as a shortcut and provides a single option that replaces these three ::
    1. --data [arg]
    2. --header "Content-Type: application/json"
    3. --header "Accept: application/json"
  + the option does not make curl actually understand or know about the JSON data it sends, but makes it easier to send it. curl does not touch or parse the data that it sends, so you need to make sure it is valid JSON yourself
  + can use multiple --json options on the same command line. This makes curl concatenate the contents from the options and send all data in one go to the server. Note that the concatenation is plain text based and it does not merge the JSON objects as per JSON 
  + receiving json ::
    - curl itself does not know or understand the contents it sends or receives, including when the server returns JSON in its response. Using a separate tool for the purpose of parsing or pretty-printing JSON responses might make things easier for you, and one tool in particular that might help you accomplish this is 'jq'. example:
      + send a basic JSON object to a server, and pretty-print the JSON response :: curl --json '{"tool": "curl"}' https://example.com/ | jq
      + send the JSON with jo, print the response with jq :: jo -p name=jo n=17 | curl --json @- https://example.com/ | jq
  + examples ::
    - send a basic JSON object to a server :: curl --json '{"tool": "curl"}' https://example.com/
    - send JSON from a local file :: curl --json @json.txt https://example.com/
    - send JSON passed to curl on stdin :: echo '{"a":"b"}' | curl --json @- https://example.com/
    - send JSON from a file and concatenate a string to the end :: curl --json @json.txt --json ', "end": "true"}' https://example.com/
* Computer engineering linguee
[2024-01-27 Sat 23:08]
- LDAP authentication :: Lightweight directory access protocol (LDAP): a protocol that helps users find data about organizations, persons, and more. LDAP has two main goals: 1. store data in the LDAP directory, 2. authenticate users to access the directory. Allows for example to create users and groups that match one's organizational structure
- stateful vs stateless apps/service :: stateful: apps that persist data (has to remember stuff, eg: if a stateful app stops, then the data that was being persisted has to be stored so that when doing a retry on a different target, one case use the same input data (or keep states/logs before coming back up)); stateless: no storage needed, just retrieve as was when crushed for example
