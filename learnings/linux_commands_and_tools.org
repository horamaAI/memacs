# -*- mode: org -*-
#+title: Linux (mainly, but not just) commands/tools that are fun and handy
#+SETUPFILE: ~/set-up-files/basic-setups.org

* shell and shell scripting
[2024-05-26 Sun 21:56]
** Notes
[2024-06-01 Sat 16:28]
- cheatsheet :: https://devhints.io/bash, examples:
  + shell arguments ::
    - $# :: number of arguments
    - $* :: all positional arguments (as a single word)
    - $@ :: all positional arguments (as separate strings)
    - $1 :: first argument
    - $_ :: last argument of the previous command
  + special variables ::
    - $? :: exit status of last task
    - $! :: PID of last background task
    - $$ :: PID of shell
    - $0 :: filename of the shell script, or name of current script
    - $_ :: last argument of the previous command
    - ${PIPESTATUS[n]} :: return value of piped commands (array)
** shell types and shell startup configs
*** notes
- [non-]interactive shells and [non-]login shells :: there are two kinds of shells:
  + [non-]interactive shells :: you type into them (shell scripts)
  + [non-]login shells :: the shell run them when you first login (subshells)
- shells execution order ::
  + all shells will first run ==env==,
  + then login shells will run ==login==,
  + then interactive shells will run ==interactive==,
  + once finished, login shells will run ==logout==
- managing multiple shells :: when using more than one shell, always remember to manage files with respect to their specific shell and specify things which any POSIX-compliant shell (like aliases and environment variables) can understand in a common startup file
  + dotfiles :: when organising things in dotfiles for example, a solution would be to organise dotfiles folders, one for each shell (==.bash/, .zsh/== and ==.sh/==), and one for shell-independent files (==.shell/==):
    #+begin_example
    .bash/
        env
        interactive
        login
        logout
    .sh/
        env
        interactive
        login
    .shell/
        env
        interactive
        login
        logout
    .zsh/
        env
        interactive
        login
        logout
    #+end_example
*** where to put configs
[2024-05-27 Mon 19:56]
- deciding when action needs to be run:
  + does the command set/modify environment variables ? :: then ==login==
  + is it an alias, or a terminal specific environment variable (eg: $GREP_COLOR) ? :: then ==interactive==
  + ==env== :: for useful functions and other tools, eg: ~umask~, or modifying $PATH, for example to filter out duplicate
*** implementing shell configs
[2024-05-27 Mon 22:28]
- notes ::
  + fortunately [[https://blog.flowblok.id.au/2013-02/shell-startup-scripts.html][flowblok]] did a wonderful job of following the flow of startup files and help in finding which level of the shell these config files have to be implemented:
    #+CAPTION: shell startup flow
    #+NAME: fig:shell-startup-actual
    #+ATTR_ORG: :width 500
    #+ATTR_HTML: :width 300px
    [[../images/shell-startup-actual.png]]
** shell scripting
[2024-06-23 Sun 02:07]
*** Notes
- utils ::
  + run a child process :: ==bash -c==, eg: ~bash -c 'echo "$me"';~ to print local variable $me in a child process (subshell)
*** shell special variables
[2024-06-23 Sun 03:45]
**** environment variables with set vs export
[2024-07-04 Thu 22:41]
- set :: allows changes to environment variables and options on current shell session, and also show values of variables *and functions*
  + syntax :: ~set [options] [arguments]~
  + if no option or argument, then the set command prints all variables and functions in the current shell
  + set has many options that can be used to modify various aspects of the shell: error handling, debugging, expansion, job control, etc.
  + set as a positional parameters assigner ::
    #+begin_example
    #!/usr/bin/env bash
    set me=great foo bar;
    echo "$@";
    # returns:
    # me=great foo bar // $1 is "me=great" (literal, not variable "me"), $2 "foo", $3 "bar"
    #+end_example
  + set is a local variable assigner. The values assigned in current shell are not inherited by its child processes (or set positional parameters for that matter)
  + some options ::
    - -e :: exit right away if a command exits with a non-zero status (~set +e~ to turn off option after use)
    - -u :: treat unset variables as an error
    - -x :: print commands and their arguments as they are run, mostly useful for debugging and tracing
    - -o :: set or unset one of the shell options 
- export :: for creating global variables (environment variables in particular) that are accessible by all processes running in the current shell session and its child processes
  + can use export to configure system-wide settings by exporting environment variables in ==~/.bashrc== or ==/etc/profile== for example
- pitfalls of export and set ::
  + quoted variables ::
    - need for quoted variables :: prevent issues with spaces and special characters
    - eg:
      #+begin_example
      !/usr/bin/env bash
      
      # Initialize variable with special characters
      var="Hello * World!"
      
      # Print variable without quotes
      echo $var
      
      # Output: Hello (and the files in the current directory)
      # unexpected output since shell interprets asterisks as pathname expansion
      
      # Print variable with quotes
      echo "$var"
      
      # Output: Hello * World!
      # The output is expected because the quotes preserve the special characters
      #+end_example
  + overwriting built-in variables/functions ::
    - eg:
      #+begin_example
      !/usr/bin/env bash
      
      echo "Current user: $USER" # Outputs the current user (e.g. "john")
      
      # Overwrite the built-in variable USER with our own value
      USER="hacker"
      echo "Current user: $USER" # Outputs "hacker" instead of the actual user
      
      # Restore the original value of USER
      USER=$(/usr/bin/id -un)
      echo "Current user: $USER" # Outputs the actual user again
      #+end_example
  + overusing set options ::
    - risk :: unexpected behaviour, harder to read scripts
    - eg:
      + use set -x only for the parts that need debugging, and use explicit commands to output relevant information elsewhere:
        #+begin_example
        !/usr/bin/env bash
        echo "This is a simple script"
        x=10
        y=20
        set -x                 # Turn on tracing only for the calculation, and not before starting assignments
        z=$((x+y))
        set +x                 # Turn off tracing
        echo "The sum of x and y is $z"
        #+end_example
      + unclutter debugging outputs when using ~set -x~: use it only when necessary, prefer other debugging techniques such as logging or using a debugger
  + conventions ::
    - by convention, environment variables are named with uppercase letters, not local variables
  + always remember to unset used variables after use (with ~unset~)
**** separators with FS (Field Separator), IFS (Internal FS), OFS (Output FS), RS (record separator), ORS, NR, NF, FILENAME, FNR
***** IFS (Internal Field Separator) for bash separator in general
- IFS :: variable used by the shell to determine word boundaries while splitting a sequence of character strings
  + default values: at least three-characters string: space, tab, and newline
  + temporary IFS value ::
    - eg: iterate over content of $PATH:
      #+begin_example
      buff="$IFS"
      IFS=:
      (
          for ele in $PATH[@]; do
      	echo "content: $ele"
          done
          )
      IFS="$buff"
      #+end_example
  + IFS and ==read== command ::
    - eg: read from a csv file (comma separated): read the 2 last lines only (tail -n +2)
      #+begin_example
      #!/bin/bash
      while IFS="," read -r rec_column1 rec_column2 rec_column3
      do
       echo "Displaying Record-$rec_column1"
       echo "Quantity: $rec_column2"
       echo "Price: $rec_column3"
       echo ""
      done < <(tail -n +2 input.csv)
      #+end_example
***** awk separators with FS, OFS, RS, ORS, NR, NF, FILENAME, FNR
- ==FS== for awk internal field separators (example: ~awk -F':'~) and ==OFS== for output separator formatting
  + in the print/output command, the comma (",") concatenates two parameters with the value of awk OFS
  + eg: parse '/etc/passwd', and display columns 1, 3, and 6 (Name, UserID, HomeDirectory respectively), with " XXX " as field separator
    - inline: ~awk -F':' 'BEGIN{OFS=" XXX ";print "Name\tUserID\tHomeDirectory";} {print $1,$3,$6}END {print NR,"Records Processed";}' /etc/passwd | head -10~
    - or using a script in an awk file:
      + ~$cat etc_passwd.awk~ ::
        #+begin_example
        BEGIN{
          OFS=" XXX ";
          print "Name\tUserID\tHomeDirectory";
        }
        {
          print $1,$3,$6
        }
        END {
          print NR,"Records Processed";
        }
        #+end_example
      + command :: ~$awk -f etc_passwd.awk /etc/passwd | head -10~
      + result ::
        #+begin_example
        Name    UserID  HomeDirectory
        root XXX 0 XXX /root
        daemon XXX 1 XXX /usr/sbin
        bin XXX 2 XXX /bin
        sys XXX 3 XXX /dev
        sync XXX 4 XXX /bin
        games XXX 5 XXX /usr/games
        man XXX 6 XXX /var/cache/man
        lp XXX 7 XXX /var/spool/lpd
        mail XXX 8 XXX /var/mail
        #+end_example
- RS :: allows using separators for records ("sets" or "groups" of records)
  + ORS :: for format records output
  + example :: records of students
    - record ::
      #+begin_example
      $cat student.txt
      Jones
      2143
      78
      84
      77
       
      Gondrol
      2321
      56
      58
      45
       
      RinRao
      2122
      38
      37
      65
      #+end_example
    - awk script ::
      #+begin_example
      $cat student.awk
      BEGIN {
              RS="\n\n";
              FS="\n";
        
      }
      {
              print $1,$2;
      }
      #+end_example
    - result ::
      #+begin_example
      $ awk -f student.awk  student.txt
      Jones 2143
      Gondrol 2321
      RinRao 2122
      #+end_example
- ==FILENAME== for filename output :), example: ~$ awk '{print FILENAME}' student-marks~, where student-marks has 5 lines, will return 5 "student-marks" lines
- NR, NF, and FNR :: for number of records and fields
  + ==NR== for number of records variable, ==NF== for number of fields in a record
  + ==FNR==: when dealing with multiple files, it returns the number of records relative to the current input file
  + example: have 2 files with 5 lines each "student-marks" and "bookdetails". command: ~awk '{print FILENAME, FNR;}' student-marks bookdetails~ will return:
    #+begin_example
    student-marks 1
    student-marks 2
    student-marks 3
    student-marks 4
    student-marks 5
    bookdetails 1
    bookdetails 2
    bookdetails 3
    bookdetails 4
    bookdetails 5
    #+end_example
- example :: FS, OFS, RS, ORS, NF, NR
  + input ::
   #+begin_example
   $ cat <<$'EOF' | awk 'BEGIN {RS="\n\n"; FS="\n"; OFS=" -- "; ORS="\nXX\n";} {print "record#"NR":"$1,$2" has nfields:"NF;} END {print "#records: "NR;}'
   Jones
   2143
   78
   84
    
   Gondrol
   2321
   56
   58
   45
    
   RinRao
   2122
   38
   37
   65
    
   Edwin
   2537
   78
   67
   45
    
   Dayan
   2415
   30
   47
   20
   EOF
   #+end_example
  + result ::
    #+begin_example
    record#1:Jones -- 2143 has nfields:4
    XX
    record#2:Gondrol -- 2321 has nfields:5
    XX
    record#3:RinRao -- 2122 has nfields:5
    XX
    record#4:Edwin -- 2537 has nfields:5
    XX
    record#5:Dayan -- 2415 has nfields:6
    XX
    #records: 5
    XX
    #+end_example
  
**** FS/IFS/RS/etc. default separators (==\034==, ==\035==, etc.)
[2024-07-02 Tue 21:19]
- ==\034==, ==\035== are ASCII octal values for non printable characters usually used as special default characters to separate data into strings
  + find references here :: https://donsnotes.com/tech/charsets/ascii.html
- eg ::
  #+begin_example
  BEGIN { FS="\034"; RS="\035"; OFS="," }
  { gsub( /\n/, "" )
    $1=$1
    # If last field is empty, remove it.
    if ( ""==$NF )  NF--
    print
  }
  #+end_example
- why ? ::
  + usecase :: awk multidimensional arrays
    - awk treats multidimensional arrays as a one dimensional arrays where the index is a sequence of indexes separated by a special separator ==SUBSEP==
      + eg: awk stores ~grid[x,y]=value~ with SUBSEP ==@== as ~grid["x@y"]=value~
    - more details: https://www.gnu.org/software/gawk/manual/html_node/Multidimensional.html
  + however, some separators such as ==@== might be used somewhere in the data or the awk program and then lead to ambiguities
    - eg: ~foo["a@b", "c"]~ and ~foo["a", "b@c"]~ are stored as ~foo["a@b@c"]~,awk is not able to tell one from the other if SUBSEP is ==@==
  + thus the use of these non printable characters that are unlikely to appear in an awk program or in most input data
*** shell commands
[2024-06-23 Sun 19:39]
- ==local== ::
  + ~local [option] name[=value] ...~ :: create local variable ==NAME==, and with value ==VALUE==
  + ==...== means that previous argument can be repeated
  + eg: ~local cword words=()~ defines scalar variable ==cword== and empty array ==words==
- set vs export ::
*** file read/write
[2024-07-03 Wed 20:43]
- read file ::
  + eg ::
   #+begin_example
   #!/bin/bash
   INFILE=/root/names.txt
   
   # with while loop
   while read -r col1 col2
   do
     printf '%s - %s' "$col1" "$col2"
   done < "$INFILE"

   # with for loop
   IFS=$'\n' # set the Internal Field Separator to newline
   for LINE in $(cat "$INFILE")
   do
       echo "$LINE"
   done
   #+end_example
  + -r :: option prevents backslashes in the input from being interpreted as escape characters: "Hello\nWorld" is then read as is, not as two lines

*** Arrays
[2024-06-23 Sun 19:38]
- arrays and environment variables ::
  + array can not be an array of environment variables as environment variables may only be key-value string pairs
  + however, as done for $PATH, the trick is to turn array into a string delimited with a character not otherwise present in the values:
    #+begin_example
    $ arr=( aa bb cc "some string" )
    $ arr=$( printf '%s:' "${arr[@]}" )
    # ~printf '%s\n' "$arr"~ gives "aa:bb:cc:some string:"
    #+end_example
* args and xargs
** Notes
- use :: passing arguments from standard input to other commands. Whereas some commands allow piping (grep for example), others don't (find, ls, or echo for example: can't ~seq 5 | echo~, need to pipe to xargs first: ~seq 5 | xargs echo~)
- piping to xargs with no following argument, defaults to echo
- can be faster as some commands, for example, piping to 'find' through xargs can be faster than using the -exec command of find: ~time (find -iname '*.org' | xargs grep -rail 'file:')~ < ~time (find -iname '*.org' -exec grep -rail 'file:' {} \;)~
- examples ::
  + ~cut -d: -f1 < /etc/passwd | sort | xargs~ :: print sorted first field of each line in file /etc/passwd

** Some options
- -t :: print complete command performed before printing the result
- -I [replace-str] :: replace occurrences of replace-str in the initial-arguments with names read from standard input, examples:
  + ~ls ~/Documents | xargs -I {} echo "mamamia/{}"~, where =={}== denote the arguments for xargs (the output of the first command): will print with replaced (added in this case) occurrences of input, eg: mamamia/[adocumentinfolderDocuments]
  + ~seq 1000 | xargs -I {} touch {}.txt~: create 1000 text files with name n.txt
  + ~ls | cut -d. -f1 | xargs -I {} mv {}.txt {}.text~: rename .txt extension files to .text, where the delimiter used is the dot, and considering that the files have only one dot in their names, and that it is used for the extension
- -n [max-args] :: use at most max-args arguments per command line: will run the following command by group of max-args arguments until all arguments are done, eg: ~ls | xargs -t -n 2~ prints the following command (xargs defaults: echo), and then its results (suppose folder contains 5 files: file1.txt...file5.txt):
  #+begin_example
  echo file1.txt file2.txt
  file1.txt file2.txt
  echo file3.txt file4.txt
  file3.txt file4.txt
  echo file5.txt
  file5.txt
  #+end_example
- -P :: set number of processes to run together, eg:
  + ~seq 10 | xargs -n 1 -P 3 bash -c 'echo $0; sleep 2'~: takes 1 arg at the time, executes 3 bash processes, and then sleeps for 2secs. The 10 numbers displayed may not be in order in the final display, but will be by groups of 3, eg: ==213-456-798-10== ('-' is the 2secs delay)
    - differs from ~seq 10 | xargs -n 3 -P 1 bash -c 'echo $0; sleep 2'~ that takes in 3 args, but only print the first element of the bunch, eg: ==1-4-7-10==
    - a last one for the road: ~seq 10 | xargs -n 3 -P 2 bash -c 'echo $0; sleep 2'~: ==14-710==
* text processing with scripting tools: sed, awk, perl, and python
** Notes
- fight! :: sed, awk, perl, python
  + sed :: to apply actions from a script to each line if input files
  + awk :: initially made for formatting reports, but now mainly a "program based on 'patterns matched' and 'actions taken when the pattern matches'"
  + perl :: initially written in part as an awk-killer and sed-killer. Provides access to almost all unix system calls and has great extensibility
  + python :: came after perl (age(sed) > awk > perl > python)
- when use one over another ? ([[https://stackoverflow.com/questions/366980/what-are-the-differences-between-perl-python-awk-and-sed][source]]) :: (given just as example, more than one reasons to use one over another)
  + sed :: when need to do simple text transforms on files
  + awk :: when need simple formatting, summarising, or transformation of data
  + perl :: for almost any task, but especially when task needs complex regular expressions
  + python :: for the same tasks that you could use Perl for
** sed
*** Notes
- sources :: https://www.gnu.org/software/sed/manual/sed.html
*** how sed works (in a nutshell) ::
+ sed maintains two data buffers: the active *pattern space*, and the auxiliary *hold space* (all initially empty)
+ sed operates by performing the following cycle on each line of input:
  1. reads one line from the input stream, removes any trailing newline, and places it in the pattern space
  2. execute the command
     - (commands can have an address associated, and the command is only executed if the condition is verified before the command is to be executed)
  3. when end of the script reached (unless -n option is used), print content of *pattern space* to the output stream
  4. start next cycle for the next input line
+ unless special commands (such as 'D') are used, the pattern space is deleted between two cycles.
  - only the hold space keeps its data between cycles
+ examples of sed commands ::
  - 'b [label]' :: branch unconditionally to [label]. Label may be omitted, in which case the next cycle is started
  - 'c [text]' :: replace (change) lines with [text]
  - 'd' :: delete pattern space
  - 'g' :: replace the contents of the pattern space with the contents of the hold space
    + attention !! :: do not confuse command 'g', with flag 'g'
  - 'i [text]' :: insert [text] before line
  - 's' :: ~s/regexp/replacement/[flags]~, match regular-expression against content of pattern space, if match, then replace
*** 's' command ::
+ some s command sed extensions :: (made of a backslash and one of the letters L, l, U, u, or E)
  - \U :: turn the s replacement to uppercase until a \L or \E is found
  - \u :: turn the next character to uppercase
  - \E :: stop case conversion started by \L or \U
+ 's' command's flags (some) ::
  - g :: apply replacement to all matches
  - e :: allows one to pipe input from a shell command into pattern space
  - i :: or I, matches in a case-insensitive manner
  - p :: if the substitution was made, then print the new pattern space
    + by default, sed prints everything, but with -n option, it suppresses default printing, and together with the p flag, only matching patters are printed
*** sed command options
[2024-06-30 Sun 01:58]
- ~e~ :: add script to be executed
- ~n~ :: suppress automatic printing of pattern space
*** examples ::
1. ~sed s/^\(.\{81\}\).*$/\1/~, to keep first 81 chars (80 + new line). '\1' is referring to first occurrence of pattern
2. ~echo 'a-b-' | sed 's/\(b\?\)-/x\u\1/g'~, outputs: 'axxB'
   - ~/x\u\1~ ::
     1. replace occurrences of 'b-', or just '-' by an x
     2. \u turns the next character to uppercase
     3. the next character being the first referenced set of characters matched (the referenced nth character within \( and its matching \) here being '\1')
        - the character may or may not exist in this case (due to '?': ~\(b\?\)~)
   - g flag is used, so case conversion (uppercase '\u') does not propagate from one occurrence of the regular expression to another, eg: first pattern matched '-' only affects empty replacement of '\1'
   - thus ::
     + first chain matched: '-' => replaced with just x, since \1 is empty
     + second chain matched: 'b-' => replaced by 'xB', since first char is b
3. pretty print folders architecture with sed :: ~ls -R | grep ":$" | sed -e 's/:$//' -e 's/[^-][^\/]*\//──/g' -e 's/─/├/' -e '$s/├/└/'~
   - assume in folder containing sub directories and one file: ~a/b/c/d.txt~
   - ~ls -R~ :: prints folders recursively content until there is no folder to visit:
     #+begin_example
     .:
     b
    
     ./b:
     c
    
     ./b/c:
     c.txt
     #+end_example
   - grep :: will select only folders (ending with ~:$~)
   - sed scripts (-e) :: in input lines returned by grep (only folders, files not included)
     + 's/:$//' :: remove colons
     + 's/[^-][^\/]*\//──/g' :: remove all occurrences of sub strings ending with '/' (and not starting with a '-'), and replace them with '--' (each successions of matches will be replaced by successions of '--', adding more depth to sub folders)
       - [^-][^\/]*\/ :: the inner (~[^\/]*~) to restrict the expressions to omit inner folders (in 'a/b/c/', select a/, b/, and c/, instead of matching the whole string 'a/b/c/' when the inner ~[^\/]*~ is not used) 
       - g :: to apply command on all matches
     + 's/─/├/' :: replace the first '-' with '├', for pretty printing
     + '$s/├/└/' :: further pretty print (notice the '$')
* cat, printf
[2024-06-23 Sun 18:51]
- examples ::
  + print IFS characters ::
    - ~echo "$IFS" | cat -et~: add ==$== at each en of line (-e), and print tabulation as ==^I== (-t)
    - ~printf %q "$IFS"~: ==%q== to escape non-printable characters with POSIX ==$''==
* cron (crontab, cronjob, etc.)
** Notes
+ cron tool format explainer :: https://crontab.cronhub.io/
+ Simply put, cron is a basic utility available on Unix-based systems. It enables users to schedule tasks to run periodically at a specified date/time, without requiring human intervention.
+ Cron runs as a daemon process (needs to be started once and it keeps running in the background). The process makes use of crontab to read the entries of the schedules and kicks off the tasks.
** Working with crontab
+ A cron schedule is a simple text file located under ==/var/spool/cron/crontabs== on Linux systems.
+ crontab files cannot be edited directly, so they need to be accessed using the crontab command.
+ commands ::
  - to open crontab file :: ==crontab -e==
  - a crontab line is an entry with an expression and a command to run ::
    + eg :: the entry ==* * * * * /usr/local/ispconfig/server/server.sh== runs the mentioned script (server.sh) every single minute.
*** Cron Expression
+ the cron expression consists of 5 fields :: ==<minute> <hour> <day-of-month> <month> <day-of-week> <command>== 
**** Special Characters in Cron Expression
+ ==*== (all) specifies that event should happen for every time unit
+ ==?== (any) is used in the <day-of-month> and <day-of-week> fields to denote the arbitrary value and thus neglect the field value. For example, if we want to fire a script at the 5th of every month, irrespective of what day of the week falls on that date, we specify a ==?== in the <day-of-week> field
+ ==–== (range) determines the value range. For example, "10-11" in the <hour> field means "10th and 11th hours"
+ ==,== (values) specifies multiple values. For example, "MON, WED, FRI" in <day-of-week> field means on the days "Monday, Wednesday and Friday"
+ ==/== (increments) specifies the incremental values. For example, a "5/15" in the <minute> field means at "5, 20, 35 and 50 minutes of an hour"
+ ==L== (last) has different meanings when used in various fields. For example, if it’s applied in the <day-of-month> field, it means last day of the month, i.e. 31st of January and so on as per the calendar month.
  - it can be used with an offset value, like =L-3=, which denotes the "third to last day of the calendar month"
  - in <day-of-week>, it specifies the "last day of a week"
  - it can also be used with another value in <day-of-week>, like ==6L==, which denotes the "last Saturday"
+ ==W== (weekday) determines the weekday (Monday to Friday) nearest to a given day of the month. For example, if we specify "10W" in the <day-of-month> field, it means the "weekday near to 10th of that month". So if "10th" is a Saturday, the job will be triggered on "9th" and if "10th" is a Sunday, it will trigger on "11th". If we specify "1W" in <day-of-month> and if "1st" is Saturday, the job will be triggered on "3rd" which is Monday, it will not jump back to the previous month
+ ==#== specifies the "N-th" occurrence of a weekday of the month (<day-of-week>), for example, "third Friday of the month" can be indicated as "5#3"
**** Examples
+ at 12:00 p.m. (noon) every day :: ==0 12 * * ?==
+ every 15 minutes every day :: ==0/15 0 * * ?==
+ using increments to run the job every odd minute :: ==1/2 0 * * ?==
+ every five minutes starting at 1 p.m. and ending at 1:55 p.m. (cron job reference by default is per hour) and then starting at 6 p.m. and ending at 6:55 p.m., every day :: ==0/5 13,18 * * ?==
+ every minute starting at 1 p.m. and ending at 1:05 p.m. (not default here, but rather range), every day :: ==0-5 13 * * ?==
+ at 1:15 p.m. and 1:45 p.m. every Tuesday in the month of June :: ==15,45 13 ? 6 Tue==
+ at 9:30 a.m. every Monday, Tuesday, Wednesday, Thursday and Friday :: ==30 9 ? * MON-FRI==
+ at 6 p.m. on the third to last day of every month :: ==0 18 L-3 * ?==
+ at 10:30 a.m. on the last Thursday of every month :: ==30 10 ? * 4L==
+ at 10 a.m. on the third Monday of every month :: ==0 10 ? * 1#3==
+ at 12 midnight on every 5th day, starting from the 10th until the end of the month :: ==0 0 10/5 * ?==
**** Cron Special Strings
In addition to the fields specified in the cron expression, there’s also support for some special, predefined values that we can use instead of the fields :
+ ==@reboot== :: run once at the startup
+ ==@yearly== or ==@annualy== :: run once a year
+ ==@monthly== :: run once a month
+ ==@weekly== :: run once a week
+ ==@daily== or ==@midnight== :: run once a day
+ ==@hourly== :: run hourly
* curl
[2024-01-29 Mon 20:38]
** Notes
[2024-01-29 Mon 21:17]
- in every HTTP request, there is a method, sometimes called a verb. The most commonly used ones are GET, POST, HEAD and PUT
- normally however you do not specify the method in the command line, but instead the exact method used depends on the specific options you use ::
  + GET is the default,
  + using -d or -F makes it a POST,
  + -I generates a HEAD,
  + and -T sends a PUT.
- some options ::
  + -H :: eg: ~-H "Accept: application/json"~: specifies the HTTP request header, indicating an expected response in JSON format
  + -o :: for output file
** examples
- Get a README file from an FTP server :: curl ftp://ftp.example.com/README
- Get a webpage from a server using port 8000 :: curl http://www.example.com:8000/
- Get all terms matching curl from a dictionary :: curl dict://dict.example.com/m:curl
- Get a file from an SSH server using SFTP :: curl -u username sftp://example.com/etc/issue
- Get a file from an SSH server using SCP using a private key (not password-protected) to authenticate :: curl -u username: --key ~/.ssh/id_rsa scp://example.com/~/file.txt
** Http methods with curl
[2024-01-29 Mon 20:59]
*** Post
- notes ::
  + to send form data, a browser URL encodes it as a series of name=value pairs separated by ampersand (&) symbols. The resulting string is sent as the body of a POST request. To do the same with curl, use the -d (or --data) argument
- simple post :: curl -d 'name=admin&shoesize=12' http://example.com/
- when specifying multiple -d options on the command line, curl concatenates them and insert ampersands in between :: curl -d name=admin -d shoesize=12 http://example.com/
- if the amount of data to send is too large for a mere string on the command line, can read it from a filename in standard curl style :: curl -d @filename http://example.com
- content-type ::
  + POSTing with curl's -d option makes it include a default header that looks like ~Content-Type: application/x-www-form-urlencoded~. That is what your typical browser uses for a plain POST.
  + if that header is not good enough for you, you should, of course, replace that and instead provide the correct one. Such as if you POST JSON to a server and want to more accurately tell the server about what the content is: ~curl -d '{json}' -H 'Content-Type: application/json' https://example.com~
- json :: 
  + curl 7.82.0 introduced the --json option as a new way to send JSON formatted data to HTTP servers using POST. This option works as a shortcut and provides a single option that replaces these three ::
    1. --data [arg]
    2. --header "Content-Type: application/json"
    3. --header "Accept: application/json"
  + the option does not make curl actually understand or know about the JSON data it sends, but makes it easier to send it. curl does not touch or parse the data that it sends, so you need to make sure it is valid JSON yourself
  + can use multiple --json options on the same command line. This makes curl concatenate the contents from the options and send all data in one go to the server. Note that the concatenation is plain text based and it does not merge the JSON objects as per JSON 
  + receiving json ::
    - curl itself does not know or understand the contents it sends or receives, including when the server returns JSON in its response. Using a separate tool for the purpose of parsing or pretty-printing JSON responses might make things easier for you, and one tool in particular that might help you accomplish this is 'jq'. example:
      + send a basic JSON object to a server, and pretty-print the JSON response :: curl --json '{"tool": "curl"}' https://example.com/ | jq
      + send the JSON with jo, print the response with jq :: jo -p name=jo n=17 | curl --json @- https://example.com/ | jq
  + examples ::
    - send a basic JSON object to a server :: curl --json '{"tool": "curl"}' https://example.com/
    - send JSON from a local file :: curl --json @json.txt https://example.com/
    - send JSON passed to curl on stdin :: echo '{"a":"b"}' | curl --json @- https://example.com/
    - send JSON from a file and concatenate a string to the end :: curl --json @json.txt --json ', "end": "true"}' https://example.com/
* cut
* git
[2024-05-31 Fri 22:58]
** git worktrees (work with different "branches" at the time)
- allows switching between different works in progress (WIP) and branches/commits/etc.
  + instead of usual check out, for which can't have different branches at once, worktree allows checking out on different branches at once. Each working tree is checked out in different folders
  + worktree is somehow similar to cloning the same repository in different folders, but with worktree all work-trees are still linked to the same clone
*** git worktree basic commands (some)
[2024-05-31 Fri 23:34]
- notes ::
  + as always with git, there are plenty of ways of doing the same thing
- create worktree from current branch :: and have two different branches of the same clone at once
  + ~git worktree add [path-to-new-worktree] [target-branch-to-check-out-to]~
    - eg: ~git worktree add ../new-worktree current-or-other-branch-or-commit~
    - ~new-worktree~ will be a new folder with current WIP snapshot, and parallel to the current repo (~../~)
    - ~current-or-other-branch-or-commit~ will be the target commit/branch after creating the new worktree
    - ~new-worktree~ will not have any ~.git/~ within, but instead a ~.git~ file
      + ~.git~ points to the original ~.git/~ directory in its parent clone => all git commands will work with it, as well as its parent
  + as for git switch, can even use remote branch to switch to after adding worktree: eg:  ~git worktree add ../new-worktree a-remote/a-remote-branch -b name-of-local-from-remote-branch~
    - means 
- remove worktree ::
  + ~git worktree remove [path-to-worktree-from-within-parent]~
    - if still has change, unless use ==--force==, git will block the worktree deletion
- main con of worktree :: can't check out on the same branch for more than one worktree at the time, get an error
** git subtrees and git submodules
[2024-06-04 Tue 21:58]
*** git submodules vs git subtrees
[2024-06-04 Tue 21:59]
- subtrees and submodules allow nesting subprojects within projects, useful for example when subprojects are dependencies of "super" projects
- submodules are pointers to commits, usually in another repositories
- subtrees are directories in the "main" repositories
- when use one over another ? ::
  + project has dependency updated in another repo? :: => submodules
  + project has dependency maintained in same repo? :: => subtrees
  + want to push a subdirectory, not the whole directory, to a branch on the same repo? :: => subtrees, but can work with worktrees ([[https://nicolevanderhoeven.com/blog/20210302-presentation-slides-as-code/][checkout this]])
*** git submodules
[2024-06-04 Tue 21:36]
- allows tracking version history of external submodules into parent repo
- with submodules it's possible to lock the version in its parent (no git submodule update)
- the same way a git repo has a .git folder, when it has submodules, there is a .gitmodules file added to it
- link submodule to current repo (of course current has to be a valid repo) : ~git submodule add [link-to-existing-submodule-repo]~
- it's possible to have nested submodules, and in that case to update from the "super" parent, use: ~git submodule update –init –recursive~
- clone repo with submodules, eg:
  #+begin_example
  git clone /url/to/repo/with/submodules
  # git submodule init: copies mapping from .gitmodules file into the local ./.git/config file
  git submodule init
  git submodule update
  #+end_example
  
*** git subtrees
[2024-06-04 Tue 21:56]
* screen (terminal multiplexer)
:PROPERTIES:
:CUSTOM_ID: screen_configs
:END:
[2024-06-22 Sat 19:28]
- Notes ::
  + terminal multiplexer :: tool used to multiplex several pseudoterminal-based login sessions inside a single terminal display/terminal emulator window/PC/workstation system console/remote login session, or to detach and reattach sessions from a terminal
    - some examples: tmux, GNU screen, terminator
  + some main features ::
    - open multiple windows within the screen session
    - disconnect and reconnect Screen without losing the current session
- screen control sequence prefix :: default is: ~C-a~ as in ~C-a [command]~
  + sometimes default control sequence ~C-a~ is used somewhere else (eg: emacs), so it can be changed by setting the command into ==~/.screenrc==
    - eg: to change the default screen prefix to ~C-x~: ~$echo "escape ^Xx" >> ~/.screenrc~
  + since prefix can be changed, from now forward, it will be referred to as ~[screen-prefix]~
  + ~M-1~ as prefix ::
    + source :: https://stackoverflow.com/questions/1543427/gnu-screen-changing-the-default-escape-command-key-to-alt-x
    + ~/.screenrc ::
      #+begin_example
      # trick to use M-1 as the gnu screen prefix (source: https://stackoverflow.com/questions/1543427/gnu-screen-changing-the-default-escape-command-key-to-alt-x)
      # set prefix to "C-@" (or ~C-2~), and also set its "alias": ~M-1~
      escape ^@a
        
      # use screen auxiliary register to save C-a into register S
      register S ^@
        
      # M-1 produces ^A and acts as an escape key
      bindkey "^[1" process S
      #+end_example
- help :: ~[screen-prefix] ?~
- sessions ::
  + launch a session :: ~$screen~
    - start named session :: ~screen -S nameofsession~
  + (de)attach screen session ::
    - disconnect from screen session and keep it running on the host (detach: d) :: ~[screen-prefix] d~
    - resume (-r) connection to session :: ~$screen -r~
    - resume connection to a session that was not disconnected properly due to unstable network for example :: ~$screen -r -d~ (disconnect first from host: -d, and then resume to reconnect)
    - resume session, or create new one if none pending :: ~screen -d -RR~
    - start detach session and run command on it (option -m) :: ~screen -d -m ping 10.10.10.10~
  + kill session ::
    - ~screen -X -S nameofsession quit~ :: -X: send command to alive session, -S: define session name
    - ~screen -X 269 quit~ :: auto-completes if unique
  + rename session ::
    - ~screen -S OLDSESSIONNAME -X sessionname NEWSESSIONNAME~
    - ~[screen-prefix] :sessionname NEW-NAME~ :: in-screen session
- screen windows ::
  + start new window (independent window with its own shell, in same region/) :: ~[screen-prefix] c~
  + move to next/previous window :: ~[screen-prefix] n~ or ~[screen-prefix] <space>~ / ~[screen-prefix] p~
  + move to windows/screen by number :: ~[screen-prefix] [n]~
  + rename current windows :: ~[screen-prefix] A~
  + list all windows and select with cursor :: ~[screen-prefix] "~
  + list all running sessions (screen offline mode) :: ~screen -ls~
  + kill current window :: ~[screen-prefix] K~, or ~exit~, or ~C-d~
- split screens ::
  + horizontal split :: ~[screen-prefix] S~
  + vertical split :: ~[screen-prefix] |~
  + switch between screens :: ~[screen-prefix] <Tab>~
  + close current split/region :: ~[screen-prefix] x~
  + remove all screen region but the current one (doesn't kill windows) :: ~[screen-prefix] Q~
- lookup if in screen session ::
  + ~echo $STY~ :: custom screen environment variable
  + ~echo $TERM~ :: linux terminal environment variable
  + ~[screen-prefix] x~ :: screen will print time, if does then current is a screen session
- copy paste mode ::
  1. enter copy mode :: ~[screen-prefix] [~, or ~[screen-prefix] <Esc>~
  2. select region :: ~<Space>~ + move to end of region to copy + ~<Space>~
  3. paste copy buffer to cursor :: ~[screen-prefix] ]~
  4. leave copy mode :: ~q~
- advanced ::
  + logging ::
    - start logging current session :: ~[screen-prefix] H~
      + generates a file named ~screenlog.n~, where n is the screen session number
      + can also enable logging in detached mode with -L: ~screen -L~
        - customise target file ~screen -L -Logfile /path/to/logfile.txt~
    - screenshot ::
      + make screenshot of what's currently on the screen :: ~[screen-prefix] h~
        - generates a file named ~hardcopy.n~, n being the current window number
          + by default, each time a screenshot is taken it overwrites the previous content of ~hardcopy.n~
            - to append instead of the overwrite, add to configuration file (==screenrc==): ~hardcopy_append on~
            - and to customise target directory for screenshots: ~hardcopydir /your/dir/~
  + lock screen session ::
    - lock current screen session :: ~[screen-prefix] x~ (locks screen with user's password)
- examples ::
  + monitor the outputs of a detached session :: 
    - ~screen -dmS test -L~ :: start screen session named 'test' (-S defines session name), but start it in detached mode (-dm), and enable logging (-L)
      + can then attach to session with ~screen -r test~, but here want to do it from another session
    - ~screen -S test -X colon "logfile flush 0^M"~ :: enable real-time logging:
      + send command to an alive session (-X)
      + signal the session that a command is being sent (~colon~)
      + set the screen session to fix the log interval time in seconds (~logfile flush~), and value 0 for real-time logging (default is 10secs)
      + execute (enter) sent command with ~C-M~ (~^M~)
    - ~tail -Fn 0 screenlog.0~ :: monitor outputs from detached screen
      + ~-F~ ::  keep watching screen log file for changes
      + ~-n 0~ :: only watch, don’t output older lines
      + ~screenlog.0~ :: .0 since the detached screen is supposed to be the first
    - ~tail -Fn 0 screenlog.0 | ccze | grep -i error~ :: watch for errors using ccze (logs colorizer)
* showkey : useful for keybindings when whants to get text version of keys from keyboard
  - find mappings :: showkey -a
* other nice utilities
- jq and json, yq and yaml :: yaml files parser and processor
  + yaml ::
    - format ::
      + yaml file has start (~---~) and end (~...~) line indicators (just convention)
      + comments :: start with hash symbol (#)
    - structure ::
      + structure of a YAML file is a map or a list, it follows a hierarchy depending on the indentation
      + mapping blocks ::
        - maps allow key-value pairs association
        - yaml maps need to be resolved before it can be closed, and a new map created
          #+begin_example
          # KO!
          akey: some value
            a_wrong_new_map: will generate an error
          # OK!
          akey: # no value to allow creation of a new embedded map
            a_correct_new_map: this one will not generate an error
          #+end_example
        - avoid flat mapping when dealing with lists:
          + eg:
            #+begin_example
            Store: Bakery
              - Sourdough loaf
              - Bagels
            #+end_example
          + yaml is valid, but flat and somehow ambiguous since json counterpart is =={“Store”: “Bakery - Sourdough loaf - Bagels”}==
          + furthermore, ==store== mapping block is never closed
          + better version:
            #+begin_example
            Store:
              Bakery
                - Sourdough loaf
                - Bagels
            #+end_example      
      + each key must be unique, order doesn't matter
      + a new map can be created by either increasing the indentation or by resolving previous maps and starting an adjacent map
      + lists ::
        - lists include values listed in a specific order
        - a list sequence starts with a dash (-) and a space, while indentation separates it from the parent
        - embedded lists: (json: ~[“Flour”, “Salt”, “Water”, {“Sugar”: [“caster”, “granulated”, “icing”]}]~)
          #+begin_example
          ---
          - Flour
          - Water
          - Salt
          - Sugar:
              - caster
              - granulated
              - icing
          ...
          #+end_example
  + yaml and json processing with jq ::
    - at time of writing this notes, exists 2 versions of yq: in go, and in python
    - yq ::
      + yq: Command-line YAML processor, a jq wrapper for YAML documents (converts yaml to json)
      + preserve yaml format in output :: (by defaults no conversion of jq output is done and the output is in json format) use ==--yaml-output/-y== option to convert it back into YAML, eg: ~cat input.yml | yq -y .foo.bar~
    - input example :: apt.yaml, and later on a json example file fruits.json
      #+begin_example
      ---
      note: install with debian like apt tool
      apt:
        command: sudo apt install
        pkgs:
          - git
          - tree
          - curl  # likely already installed, but do it anyway
          - screen  # GNU screen, multiplexer
          - g++
          - python
          - yamllint  # yaml file linter
          - ccze  # log files coloriser
          - wget
      dpkg:
        command: ""
        pkgs:
          - ""
      jtodo:
        - java jdk (basic one: default-jre), valgrind
      ...
      #+end_example
    - use ::
      + prettify :: ~echo '{"fruit":{"name":"apple","color":"green","price":1.20}}' | jq '.'~
      + json to yaml ::
        #+begin_example
        $cat << EOF > fruits.json | yq -y
        [
          {
            "name": "apple",
            "color": "green",
            "price": 1.2
          },
          {
            "name": "banana",
            "color": "yellow",
            "price": 0.5
          },
          {
            "name": "kiwi",
            "color": "green",
            "price": 1.25
          }
        ]
        EOF
        - name: apple
          color: green
          price: 1.2
        - name: banana
          color: yellow
          price: 0.5
        - name: kiwi
          color: green
          price: 1.25
        #+end_example
      + accessing properties ::
        - access property values with ==.field== operator: filter operator + property name
        - brackets (==[]==) provide all values from a structure/array
        - ~$jq '.[] | .name' fruits.json~: ~"apple" "banana" "kiwi"~
          + brackets iterate over content of list, pass each object to next filter using a pipe (==|==), and then outputs 'name' field content
          + same outcome with: ~$ jq '.[].name' fruits.json~
        - ~$cat configs/basic/apt.yaml | yq '.apt.pkgs[]'~ (or ~yq '.apt.pkgs[]' configs/basic/apt.yaml~)
          + results:
            #+begin_example
            "git"
            "tree"
            "curl"
            "screen"
            "g++"
            "python"
            "yamllint"
            "ccze"
            "wget"          
            #+end_example
        - ~$yq '.apt.command' configs/basic/apt.yaml~ :: ~"sudo apt install"~
        - fetch multiple keys :: use comma as separator: ~yq '.apt.pkgs,.apt' configs/basic/apt.yaml~
          #+begin_example
          [
            "git",
            "tree",
            "curl",
            "screen",
            "g++",
            "python",
            "yamllint",
            "ccze",
            "wget"
          ]
          {
            "command": "sudo apt install",
            "pkgs": [
              "git",
              "tree",
              "curl",
              "screen",
              "g++",
              "python",
              "yamllint",
              "ccze",
              "wget"
            ]
          }
          #+end_example
        - access by index :: ~$ jq '.[1].name' fruits.json~: "banana"
        - slicing (subarray) ::
          + ~echo '[1,2,3,4,5,6,7,8,9,10]' | jq '.[6:9]'~ : ~[7,8,9]~
          + ~echo '[1,2,3,4,5,6,7,8,9,10]' | jq '.[:6]' | jq '.[-2:]'~ : ~[5,6]~
        - ~$yq '.apt.pkgs[] | select(. == "screen")' configs/basic/apt.yaml~ :: ~screen~
      + built-in functions ::
        - keys (and 'value') :: ~jq '.[] | keys' fruits.json~ : ~["color", "name", "price"]~ 3 times
        - length ::
          + ~jq '.[].name | length' fruits.json~ : ~5,6,4~, the length of 'name' contents
          + ~jq '.[] | length' fruits.json~ : ~3,3,3~, the number of properties in each object
        - map ::
          + handy when filtering needed to then apply further operations on selected values
          + ~jq 'map(has("name"))' fruits.json~: keep objects in list that have property 'name'
          + ~jq 'map(.price+2)' fruits.json~: increase content of property 'price' by 2
        - min/max ::
          + ~jq '[.[].price] | max' fruits.json~ : returns the price of the most expensive fruit
        - min_by/max_by ::
          + returns *object* that matches the given min/max condition
          + ~jq 'max_by(.price)' tmp.json~: ~{"name": "kiwi", "color": "green", "price": 1.25}~
        - to_entries[] :: convert each object in the array into an array of key-value pairs
        - select ::
          + for searching for values
          + ~jq '.[] | select(.color=="yellow" and .price>=0.5)' fruits.json~
          + ~jq '.[] | to_entries[] | select(.key | startswith("name")) | .value' fruits.json~ : ~"apple" "banana" "kiwi"~
        - unique ::
          + find unique occurrences of a value, or remove duplicates
          + ~jq 'map(.color) | unique' fruits.json~
        - del ::
          + remove keys/values from the outputs
          + ~jq 'del(.[].name)' fruit.json~
        - support for regex (with 'test' function) ::
          + tests if an input matches against a given regular expression
          + ~jq '.[] | select(.name|test("^a.")) | .price' fruits.json~
      + data transformation with jq ::
        - pages.json ::
          #+begin_example
          {
            "query": {
              "pages": [
                {
                  "21721040": {
                    "pageid": 21721040,
                    "ns": 0,
                    "title": "Stack Overflow",
                    "extract": "Some interesting text about Stack Overflow"
                  }
                },
                {
                  "21721041": {
                    "pageid": 21721041,
                    "ns": 0,
                    "title": "Baeldung",
                    "extract": "A great place to learn about Java"
                  }
                }
              ]
            }
          }
          #+end_example
        - command :: ~jq '.query.pages | [.[] | map(.) | .[] | {page_title: .title, page_description: .extract}]' pages.json~
          + ~.[] | map(.) | .[]~:
            1. take all content of .query.pages
            2. map it <=> turn pages objects into arrays of their content: in the original structure, the pages are objects identified by their ISBN, getting their content leaves out the identifier/key and keeps only the content
            3. take content => 1-3: transform original pages with ISBNs as keys into just their content
        - results ::
          #+begin_example
          [
            {
              "page_title": "Stack Overflow",
              "page_description": "Some interesting text about Stack Overflow"
            },
            {
              "page_title": "Baeldung",
              "page_description": "A great place to learn about Java"
            }
          ]
          #+end_example
      + going further ::
        - cookbook :: https://github.com/jqlang/jq/wiki/Cookbook
        - official documentation :: https://jqlang.github.io/jq/manual/
        - cheatsheet :: https://gist.github.com/olih/f7437fb6962fb3ee9fe95bda8d2c8fa4
- time [command] :: times how long a command takes, eg: ~time (find -iname '*.org' | xargs -I {} basename {})~
  + has even formatting options, eg : ~time -f '%E %W %r' [command]~
    - to get a better format for zsh as for bash, set variable TIMEFMT to ~TIMEFMT=$'real\t%E\nuser\t%U\nsys\t%S'~
  + for longer commands, can either use braces (~time { ls; pwd; whoami; }~), or use a subshell (~time (ls; pwd)~)
- tree [path-to-folder-to-visualise] :: to visualise directories structures
  + Some options ::
    - -a :: include hidden files
    - -d :: list folders only
  + HOWEVER :: can do basic folders' list formatting with sed scripting, examples ::
    - with grep and sed :: ~ls -R | grep ":$" | sed -e '"'"'s/:$//'"'"' -e '"'"'s/[^-][^\/]*\//--/g'"'"' -e '"'"'s/^/   /'"'"' -e '"'"'s/-/|/'"'"~
    - with grep and perl :: ~ls -aR | grep ":$" | perl -pe 's/:$//;s/[^-][^\/]*\//    /g;s/^    (\S)/└── \1/;s/(^    |    (?= ))/│   /g;s/    (\S)/└── \1/'~
    
  

