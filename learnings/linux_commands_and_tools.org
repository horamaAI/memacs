# -*- mode: org -*-
#+title: Linux (not only) commands/tools that are fun and handy
#+SETUPFILE: ~/set-up-files/basic-setups.org

* shell and shell scripting
[2024-05-26 Sun 21:56]
** Notes
[2024-06-01 Sat 16:28]
- cheatsheet :: https://devhints.io/bash, examples:
  + shell arguments ::
    - $# :: number of arguments
    - $* :: all positional arguments (as a single word)
    - $@ :: all positional arguments (as separate strings)
    - $1 :: first argument
    - $_ :: last argument of the previous command
  + special variables ::
    - $? :: exit status of last task
    - $! :: PID of last background task
    - $$ :: PID of shell
    - $0 :: filename of the shell script, or name of current script
    - $_ :: last argument of the previous command
    - ${PIPESTATUS[n]} :: return value of piped commands (array)
** shell types and shell startup configs
*** notes
- [non-]interactive shells and [non-]login shells :: there are two kinds of shells:
  + [non-]interactive shells :: you type into them (shell scripts)
  + [non-]login shells :: the shell run them when you first login (subshells)
- shells execution order ::
  + all shells will first run ==env==,
  + then login shells will run ==login==,
  + then interactive shells will run ==interactive==,
  + once finished, login shells will run ==logout==
- managing multiple shells :: when using more than one shell, always remember to manage files with respect to their specific shell and specify things which any POSIX-compliant shell (like aliases and environment variables) can understand in a common startup file
  + dotfiles :: when organising things in dotfiles for example, a solution would be to organise dotfiles folders, one for each shell (==.bash/, .zsh/== and ==.sh/==), and one for shell-independent files (==.shell/==):
    #+begin_example
    .bash/
        env
        interactive
        login
        logout
    .sh/
        env
        interactive
        login
    .shell/
        env
        interactive
        login
        logout
    .zsh/
        env
        interactive
        login
        logout
    #+end_example
*** where to put configs
[2024-05-27 Mon 19:56]
- deciding when action needs to be run:
  + does the command set/modify environment variables ? :: then ==login==
  + is it an alias, or a terminal specific environment variable (eg: $GREP_COLOR) ? :: then ==interactive==
  + ==env== :: for useful functions and other tools, eg: ~umask~, or modifying $PATH, for example to filter out duplicate
*** implementing shell configs
[2024-05-27 Mon 22:28]
- notes ::
  + fortunately [[https://blog.flowblok.id.au/2013-02/shell-startup-scripts.html][flowblok]] did a wonderful job of following the flow of startup files and help in finding which level of the shell these config files have to be implemented:
    #+CAPTION: shell startup flow
    #+NAME: fig:shell-startup-actual
    #+ATTR_ORG: :width 500
    #+ATTR_HTML: :width 300px
    [[../images/shell-startup-actual.png]]
** shell scripting
[2024-06-23 Sun 02:07]
*** Notes
*** shell special variables
[2024-06-23 Sun 03:45]
- IFS (Internal Field Separator) ::
  + IFS :: variable used by the shell to determines word boundaries while splitting a sequence of character strings
    - default values: at least three-characters string: space, tab, and newline
  + temporarily use different IFS value ::
    - eg: iterate over content of $PATH:
      #+begin_example
      buff="$IFS"
      IFS=:
      (
          for ele in $PATH[@]; do
      	echo "content: $ele"
          done
          )
      IFS="$buff"
      #+end_example
  + IFS and ==read== command ::
    - eg: read from a csv file (comma separated): read the 2 last lines only (tail -n +2)
      #+begin_example
      #!/bin/bash
      while IFS="," read -r rec_column1 rec_column2 rec_column3
      do
       echo "Displaying Record-$rec_column1"
       echo "Quantity: $rec_column2"
       echo "Price: $rec_column3"
       echo ""
      done < <(tail -n +2 input.csv)
      #+end_example
  
*** shell commands
[2024-06-23 Sun 19:39]
- ==local== ::
  + ~local [option] name[=value] ...~ :: create local variable ==NAME==, and with value ==VALUE==
  + ==...== means that previous argument can be repeated
  + eg: ~local cword words=()~ defines scalar variable ==cword== and empty array ==words==
- set vs export ::
*** Arrays
[2024-06-23 Sun 19:38]
- arrays and environment variables ::
  + array can not be an array of environment variables as environment variables may only be key-value string pairs
  + however, as done for $PATH, the trick is to turn array into a string delimited with a character not otherwise present in the values:
    #+begin_example
    $ arr=( aa bb cc "some string" )
    $ arr=$( printf '%s:' "${arr[@]}" )
    # ~printf '%s\n' "$arr"~ gives "aa:bb:cc:some string:"
    #+end_example
* args and xargs
** Notes
- use :: passing arguments from standard input to other commands. Whereas some commands allow piping (grep for example), others don't (find, ls, or echo for example: can't ~seq 5 | echo~, need to pipe to xargs first: ~seq 5 | xargs echo~)
- piping to xargs with no following argument, defaults to echo
- can be faster as some commands, for example, piping to 'find' through xargs can be faster than using the -exec command of find: ~time (find -iname '*.org' | xargs grep -rail 'file:')~ < ~time (find -iname '*.org' -exec grep -rail 'file:' {} \;)~
- examples ::
  + ~cut -d: -f1 < /etc/passwd | sort | xargs~ :: print sorted first field of each line in file /etc/passwd

** Some options
- -t :: print complete command performed before printing the result
- -I [replace-str] :: replace occurrences of replace-str in the initial-arguments with names read from standard input, examples:
  + ~ls ~/Documents | xargs -I {} echo "mamamia/{}"~, where =={}== denote the arguments for xargs (the output of the first command): will print with replaced (added in this case) occurrences of input, eg: mamamia/[adocumentinfolderDocuments]
  + ~seq 1000 | xargs -I {} touch {}.txt~: create 1000 text files with name n.txt
  + ~ls | cut -d. -f1 | xargs -I {} mv {}.txt {}.text~: rename .txt extension files to .text, where the delimiter used is the dot, and considering that the files have only one dot in their names, and that it is used for the extension
- -n [max-args] :: use at most max-args arguments per command line: will run the following command by group of max-args arguments until all arguments are done, eg: ~ls | xargs -t -n 2~ prints the following command (xargs defaults: echo), and then its results (suppose folder contains 5 files: file1.txt...file5.txt):
  #+begin_example
  echo file1.txt file2.txt
  file1.txt file2.txt
  echo file3.txt file4.txt
  file3.txt file4.txt
  echo file5.txt
  file5.txt
  #+end_example
- -P :: set number of processes to run together, eg:
  + ~seq 10 | xargs -n 1 -P 3 bash -c 'echo $0; sleep 2'~: takes 1 arg at the time, executes 3 bash processes, and then sleeps for 2secs. The 10 numbers displayed may not be in order in the final display, but will be by groups of 3, eg: ==213-456-798-10== ('-' is the 2secs delay)
    - differs from ~seq 10 | xargs -n 3 -P 1 bash -c 'echo $0; sleep 2'~ that takes in 3 args, but only print the first element of the bunch, eg: ==1-4-7-10==
    - a last one for the road: ~seq 10 | xargs -n 3 -P 2 bash -c 'echo $0; sleep 2'~: ==14-710==
* text processing with scripting tools: sed, awk, perl, and python
** Notes
- fight! :: sed, awk, perl, python
  + sed :: to apply actions from a script to each line if input files
  + awk :: initially made for formatting reports, but now mainly a "program based on 'patterns matched' and 'actions taken when the pattern matches'"
  + perl :: initially written in part as an awk-killer and sed-killer. Provides access to almost all unix system calls and has great extensibility
  + python :: came after perl (age(sed) > awk > perl > python)
- when use one over another ? ([[https://stackoverflow.com/questions/366980/what-are-the-differences-between-perl-python-awk-and-sed][source]]) :: (given just as example, more than one reasons to use one over another)
  + sed :: when need to do simple text transforms on files
  + awk :: when need simple formatting, summarising, or transformation of data
  + perl :: for almost any task, but especially when task needs complex regular expressions
  + python :: for the same tasks that you could use Perl for
** sed
*** Notes
- sources :: https://www.gnu.org/software/sed/manual/sed.html
*** how sed works (in a nutshell) ::
+ sed maintains two data buffers: the active *pattern space*, and the auxiliary *hold space* (all initially empty)
+ sed operates by performing the following cycle on each line of input:
  1. reads one line from the input stream, removes any trailing newline, and places it in the pattern space
  2. execute the command
     - (commands can have an address associated, and the command is only executed if the condition is verified before the command is to be executed)
  3. when end of the script reached (unless -n option is used), print content of *pattern space* to the output stream
  4. start next cycle for the next input line
+ unless special commands (such as 'D') are used, the pattern space is deleted between two cycles.
  - only the hold space keeps its data between cycles
+ examples of sed commands ::
  - 'b [label]' :: branch unconditionally to [label]. Label may be omitted, in which case the next cycle is started
  - 'c [text]' :: replace (change) lines with [text]
  - 'd' :: delete pattern space
  - 'g' :: replace the contents of the pattern space with the contents of the hold space
    + attention !! :: do not confuse command 'g', with flag 'g'
  - 'i [text]' :: insert [text] before line
  - 's' :: ~s/regexp/replacement/[flags]~, match regular-expression against content of pattern space, if match, then replace
*** 's' command ::
+ some s command sed extensions :: (made of a backslash and one of the letters L, l, U, u, or E)
  - \U :: turn the s replacement to uppercase until a \L or \E is found
  - \u :: turn the next character to uppercase
  - \E :: stop case conversion started by \L or \U
+ 's' command's flags (some) ::
  - g :: apply replacement to all matches
  - e :: allows one to pipe input from a shell command into pattern space
  - i :: or I, matches in a case-insensitive manner
*** sed command options
[2024-06-30 Sun 01:58]
- ~e~ :: add script to be executed
- ~n~ :: suppress automatic printing of pattern space
*** examples ::
1. ~sed s/^\(.\{81\}\).*$/\1/~, to keep first 81 chars (80 + new line). '\1' is referring to first occurrence of pattern
2. ~echo 'a-b-' | sed 's/\(b\?\)-/x\u\1/g'~, outputs: 'axxB'
   - ~/x\u\1~ ::
     1. replace occurrences of 'b-', or just '-' by an x
     2. \u turns the next character to uppercase
     3. the next character being the first referenced set of characters matched (the referenced nth character within \( and its matching \) here being '\1')
        - the character may or may not exist in this case (due to '?': ~\(b\?\)~)
   - g flag is used, so case conversion (uppercase '\u') does not propagate from one occurrence of the regular expression to another, eg: first pattern matched '-' only affects empty replacement of '\1'
   - thus ::
     + first chain matched: '-' => replaced with just x, since \1 is empty
     + second chain matched: 'b-' => replaced by 'xB', since first char is b
3. pretty print folders architecture with sed :: ~ls -R | grep ":$" | sed -e 's/:$//' -e 's/[^-][^\/]*\//──/g' -e 's/─/├/' -e '$s/├/└/'~
   - assume in folder containing sub directories and one file: ~a/b/c/d.txt~
   - ~ls -R~ :: prints folders recursively content until there is no folder to visit:
     #+begin_example
     .:
     b
    
     ./b:
     c
    
     ./b/c:
     c.txt
     #+end_example
   - grep :: will select only folders (ending with ~:$~)
   - sed scripts (-e) :: in input lines returned by grep (only folders, files not included)
     + 's/:$//' :: remove colons
     + 's/[^-][^\/]*\//──/g' :: remove all occurrences of sub strings ending with '/' (and not starting with a '-'), and replace them with '--' (each successions of matches will be replaced by successions of '--', adding more depth to sub folders)
       - [^-][^\/]*\/ :: the inner (~[^\/]*~) to restrict the expressions to omit inner folders (in 'a/b/c/', select a/, b/, and c/, instead of matching the whole string 'a/b/c/' when the inner ~[^\/]*~ is not used) 
       - g :: to apply command on all matches
     + 's/─/├/' :: replace the first '-' with '├', for pretty printing
     + '$s/├/└/' :: further pretty print (notice the '$')
* cat, printf
[2024-06-23 Sun 18:51]
- examples ::
  + print IFS characters ::
    - ~echo "$IFS" | cat -et~: add ==$== at each en of line (-e), and print tabulation as ==^I== (-t)
    - ~printf %q "$IFS"~: ==%q== to escape non-printable characters with POSIX ==$''==
* cron (crontab, cronjob, etc.)
** Notes
+ cron tool format explainer :: https://crontab.cronhub.io/
+ Simply put, cron is a basic utility available on Unix-based systems. It enables users to schedule tasks to run periodically at a specified date/time, without requiring human intervention.
+ Cron runs as a daemon process (needs to be started once and it keeps running in the background). The process makes use of crontab to read the entries of the schedules and kicks off the tasks.
** Working with crontab
+ A cron schedule is a simple text file located under ==/var/spool/cron/crontabs== on Linux systems.
+ crontab files cannot be edited directly, so they need to be accessed using the crontab command.
+ commands ::
  - to open crontab file :: ==crontab -e==
  - a crontab line is an entry with an expression and a command to run ::
    + eg :: the entry ==* * * * * /usr/local/ispconfig/server/server.sh== runs the mentioned script (server.sh) every single minute.
*** Cron Expression
+ the cron expression consists of 5 fields :: ==<minute> <hour> <day-of-month> <month> <day-of-week> <command>== 
**** Special Characters in Cron Expression
+ ==*== (all) specifies that event should happen for every time unit
+ ==?== (any) is used in the <day-of-month> and <day-of-week> fields to denote the arbitrary value and thus neglect the field value. For example, if we want to fire a script at the 5th of every month, irrespective of what day of the week falls on that date, we specify a ==?== in the <day-of-week> field
+ ==–== (range) determines the value range. For example, "10-11" in the <hour> field means "10th and 11th hours"
+ ==,== (values) specifies multiple values. For example, "MON, WED, FRI" in <day-of-week> field means on the days "Monday, Wednesday and Friday"
+ ==/== (increments) specifies the incremental values. For example, a "5/15" in the <minute> field means at "5, 20, 35 and 50 minutes of an hour"
+ ==L== (last) has different meanings when used in various fields. For example, if it’s applied in the <day-of-month> field, it means last day of the month, i.e. 31st of January and so on as per the calendar month.
  - it can be used with an offset value, like =L-3=, which denotes the "third to last day of the calendar month"
  - in <day-of-week>, it specifies the "last day of a week"
  - it can also be used with another value in <day-of-week>, like ==6L==, which denotes the "last Saturday"
+ ==W== (weekday) determines the weekday (Monday to Friday) nearest to a given day of the month. For example, if we specify "10W" in the <day-of-month> field, it means the "weekday near to 10th of that month". So if "10th" is a Saturday, the job will be triggered on "9th" and if "10th" is a Sunday, it will trigger on "11th". If we specify "1W" in <day-of-month> and if "1st" is Saturday, the job will be triggered on "3rd" which is Monday, it will not jump back to the previous month
+ ==#== specifies the "N-th" occurrence of a weekday of the month (<day-of-week>), for example, "third Friday of the month" can be indicated as "5#3"
**** Examples
+ at 12:00 p.m. (noon) every day :: ==0 12 * * ?==
+ every 15 minutes every day :: ==0/15 0 * * ?==
+ using increments to run the job every odd minute :: ==1/2 0 * * ?==
+ every five minutes starting at 1 p.m. and ending at 1:55 p.m. (cron job reference by default is per hour) and then starting at 6 p.m. and ending at 6:55 p.m., every day :: ==0/5 13,18 * * ?==
+ every minute starting at 1 p.m. and ending at 1:05 p.m. (not default here, but rather range), every day :: ==0-5 13 * * ?==
+ at 1:15 p.m. and 1:45 p.m. every Tuesday in the month of June :: ==15,45 13 ? 6 Tue==
+ at 9:30 a.m. every Monday, Tuesday, Wednesday, Thursday and Friday :: ==30 9 ? * MON-FRI==
+ at 6 p.m. on the third to last day of every month :: ==0 18 L-3 * ?==
+ at 10:30 a.m. on the last Thursday of every month :: ==30 10 ? * 4L==
+ at 10 a.m. on the third Monday of every month :: ==0 10 ? * 1#3==
+ at 12 midnight on every 5th day, starting from the 10th until the end of the month :: ==0 0 10/5 * ?==
**** Cron Special Strings
In addition to the fields specified in the cron expression, there’s also support for some special, predefined values that we can use instead of the fields :
+ ==@reboot== :: run once at the startup
+ ==@yearly== or ==@annualy== :: run once a year
+ ==@monthly== :: run once a month
+ ==@weekly== :: run once a week
+ ==@daily== or ==@midnight== :: run once a day
+ ==@hourly== :: run hourly
* curl
[2024-01-29 Mon 20:38]
** Notes
[2024-01-29 Mon 21:17]
- in every HTTP request, there is a method, sometimes called a verb. The most commonly used ones are GET, POST, HEAD and PUT
- normally however you do not specify the method in the command line, but instead the exact method used depends on the specific options you use ::
  + GET is the default,
  + using -d or -F makes it a POST,
  + -I generates a HEAD,
  + and -T sends a PUT.
- some options ::
  + -H :: eg: ~-H "Accept: application/json"~: specifies the HTTP request header, indicating an expected response in JSON format
  + -o :: for output file
** examples
- Get a README file from an FTP server :: curl ftp://ftp.example.com/README
- Get a webpage from a server using port 8000 :: curl http://www.example.com:8000/
- Get all terms matching curl from a dictionary :: curl dict://dict.example.com/m:curl
- Get a file from an SSH server using SFTP :: curl -u username sftp://example.com/etc/issue
- Get a file from an SSH server using SCP using a private key (not password-protected) to authenticate :: curl -u username: --key ~/.ssh/id_rsa scp://example.com/~/file.txt
** Http methods with curl
[2024-01-29 Mon 20:59]
*** Post
- notes ::
  + to send form data, a browser URL encodes it as a series of name=value pairs separated by ampersand (&) symbols. The resulting string is sent as the body of a POST request. To do the same with curl, use the -d (or --data) argument
- simple post :: curl -d 'name=admin&shoesize=12' http://example.com/
- when specifying multiple -d options on the command line, curl concatenates them and insert ampersands in between :: curl -d name=admin -d shoesize=12 http://example.com/
- if the amount of data to send is too large for a mere string on the command line, can read it from a filename in standard curl style :: curl -d @filename http://example.com
- content-type ::
  + POSTing with curl's -d option makes it include a default header that looks like ~Content-Type: application/x-www-form-urlencoded~. That is what your typical browser uses for a plain POST.
  + if that header is not good enough for you, you should, of course, replace that and instead provide the correct one. Such as if you POST JSON to a server and want to more accurately tell the server about what the content is: ~curl -d '{json}' -H 'Content-Type: application/json' https://example.com~
- json :: 
  + curl 7.82.0 introduced the --json option as a new way to send JSON formatted data to HTTP servers using POST. This option works as a shortcut and provides a single option that replaces these three ::
    1. --data [arg]
    2. --header "Content-Type: application/json"
    3. --header "Accept: application/json"
  + the option does not make curl actually understand or know about the JSON data it sends, but makes it easier to send it. curl does not touch or parse the data that it sends, so you need to make sure it is valid JSON yourself
  + can use multiple --json options on the same command line. This makes curl concatenate the contents from the options and send all data in one go to the server. Note that the concatenation is plain text based and it does not merge the JSON objects as per JSON 
  + receiving json ::
    - curl itself does not know or understand the contents it sends or receives, including when the server returns JSON in its response. Using a separate tool for the purpose of parsing or pretty-printing JSON responses might make things easier for you, and one tool in particular that might help you accomplish this is 'jq'. example:
      + send a basic JSON object to a server, and pretty-print the JSON response :: curl --json '{"tool": "curl"}' https://example.com/ | jq
      + send the JSON with jo, print the response with jq :: jo -p name=jo n=17 | curl --json @- https://example.com/ | jq
  + examples ::
    - send a basic JSON object to a server :: curl --json '{"tool": "curl"}' https://example.com/
    - send JSON from a local file :: curl --json @json.txt https://example.com/
    - send JSON passed to curl on stdin :: echo '{"a":"b"}' | curl --json @- https://example.com/
    - send JSON from a file and concatenate a string to the end :: curl --json @json.txt --json ', "end": "true"}' https://example.com/
* cut
* git
[2024-05-31 Fri 22:58]
** git worktrees (work with different "branches" at the time)
- allows switching between different works in progress (WIP) and branches/commits/etc.
  + instead of usual check out, for which can't have different branches at once, worktree allows checking out on different branches at once. Each working tree is checked out in different folders
  + worktree is somehow similar to cloning the same repository in different folders, but with worktree all work-trees are still linked to the same clone
*** git worktree basic commands (some)
[2024-05-31 Fri 23:34]
- notes ::
  + as always with git, there are plenty of ways of doing the same thing
- create worktree from current branch :: and have two different branches of the same clone at once
  + ~git worktree add [path-to-new-worktree] [target-branch-to-check-out-to]~
    - eg: ~git worktree add ../new-worktree current-or-other-branch-or-commit~
    - ~new-worktree~ will be a new folder with current WIP snapshot, and parallel to the current repo (~../~)
    - ~current-or-other-branch-or-commit~ will be the target commit/branch after creating the new worktree
    - ~new-worktree~ will not have any ~.git/~ within, but instead a ~.git~ file
      + ~.git~ points to the original ~.git/~ directory in its parent clone => all git commands will work with it, as well as its parent
  + as for git switch, can even use remote branch to switch to after adding worktree: eg:  ~git worktree add ../new-worktree a-remote/a-remote-branch -b name-of-local-from-remote-branch~
    - means 
- remove worktree ::
  + ~git worktree remove [path-to-worktree-from-within-parent]~
    - if still has change, unless use ==--force==, git will block the worktree deletion
- main con of worktree :: can't check out on the same branch for more than one worktree at the time, get an error
** git subtrees and git submodules
[2024-06-04 Tue 21:58]
*** git submodules vs git subtrees
[2024-06-04 Tue 21:59]
- subtrees and submodules allow nesting subprojects within projects, useful for example when subprojects are dependencies of "super" projects
- submodules are pointers to commits, usually in another repositories
- subtrees are directories in the "main" repositories
- when use one over another ? ::
  + project has dependency updated in another repo? :: => submodules
  + project has dependency maintained in same repo? :: => subtrees
  + want to push a subdirectory, not the whole directory, to a branch on the same repo? :: => subtrees, but can work with worktrees ([[https://nicolevanderhoeven.com/blog/20210302-presentation-slides-as-code/][checkout this]])
*** git submodules
[2024-06-04 Tue 21:36]
- allows tracking version history of external submodules into parent repo
- with submodules it's possible to lock the version in its parent (no git submodule update)
- the same way a git repo has a .git folder, when it has submodules, there is a .gitmodules file added to it
- link submodule to current repo (of course current has to be a valid repo) : ~git submodule add [link-to-existing-submodule-repo]~
- it's possible to have nested submodules, and in that case to update from the "super" parent, use: ~git submodule update –init –recursive~
- clone repo with submodules, eg:
  #+begin_example
  git clone /url/to/repo/with/submodules
  # git submodule init: copies mapping from .gitmodules file into the local ./.git/config file
  git submodule init
  git submodule update
  #+end_example
  
*** git subtrees
[2024-06-04 Tue 21:56]
* screen (terminal multiplexer)
:PROPERTIES:
:CUSTOM_ID: screen_configs
:END:
[2024-06-22 Sat 19:28]
- Notes ::
  + terminal multiplexer :: tool used to multiplex several pseudoterminal-based login sessions inside a single terminal display/terminal emulator window/PC/workstation system console/remote login session, or to detach and reattach sessions from a terminal
    - some examples: tmux, GNU screen, terminator
  + some main features ::
    - open multiple windows within the screen session
    - disconnect and reconnect Screen without losing the current session
- basics ::
  + launch a session :: ~$screen~
  + screen control sequence prefix :: ~C-a~ as in ~C-a [command]~
    - sometimes default control sequence ~C-a~ is used somewhere else (eg: emacs), so it can be changed by setting the command into ==~/.screenrc==
      + eg: to change the default screen prefix to ~C-x~: ~$echo "escape ^Xx" >> ~/.screenrc~
    - since prefix can be changed, from now forward, it will be referred to as ~[screen-prefix]~
    - ~M-1~ as prefix ::
      + source :: https://stackoverflow.com/questions/1543427/gnu-screen-changing-the-default-escape-command-key-to-alt-x
      + ~/.screenrc ::
        #+begin_example
        # trick to use M-1 as the gnu screen prefix (source: https://stackoverflow.com/questions/1543427/gnu-screen-changing-the-default-escape-command-key-to-alt-x)
        # set prefix to "C-@" (or ~C-2~), and also set its "alias": ~M-1~
        escape ^@a
        
        # use screen auxiliary register to save C-a into register S
        register S ^@
        
        # M-1 produces ^A and acts as an escape key
        bindkey "^[1" process S
        #+end_example
  + help :: ~[screen-prefix] ?~
- screen and windows ::
  + start new window (independent window with its own shell) :: ~[screen-prefix] c~
  + move to next/previous window :: ~[screen-prefix] n~ / ~[screen-prefix] p~
  + move to windows/screen by number :: ~[screen-prefix] [n]~
  + list all windows and select with cursor :: ~[screen-prefix] "~
  + list all running sessions (screen offline mode) :: ~screen -ls~
  + kill current window :: ~[screen-prefix] K~
- (de)attach screen session ::
  + disconnect from screen session and keep it running on the host :: ~[screen-prefix] d~
  + resume (-r) connection to session :: ~$screen -r~
  + resume connection to a session that was not disconnected properly due to unstable network for example :: ~$screen -r -d~ (disconnect first from host: -d, and then resume to reconnect)
- split screens ::
  + horizontal split :: ~[screen-prefix] S~
  + vertical split :: ~[screen-prefix] |~
  + switch between screens :: ~[screen-prefix] <Tab>~
  + remove all screen region but the current one :: ~[screen-prefix] Q~
- copy paste mode ::
  1. enter copy mode :: ~[screen-prefix] [~, or ~[screen-prefix] <Esc>~
  2. select region :: ~<Space>~ + move to end of region to copy + ~<Space>~
  3. paste copy buffer to cursor :: ~[screen-prefix] ]~
  4. leave copy mode :: ~q~
- advanced ::
  + log files ::
    - start logging current session :: ~[screen-prefix] H~
      + generates a file named ~screenlog.n~, where n is the screen session number
      + can also enable logging in detached mode with -L: ~screen -L~
  + screenshot ::
    - make a screenshot of what's currently on the screen :: ~[screen-prefix] h~
      + generates a file named ~hardcopy.n~
  + lock screen session ::
    - lock current screen session :: ~[screen-prefix] x~ (locks screen with user's password)
- examples ::
  + monitor the outputs of a detached session :: 
    - ~screen -dmS test -L~ :: start screen session named 'test' (-S defines session name), but start it in detached mode (-dm), and enable logging (-L)
      + can then attach to session with ~screen -r test~, but here want to do it from another session
    - ~screen -S test -X colon "logfile flush 0^M"~ :: enable real-time logging:
      + send command to an alive session (-X)
      + signal the session that a command is being sent (~colon~)
      + set the screen session to fix the log interval time in seconds (~logfile flush~), and value 0 for real-time logging (default is 10secs)
      + execute (enter) sent command with ~C-M~ (~^M~)
    - ~tail -Fn 0 screenlog.0~ :: monitor outputs from detached screen
      + ~-F~ ::  keep watching screen log file for changes
      + ~-n 0~ :: only watch, don’t output older lines
      + ~screenlog.0~ :: .0 since the detached screen is supposed to be the first
    - ~tail -Fn 0 screenlog.0 | ccze | grep -i error~ :: watch for errors using ccze (logs colorizer)
* showkey : useful for keybindings when whants to get text version of keys from keyboard
  - find mappings :: showkey -a
* other nice utilities
- time [command] :: times how long a command takes, eg: ~time (find -iname '*.org' | xargs -I {} basename {})~
  + has even formatting options, eg : ~time -f '%E %W %r' [command]~
    - to get a better format for zsh as for bash, set variable TIMEFMT to ~TIMEFMT=$'real\t%E\nuser\t%U\nsys\t%S'~
  + for longer commands, can either use braces (~time { ls; pwd; whoami; }~), or use a subshell (~time (ls; pwd)~)
- tree [path-to-folder-to-visualise] :: to visualise directories structures
  + Some options ::
    - -a :: include hidden files
    - -d :: list folders only
  + HOWEVER :: can do basic folders' list formatting with sed scripting, examples ::
    - with grep and sed :: ~ls -R | grep ":$" | sed -e '"'"'s/:$//'"'"' -e '"'"'s/[^-][^\/]*\//--/g'"'"' -e '"'"'s/^/   /'"'"' -e '"'"'s/-/|/'"'"~
    - with grep and perl :: ~ls -aR | grep ":$" | perl -pe 's/:$//;s/[^-][^\/]*\//    /g;s/^    (\S)/└── \1/;s/(^    |    (?= ))/│   /g;s/    (\S)/└── \1/'~
    
  

