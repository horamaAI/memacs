# -*- mode: org -*-
#+title: Linux (mainly, but not just) commands/tools that are fun and handy
#+SETUPFILE: ~/set-up-files/basic-setups.org

* shell and shell scripting
[2024-05-26 Sun 21:56]
** Notes
[2024-06-01 Sat 16:28]
- cheatsheet :: https://devhints.io/bash, examples:
  + shell arguments ::
    - $# :: number of arguments
    - $* :: all positional arguments (as a single word)
    - $@ :: all positional arguments (as separate strings)
    - $1 :: first argument
    - $_ :: last argument of the previous command
  + special variables ::
    - $? :: exit status of last task
    - $! :: PID of last background task
    - $$ :: PID of shell
    - $0 :: filename of the shell script, or name of current script
    - $_ :: last argument of the previous command
    - ${PIPESTATUS[n]} :: return value of piped commands (array)
** notorious folders in the filesystem [TODO]
[2024-07-28 Sun 17:09]
*** /proc
- ==/proc== is not a real filesystem directory, but a virtual file system (don’t actually exist on disk, created on the fly by the OS) created dynamically by Linux to provide access to certain types of hardware information and information about the running Linux system: kernel, running processes, devices, configuration parameters, etc. It is mapped to ==/proc== and mounted at boot time
  + its zero-length files are neither binary nor text, but can be examined and displayed. It's very handy to know how Linux commands work and do administrative tasks
  + most files are read-only, but a few writable ones (notably ==/proc/sys==) allow changes in the kernel parameters (of course caution is strongly advised :-P)
- ==/proc/<PID>== :: in ==/proc== are also found numbered directories depicting PIDS of running processes, and the files/links within them represent the commands in the PID context
  + each ==/proc/<PID>== contents have similar structures, and contain same file/links/directories such as (non-exhaustive):
    - cmdline :: contains the command that started the process
    - environ :: contains names and content of environment variables for the process
    - fd :: file descriptors
    - limits :: information about the limits of the process
    - mounts :: related information
    - cwd :: link to current working directory of the process
    - exe :: link to executable of the process
    - root :: link to work directory of the process
    - maps, statm, and mem :: deal with the memory in use by the process
    - stat and status :: provide infos about the status of the process (the latter is far clearer than the former)
- some notorious /proc folders/files/links :: (just 'cat' to show their contents)
  + /proc/self :: points to the currently scheduled pid (the currently running process on current logical CPU, points to the asking program's pid)
  + /proc/cmdline :: shows the parameters passed to the kernel at boot time, eg: ==BOOT_IMAGE=/boot/vmlinuz-6.1.0-23-amd64 root=UUID=d9cf6dd7-5ad1-44a9-a329-4fc1f8fa9aab ro quiet== ('root' gives the partition is the root of the filesystem)
  + /proc/cpuinfo :: cpu information
  + /proc/filesystems :: infos about filesystems supported by current kernel
  + /proc/meminfo :: stats on system's memory usage
  + /proc/version :: for kernel version and other kernel related infos
    - some commands such as ~uname -srv~ are also checking /proc/version to get their information
  + /proc/mounts :: shows all the mounts used by the machine (output similar to: ==/etc/mtab==)
  + /proc/partitions and /proc/swaps :: show all partitions and swap space
  + /proc/net :: rich of network and communication related files: network device (==/proc/net/dev==), several iptables (firewall) related files, net and socket statistics, wireless information, etc.
    - ~lsof -i TCP...~ for example is getting infos from
      ==/proc/net/tcp==, but also exists ==/proc/net/udp==, etc.
- some usecase examples ::
  + hunt zombie processes with 'status' file :: check whether status files contain "State: (Z) zombie": ~find /proc -name 'status' 2>/dev/null | xargs grep "zombie"~
  + check whether a given program is running :: exists some running PID with cmdline firefox: ~find /proc -name 'status' 2>/dev/null | xargs grep "firefox"~
    - ==ps== is using infos from /proc
** shell types and shell startup configs
*** notes
- [non-]interactive shells and [non-]login shells :: there are two kinds of shells:
  + [non-]interactive shells :: you type into them (shell scripts)
  + [non-]login shells :: the shell run them when you first login (subshells)
- shells execution order ::
  + all shells will first run ==env==,
  + then login shells will run ==login==,
  + then interactive shells will run ==interactive==,
  + once finished, login shells will run ==logout==
- managing multiple shells :: when using more than one shell, always remember to manage files with respect to their specific shell and specify things which any POSIX-compliant shell (like aliases and environment variables) can understand in a common startup file
  + dotfiles :: when organising things in dotfiles for example, a solution would be to organise dotfiles folders, one for each shell (==.bash/, .zsh/== and ==.sh/==), and one for shell-independent files (==.shell/==):
    #+begin_example
    .bash/
        env
        interactive
        login
        logout
    .sh/
        env
        interactive
        login
    .shell/
        env
        interactive
        login
        logout
    .zsh/
        env
        interactive
        login
        logout
    #+end_example
*** where to put configs
[2024-05-27 Mon 19:56]
- deciding when action needs to be run:
  + does the command set/modify environment variables ? :: then ==login==
  + is it an alias, or a terminal specific environment variable (eg: $GREP_COLOR) ? :: then ==interactive==
  + ==env== :: for useful functions and other tools, eg: ~umask~, or modifying $PATH, for example to filter out duplicate
*** implementing shell configs
[2024-05-27 Mon 22:28]
- notes ::
  + fortunately [[https://blog.flowblok.id.au/2013-02/shell-startup-scripts.html][flowblok]] did a wonderful job of following the flow of startup files and help in finding which level of the shell these config files have to be implemented:
    #+CAPTION: shell startup flow
    #+NAME: fig:shell-startup-actual
    #+ATTR_ORG: :width 500
    #+ATTR_HTML: :width 300px
    [[../images/shell-startup-actual.png]]
** shell scripting
[2024-06-23 Sun 02:07]
*** Notes
- utils ::
  + run a child process :: ==bash -c==, eg: ~bash -c 'echo "$me"';~ to print local variable $me in a child process (subshell)
- redirections ::
  + single redirection :: (==<==) to overwrite
  + double redirection :: (==<<==) to append
  + triple redirection :: (==<<<==) to redirect a string to STDIN of a command, similar to piping
  + redirection and file descriptors ::
    - command 2>&1 :: send error output to standard output
    - command > file 2>&1 :: send standard output to file (defines where stdout should go), and send stderr where stdout went ('file')
      + order counts, different than ~command 2>&1 > file~ which sends stderr to terminal, and stdout to file
      + command >> file 2>&1 :: same as ==command > file 2>&1==, but appends to file instead
    - command &> file :: send stdout and stderr to file
      + &>file is the same as >&file :: send stdout and stderr to file (&> is preferred)
      + semantically equivalent to ==>file 2>&1==
      + &>>file :: appends instead
    - command 2>&1 > file :: send stderr to stdout, and stdout to file
      + command 2>&1 >> file :: appends
    - misc ::
      + ~N>&-~ : close fd N
      + ~M>/dev/null~ : ignore whatever is redirected to fd N
        - ~>/dev/null~ is specific to stdout: ignore whatever is redirected to stdout
      + ~|&~ : abbreviation for ~2>&1 |~: redirect stderr to stdout, and pipe everything to command after pipe
      + ~&>/dev/null~ : abbreviation for ~>/dev/null 2>&1~ : ignore stdout, and redirect stderr to stdout: in other word ignore everything
    - examples ::
      + ~find /usr -name ls > myfile 2>&1~ : send both fd1 and fd2 to 'myfile'
      + ~find /usr -name cd >> myfile 2>&1~ : find in /usr files 'cd', and append both stderr and stdout to myfile
- process substitution :: ==>(command_list)== and ==<(command_list)== (and ~=(command_list)~ for zsh)
  + allow piping STDOUT of one or multiple commands into STDIN of some other command, even when piping with ==|== is not possible
  + process substitution uses ==/dev/fd/<n>== (for bash, or ==/proc/self/<n>== for zsh) files to send the results of the process(es) within parentheses to another process
  + ~=(command_list)~ :: (zsh only)
    - if used, then a temporary file is used instead of file in ~/dev/fd or a FIFO~
    - can be used in place of ~<(...)~ when the program needs to lseek in the output
    - useful as both ~/dev/fd~ and the named pipe implementation of ~<(...)~ have drawbacks:
      + ~/dev/fd~: some programmes may automatically close the file descriptor in question before examining the file on the command line, particularly if this is necessary for security reasons such as when the programme is running setuid
      + named pipe: if the programme does not actually open the file, the subshell attempting to read from or write to the pipe will (different OS may have different behaviour) block for ever and have to be killed explicitly
      + in both cases: the shell actually supplies the information using a pipe, so that programmes that expect to lseek (see man page lseek(2) for more details) on the file will not work
  + examples ::
    #+begin_example
    $ echo >(true)
    /proc/self/fd/18
    $ echo <(true)
    /proc/self/fd/14
    $ echo >(true) <(true)
    /proc/self/fd/18 /proc/self/fd/14
    $ wc <(cat /usr/share/dict/words)
     104334  104334  985084 /proc/self/fd/14
    $ cat /usr/share/dict/words | wc
     104334  104334  985084
    #+end_example
  + some usecase :: (main source: https://tldp.org/LDP/abs/html/process-sub.html)
    - compare outputs of two different commands, or outputs of different options to the same command ::
      + examples ::
        - compare command with different option :: ~$ comm <(ls -l) <(ls -al)~
        - compare content of directories :: ~diff <(ls $first_directory) <(ls $second_directory)~, ~$ comm <(sort file1) <(sort file2)~
    - generate an array variable of random numbers ::
      #+begin_example
      read -a list < <( od -Ad -w24 -t u2 /dev/urandom )
      # read random numbers from /dev/urandom, process with "od" and feed into stdin of "read" into variable "list"
      # -Ad: show offset (8 adjacent bits, distance between beginning of object and other element or point on the page): in decimal format (d);
      # -w24: limit bytes per output line to 24 bytes;
      # -t u2: dump output as unsigned decimal 2-byte units 0 (same as option: -d)
      #+end_example
    - monitor bitorrent port ::
      #+begin_example
      PORT=6881   # bittorrent
      
      # Scan the port to make sure nothing nefarious is going on.
      netcat -l $PORT | tee>(md5sum ->mydata-orig.md5) |
      gzip | tee>(md5sum - | sed 's/-$/mydata.lz2/'>mydata-gz.md5)>mydata.gz
      
      # Check the decompression:
      gzip -d<mydata.gz | md5sum -c mydata-orig.md5)
      # The MD5sum of the original checks stdin and detects compression issues.
      #+end_example
    - use buffer for compression/decompression ::
      #+begin_example
      # tar command format: tar -cf [target-archive-file] [files/dir to be archived] ; options: c to create compressed file, f to give it a name
      # bzip2 -c: block compress/decompress to stdout (will further sort compressed .tar)
      # complete command:
      # - compress data that is in $directory_name,
      # - redirect result to compressed file given by the result of >(), and then do to this result a block-sorted process with bzip2
      # - the result of bzip2 is redirected to stdout (-c), which is redirected to file file.tar.bz2
      # calls "tar cf /dev/fd/?? $directory_name", and "bzip2 -c > file.tar.bz2". // bzip2 -c: compress/decompress to standard output
      # because of the /dev/fd/<n> system feature, the pipe between both commands does not need to be named
      tar cf >(bzip2 -c > file.tar.bz2) $directory_name
      # same as:
      bzip2 -c < pipe > file.tar.bz2&
      tar cf pipe $directory_name
      rm pipe
      # or:
      # open fd3 and redirect it to stdout
      exec 3>&1
      # define fd4, redirect fd4 to stdout, redirect stdout to fd3, close fd3, pipe result to bzip2, which uses stdout data to create file.tar.bz2
      tar cf /dev/fd/4 $directory_name 4>&1 >&3 3>&- | bzip2 -c > file.tar.bz2 3>&-
      # close fd3
      exec 3>&-
      #+end_example
    - global variables with while-read and process substitution ::
      #+begin_example
      # 1: variable not available outside of subshell:
      echo "random input" | while read i
      do
        global=3D": variable $global not available outside the loop."
        # ... because it runs in a subshell.
      done
      
      # 2. share the variable with process substitution:
      echo "\$global (from outside the subprocess) = $global"
      # $global (from outside the subprocess) =
      
      echo "--"; echo
      
      while read i
      do
        echo $i
        global=3D": Available outside the loop."
        # ... because it does NOT run in a subshell.
      done < <( echo "random input" )
      #    ^ ^ : process substitution: piping instead of redirection to subshell
      
      # $global (using process substitution) = 3D: now available outside the loop
      echo "\$global (using process substitution) = $global"
      
      # 3. and likewise:
      declare -a inloop
      index=0
      cat $0 | while read line # try to read this bash script line by line
      do
        inloop[$index]="$line"
        ((index++))
        # It runs in a subshell, so ...
      done
      echo "OUTPUT = "
      echo ${inloop[*]}           # ... nothing echoes.
      
      echo; echo "--"; echo
      
      declare -a outloop
      index=0
      while read line
      do
        outloop[$index]="$line"
        ((index++))
        # It does NOT run in a subshell, so ...
      done < <( cat $0 ) # try process substitution this time
      echo "OUTPUT = "
      echo ${outloop[*]}          # ... the entire script echoes.
      
      exit $? # exit with status of last task
      #+end_example
    - global variables with while-read and process substitution :: (eg2: redirect process substitution to loop)
      #+begin_example
      #!/bin/bash
      # psub.bash
      declare -a array0
      while read
      do
        array0[${#array0[@]}]="$REPLY" # auto-increment since assigning by length of array0
        #+ $REPLY value is taken from the process substitution result redirected into the read loop
      done < <( sed -e 's/bash/CRASH-BANG!/' $0 | grep bin | awk '{print $1}' )
      # sets the default 'read' variable, $REPLY, by process substitution,
      #+ then copies it into an array
      #+ sed alone for example will replace literal sed command ~'s/bash/CRASH-BANG!/'~ with ~'s/CRASH-BANG!/CRASH-BANG!/'~, and awk will only keep the first column
      
      echo "${array0[@]}"
      
      exit $?
      
      # ====================================== #
      # because of the 'exit' above, following instructions won't be executed
      bash psub.bash
      # result of running this script should produce following result:
      #!/bin/CRASH-BANG! done #!/bin/CRASH-BANG!
      #+end_example
    - note ::
      + from "main source" above ::
        #+begin_example
        # more generally spoken
        (
        : | x=x
        # seems to start a subshell like
        : | ( x=x )
        # while
        x=x < <(:)
        # does not
        )
        # This is useful, when parsing csv and the like
        #+end_example
*** shell special variables
[2024-06-23 Sun 03:45]
**** file descriptors (fd)
[2024-07-29 Mon 22:29]
- 0: stdin
- 1: stdout
- 2: stderr
- 3..9: available additional fds that need to be opened first before they can be used, eg:
  #+begin_example
  exec 3<> /tmp/foo  #open fd 3 in read+write mode
  echo "test" >&3
  exec 3>&- #close fd 3.
  #+end_example
- 
**** environment variables with set vs export
[2024-07-04 Thu 22:41]
- set :: allows changes to environment variables and options on current shell session, and also show values of variables *and functions*
  + syntax :: ~set [options] [arguments]~
  + if no option or argument, then the set command prints all variables and functions in the current shell
  + set has many options that can be used to modify various aspects of the shell: error handling, debugging, expansion, job control, etc.
  + set as a positional parameters assigner ::
    #+begin_example
    #!/usr/bin/env bash
    set me=great foo bar;
    echo "$@";
    # returns:
    # me=great foo bar // $1 is "me=great" (literal, not variable "me"), $2 "foo", $3 "bar"
    #+end_example
  + set is a local variable assigner. The values assigned in current shell are not inherited by its child processes (or set positional parameters for that matter)
  + some options ::
    - -e :: exit right away if a command exits with a non-zero status (~set +e~ to turn off option after use)
    - -u :: treat unset variables as an error
    - -x :: print commands and their arguments as they are run, mostly useful for debugging and tracing
    - -o :: set or unset one of the shell options 
- export :: for creating global variables (environment variables in particular) that are accessible by all processes running in the current shell session and its child processes
  + can use export to configure system-wide settings by exporting environment variables in ==~/.bashrc== or ==/etc/profile== for example
- pitfalls of export and set ::
  + quoted variables ::
    - need for quoted variables :: prevent issues with spaces and special characters
    - eg:
      #+begin_example
      !/usr/bin/env bash
      
      # Initialize variable with special characters
      var="Hello * World!"
      
      # Print variable without quotes
      echo $var
      
      # Output: Hello (and the files in the current directory)
      # unexpected output since shell interprets asterisks as pathname expansion
      
      # Print variable with quotes
      echo "$var"
      
      # Output: Hello * World!
      # The output is expected because the quotes preserve the special characters
      #+end_example
  + overwriting built-in variables/functions ::
    - eg:
      #+begin_example
      !/usr/bin/env bash
      
      echo "Current user: $USER" # Outputs the current user (e.g. "john")
      
      # Overwrite the built-in variable USER with our own value
      USER="hacker"
      echo "Current user: $USER" # Outputs "hacker" instead of the actual user
      
      # Restore the original value of USER
      USER=$(/usr/bin/id -un)
      echo "Current user: $USER" # Outputs the actual user again
      #+end_example
  + overusing set options ::
    - risk :: unexpected behaviour, harder to read scripts
    - eg:
      + use set -x only for the parts that need debugging, and use explicit commands to output relevant information elsewhere:
        #+begin_example
        !/usr/bin/env bash
        echo "This is a simple script"
        x=10
        y=20
        set -x                 # Turn on tracing only for the calculation, and not before starting assignments
        z=$((x+y))
        set +x                 # Turn off tracing
        echo "The sum of x and y is $z"
        #+end_example
      + unclutter debugging outputs when using ~set -x~: use it only when necessary, prefer other debugging techniques such as logging or using a debugger
  + conventions ::
    - by convention, environment variables are named with uppercase letters, not local variables
  + always remember to unset used variables after use (with ~unset~)
**** separators with FS (Field Separator), IFS (Internal FS), OFS (Output FS), RS (record separator), ORS, NR, NF, FILENAME, FNR
***** IFS (Internal Field Separator) for bash separator in general
- IFS :: variable used by the shell to determine word boundaries while splitting a sequence of character strings
  + default values: at least three-characters string: space, tab, and newline
  + temporary IFS value ::
    - eg: iterate over content of $PATH:
      #+begin_example
      buff="$IFS"
      IFS=:
      (
          for ele in $PATH[@]; do
      	echo "content: $ele"
          done
          )
      IFS="$buff"
      #+end_example
  + IFS and ==read== command ::
    - eg: read from a csv file (comma separated): read the 2 last lines only (tail -n +2)
      #+begin_example
      #!/bin/bash
      while IFS="," read -r rec_column1 rec_column2 rec_column3
      do
       echo "Displaying Record-$rec_column1"
       echo "Quantity: $rec_column2"
       echo "Price: $rec_column3"
       echo ""
      done << (tail -n +2 input.csv)
      #+end_example
    - note :: (or rather a remainder)
      + 
***** awk separators with FS, OFS, RS, ORS, NR, NF, FILENAME, FNR
- ==FS== for awk internal field separators (example: ~awk -F':'~) and ==OFS== for output separator formatting
  + in the print/output command, the comma (",") concatenates two parameters with the value of awk OFS
  + eg: parse '/etc/passwd', and display columns 1, 3, and 6 (Name, UserID, HomeDirectory respectively), with " XXX " as field separator
    - inline: ~awk -F':' 'BEGIN{OFS=" XXX ";print "Name\tUserID\tHomeDirectory";} {print $1,$3,$6}END {print NR,"Records Processed";}' /etc/passwd | head -10~
    - or using a script in an awk file:
      + ~$cat etc_passwd.awk~ ::
        #+begin_example
        BEGIN{
          OFS=" XXX ";
          print "Name\tUserID\tHomeDirectory";
        }
        {
          print $1,$3,$6
        }
        END {
          print NR,"Records Processed";
        }
        #+end_example
      + command :: ~$awk -f etc_passwd.awk /etc/passwd | head -10~
      + result ::
        #+begin_example
        Name    UserID  HomeDirectory
        root XXX 0 XXX /root
        daemon XXX 1 XXX /usr/sbin
        bin XXX 2 XXX /bin
        sys XXX 3 XXX /dev
        sync XXX 4 XXX /bin
        games XXX 5 XXX /usr/games
        man XXX 6 XXX /var/cache/man
        lp XXX 7 XXX /var/spool/lpd
        mail XXX 8 XXX /var/mail
        #+end_example
- RS :: allows using separators for records ("sets" or "groups" of records)
  + ORS :: for format records output
  + example :: records of students
    - record ::
      #+begin_example
      $cat student.txt
      Jones
      2143
      78
      84
      77
       
      Gondrol
      2321
      56
      58
      45
       
      RinRao
      2122
      38
      37
      65
      #+end_example
    - awk script ::
      #+begin_example
      $cat student.awk
      BEGIN {
              RS="\n\n";
              FS="\n";
        
      }
      {
              print $1,$2;
      }
      #+end_example
    - result ::
      #+begin_example
      $ awk -f student.awk  student.txt
      Jones 2143
      Gondrol 2321
      RinRao 2122
      #+end_example
- ==FILENAME== for filename output :), example: ~$ awk '{print FILENAME}' student-marks~, where student-marks has 5 lines, will return 5 "student-marks" lines
- NR, NF, and FNR :: for number of records and fields
  + ==NR== for number of records variable, ==NF== for number of fields in a record
  + ==FNR==: when dealing with multiple files, it returns the number of records relative to the current input file
  + example: have 2 files with 5 lines each "student-marks" and "bookdetails". command: ~awk '{print FILENAME, FNR;}' student-marks bookdetails~ will return:
    #+begin_example
    student-marks 1
    student-marks 2
    student-marks 3
    student-marks 4
    student-marks 5
    bookdetails 1
    bookdetails 2
    bookdetails 3
    bookdetails 4
    bookdetails 5
    #+end_example
- example :: FS, OFS, RS, ORS, NF, NR
  + input ::
   #+begin_example
   $ cat <<$'EOF' | awk 'BEGIN {RS="\n\n"; FS="\n"; OFS=" -- "; ORS="\nXX\n";} {print "record#"NR":"$1,$2" has nfields:"NF;} END {print "#records: "NR;}'
   Jones
   2143
   78
   84
    
   Gondrol
   2321
   56
   58
   45
    
   RinRao
   2122
   38
   37
   65
    
   Edwin
   2537
   78
   67
   45
    
   Dayan
   2415
   30
   47
   20
   EOF
   #+end_example
  + result ::
    #+begin_example
    record#1:Jones -- 2143 has nfields:4
    XX
    record#2:Gondrol -- 2321 has nfields:5
    XX
    record#3:RinRao -- 2122 has nfields:5
    XX
    record#4:Edwin -- 2537 has nfields:5
    XX
    record#5:Dayan -- 2415 has nfields:6
    XX
    #records: 5
    XX
    #+end_example
  
**** FS/IFS/RS/etc. default separators (==\034==, ==\035==, etc.)
[2024-07-02 Tue 21:19]
- ==\034==, ==\035== are ASCII octal values for non printable characters usually used as special default characters to separate data into strings
  + find references here :: https://donsnotes.com/tech/charsets/ascii.html
- eg ::
  #+begin_example
  BEGIN { FS="\034"; RS="\035"; OFS="," }
  { gsub( /\n/, "" )
    $1=$1
    # If last field is empty, remove it.
    if ( ""==$NF )  NF--
    print
  }
  #+end_example
- why ? ::
  + usecase :: awk multidimensional arrays
    - awk treats multidimensional arrays as a one dimensional arrays where the index is a sequence of indexes separated by a special separator ==SUBSEP==
      + eg: awk stores ~grid[x,y]=value~ with SUBSEP ==@== as ~grid["x@y"]=value~
    - more details: https://www.gnu.org/software/gawk/manual/html_node/Multidimensional.html
  + however, some separators such as ==@== might be used somewhere in the data or the awk program and then lead to ambiguities
    - eg: ~foo["a@b", "c"]~ and ~foo["a", "b@c"]~ are stored as ~foo["a@b@c"]~,awk is not able to tell one from the other if SUBSEP is ==@==
  + thus the use of these non printable characters that are unlikely to appear in an awk program or in most input data
*** shell commands
[2024-06-23 Sun 19:39]
- ==local== ::
  + ~local [option] name[=value] ...~ :: create local variable ==NAME==, and with value ==VALUE==
  + ==...== means that previous argument can be repeated
  + eg: ~local cword words=()~ defines scalar variable ==cword== and empty array ==words==
- set vs export ::
- parameter expansion (TODO) ::
  + bash: https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html
  + zsh: https://zsh.sourceforge.io/Doc/Release/Expansion.html
*** file read/write
[2024-07-03 Wed 20:43]
- read file ::
  + eg ::
   #+begin_example
   #!/bin/bash
   INFILE=/root/names.txt
   
   # with while loop
   while read -r col1 col2
   do
     printf '%s - %s' "$col1" "$col2"
   done < "$INFILE"

   # with for loop
   IFS=$'\n' # set the Internal Field Separator to newline
   for LINE in $(cat "$INFILE")
   do
       echo "$LINE"
   done
   #+end_example
  + -r :: option prevents backslashes in the input from being interpreted as escape characters: "Hello\nWorld" is then read as is, not as two lines

*** Arrays
[2024-06-23 Sun 19:38]
- since all variables in general are declared implicitly in bash, to explicitly declare an array, use ==declare==: explicit declaration of a simple indexed array: ==declare -a==, declaration of an associative array: ==declare -A==
- indexed arrays ::
  + iterate/loop :: ~for index in ${!indexed_array[@]}; do echo "["$index"]:["${indexed_array[$index]}"]"; done~
  + ~${!indexed_array[@]}~ returns the indexes of the array
  + assign :: by index: ~indexed_array[2]=value~
  + append :: ~indexed_array+=([new] [more] [elements] [for] [the] [array])~
- associative arrays ::
  + iterate/loop :: ~for key in ${!associative_array[@]}; do echo "["$key"]:["${associative_array[$key]}"]"; done~
  + ~${!associative_array[@]}~ returns the keys of the array
- remove elements from array ::
  + by index/key :: ~unset indexed_array[1]~, ~unset associative_array[$"key"]~
  + whole array :: ~unset array~, or using ==@== and ==*==: ~unset array[@]~
- arrays and environment variables ::
  + array can not be an array of environment variables as environment variables may only be key-value string pairs
  + however, as done for $PATH, the trick is to turn array into a string delimited with a character not otherwise present in the values:
    #+begin_example
    $ arr=( aa bb cc "some string" )
    $ arr=$( printf '%s:' "${arr[@]}" )
    # ~printf '%s\n' "$arr"~ gives "aa:bb:cc:some string:"
    #+end_example
- offset and length traversal :: ==${array[@]:offset:length}==: ~array=("Baeldung" "is" "cool" "and" "better" "than" "before") && echo "${array[@]:1:3}"~: "is cool and"
  + if no length provided :: the result goes from offset to end: ~echo "${array[@]:1}"~: "is cool and better than before"
  + negative offset :: then relative to end of array: ~echo "${array[@]: -5:3}"~: "cool and better"
    - notice the whitespace before the negative symbol
- transforming arrays :: with parameter expansion" (some tricks other than just using loop)
  + replace elements of arrays ::
    - as for any bash variable, it's possible to format display of an array, eg:
      + suppose array "allo": ~alo=("one two three") && alo+=("twoone" "twoo" "three")~
      + replace first occurrence of an element in an item (==/==) :: since local option, it replaces first occurrence of 'e' by 'x' in each item of allo:
        - ~echo "${alo[*]/e/x}"~: ==onx two three twoonx twoo thrxe== (remaining 'e' stayed as were)
      + replace all occurrences with global option (==//==) :: ~echo "${alo[*]//e/x}"~: ==onx two thrxx twoonx twoo thrxx==
  + to uppercase/lowercase ::
    - lowercase: varies with the shell being used, eg with zsh: ~${array[@]:l}~, with bash: ==,==, or ==,,== , eg: ~${array[@],}"~
- notes ::
  + ==@== vs ==*== :: ==*== returns single element of merged items in the array separated by defined IFS, whereas ==@== returns separate elements (careful when using quotes)
* args and xargs
** Notes
- use :: passing arguments from standard input to other commands. Whereas some commands allow piping (grep for example), others don't (find, ls, or echo for example: can't ~seq 5 | echo~, need to pipe to xargs first: ~seq 5 | xargs echo~)
- piping to xargs with no following argument, defaults to echo
- can be faster as some commands, for example, piping to 'find' through xargs can be faster than using the -exec command of find: ~time (find -iname '*.org' | xargs grep -rail 'file:')~ < ~time (find -iname '*.org' -exec grep -rail 'file:' {} \;)~
- examples ::
  + ~cut -d: -f1 < /etc/passwd | sort | xargs~ :: print sorted first field of each line in file /etc/passwd

** Some options
- -t :: print complete command performed before printing the result
- -I [replace-str] :: replace occurrences of replace-str in the initial-arguments with names read from standard input, examples:
  + ~ls ~/Documents | xargs -I {} echo "mamamia/{}"~, where =={}== denote the arguments for xargs (the output of the first command): will print with replaced (added in this case) occurrences of input, eg: mamamia/[adocumentinfolderDocuments]
  + ~seq 1000 | xargs -I {} touch {}.txt~: create 1000 text files with name n.txt
  + ~ls | cut -d. -f1 | xargs -I {} mv {}.txt {}.text~: rename .txt extension files to .text, where the delimiter used is the dot, and considering that the files have only one dot in their names, and that it is used for the extension
- -n [max-args] :: use at most max-args arguments per command line: will run the following command by group of max-args arguments until all arguments are done, eg: ~ls | xargs -t -n 2~ prints the following command (xargs defaults: echo), and then its results (suppose folder contains 5 files: file1.txt...file5.txt):
  #+begin_example
  echo file1.txt file2.txt
  file1.txt file2.txt
  echo file3.txt file4.txt
  file3.txt file4.txt
  echo file5.txt
  file5.txt
  #+end_example
- -P :: set number of processes to run together, eg:
  + ~seq 10 | xargs -n 1 -P 3 bash -c 'echo $0; sleep 2'~: takes 1 arg at the time, executes 3 bash processes, and then sleeps for 2secs. The 10 numbers displayed may not be in order in the final display, but will be by groups of 3, eg: ==213-456-798-10== ('-' is the 2secs delay)
    - differs from ~seq 10 | xargs -n 3 -P 1 bash -c 'echo $0; sleep 2'~ that takes in 3 args, but only print the first element of the bunch, eg: ==1-4-7-10==
    - a last one for the road: ~seq 10 | xargs -n 3 -P 2 bash -c 'echo $0; sleep 2'~: ==14-710==
* text processing with scripting tools: sed, awk, perl, and python
** Notes
- fight! :: sed, awk, perl, python
  + sed :: to apply actions from a script to each line if input files
  + awk :: initially made for formatting reports, but now mainly a "program based on 'patterns matched' and 'actions taken when the pattern matches'"
  + perl :: initially written in part as an awk-killer and sed-killer. Provides access to almost all unix system calls and has great extensibility
  + python :: came after perl (age(sed) > awk > perl > python)
- when use one over another ? ([[https://stackoverflow.com/questions/366980/what-are-the-differences-between-perl-python-awk-and-sed][source]]) :: (given just as example, more than one reasons to use one over another)
  + sed :: when need to do simple text transforms on files
  + awk :: when need simple formatting, summarising, or transformation of data
  + perl :: for almost any task, but especially when task needs complex regular expressions
  + python :: for the same tasks that you could use Perl for
** sed
*** Notes
- sources :: https://www.gnu.org/software/sed/manual/sed.html
*** how sed works (in a nutshell) ::
+ sed maintains two data buffers: the active *pattern space*, and the auxiliary *hold space* (all initially empty)
+ sed operates by performing the following cycle on each line of input:
  1. reads one line from the input stream, removes any trailing newline, and places it in the pattern space
  2. execute the command
     - (commands can have an address associated, and the command is only executed if the condition is verified before the command is to be executed)
  3. when end of the script reached (unless -n option is used), print content of *pattern space* to the output stream
  4. start next cycle for the next input line
+ unless special commands (such as 'D') are used, the pattern space is deleted between two cycles.
  - only the hold space keeps its data between cycles
+ examples of sed commands ::
  - 'b [label]' :: branch unconditionally to [label]. Label may be omitted, in which case the next cycle is started
  - 'c [text]' :: replace (change) lines with [text]
  - 'd' :: delete pattern space
  - 'g' :: replace the contents of the pattern space with the contents of the hold space
    + attention !! :: do not confuse command 'g', with flag 'g'
  - 'i [text]' :: insert [text] before line
  - 's' :: ~s/regexp/replacement/[flags]~, match regular-expression against content of pattern space, if match, then replace
*** 's' command ::
+ some s command sed extensions :: (made of a backslash and one of the letters L, l, U, u, or E)
  - \U :: turn the s replacement to uppercase until a \L or \E is found
  - \u :: turn the next character to uppercase
  - \E :: stop case conversion started by \L or \U
+ 's' command's flags (some) ::
  - g :: apply replacement to all matches
  - e :: allows one to pipe input from a shell command into pattern space
  - i :: or I, matches in a case-insensitive manner
  - p :: if the substitution was made, then print the new pattern space
    + by default, sed prints everything, but with -n option, it suppresses default printing, and together with the p flag, only matching patters are printed
*** sed command options
[2024-06-30 Sun 01:58]
- ~e~ :: add script to be executed
- ~n~ :: suppress automatic printing of pattern space
*** examples ::
1. ~sed s/^\(.\{81\}\).*$/\1/~, to keep first 81 chars (80 + new line). '\1' is referring to first occurrence of pattern
2. ~echo 'a-b-' | sed 's/\(b\?\)-/x\u\1/g'~, outputs: 'axxB'
   - ~/x\u\1~ ::
     1. replace occurrences of 'b-', or just '-' by an x
     2. \u turns the next character to uppercase
     3. the next character being the first referenced set of characters matched (the referenced nth character within \( and its matching \) here being '\1')
        - the character may or may not exist in this case (due to '?': ~\(b\?\)~)
   - g flag is used, so case conversion (uppercase '\u') does not propagate from one occurrence of the regular expression to another, eg: first pattern matched '-' only affects empty replacement of '\1'
   - thus ::
     + first chain matched: '-' => replaced with just x, since \1 is empty
     + second chain matched: 'b-' => replaced by 'xB', since first char is b
3. pretty print folders architecture with sed :: ~ls -R | grep ":$" | sed -e 's/:$//' -e 's/[^-][^\/]*\//──/g' -e 's/─/├/' -e '$s/├/└/'~
   - assume in folder containing sub directories and one file: ~a/b/c/d.txt~
   - ~ls -R~ :: prints folders recursively content until there is no folder to visit:
     #+begin_example
     .:
     b
    
     ./b:
     c
    
     ./b/c:
     c.txt
     #+end_example
   - grep :: will select only folders (ending with ~:$~)
   - sed scripts (-e) :: in input lines returned by grep (only folders, files not included)
     + 's/:$//' :: remove colons
     + 's/[^-][^\/]*\//──/g' :: remove all occurrences of sub strings ending with '/' (and not starting with a '-'), and replace them with '--' (each successions of matches will be replaced by successions of '--', adding more depth to sub folders)
       - [^-][^\/]*\/ :: the inner (~[^\/]*~) to restrict the expressions to omit inner folders (in 'a/b/c/', select a/, b/, and c/, instead of matching the whole string 'a/b/c/' when the inner ~[^\/]*~ is not used) 
       - g :: to apply command on all matches
     + 's/─/├/' :: replace the first '-' with '├', for pretty printing
     + '$s/├/└/' :: further pretty print (notice the '$')
* cat, printf
[2024-06-23 Sun 18:51]
- examples ::
  + print IFS characters ::
    - ~echo "$IFS" | cat -et~: add ==$== at each en of line (-e), and print tabulation as ==^I== (-t)
    - ~printf %q "$IFS"~: ==%q== to escape non-printable characters with POSIX ==$''==
* cron (crontab, cronjob, etc.)
** Notes
+ cron tool format explainer :: https://crontab.cronhub.io/
+ Simply put, cron is a basic utility available on Unix-based systems. It enables users to schedule tasks to run periodically at a specified date/time, without requiring human intervention.
+ Cron runs as a daemon process (needs to be started once and it keeps running in the background). The process makes use of crontab to read the entries of the schedules and kicks off the tasks.
** Working with crontab
+ A cron schedule is a simple text file located under ==/var/spool/cron/crontabs== on Linux systems.
+ crontab files cannot be edited directly, so they need to be accessed using the crontab command.
+ commands ::
  - to open crontab file :: ==crontab -e==
  - a crontab line is an entry with an expression and a command to run ::
    + eg :: the entry ==* * * * * /usr/local/ispconfig/server/server.sh== runs the mentioned script (server.sh) every single minute.
*** Cron Expression
+ the cron expression consists of 5 fields :: ==<minute> <hour> <day-of-month> <month> <day-of-week> <command>== 
**** Special Characters in Cron Expression
+ ==*== (all) specifies that event should happen for every time unit
+ ==?== (any) is used in the <day-of-month> and <day-of-week> fields to denote the arbitrary value and thus neglect the field value. For example, if we want to fire a script at the 5th of every month, irrespective of what day of the week falls on that date, we specify a ==?== in the <day-of-week> field
+ ==–== (range) determines the value range. For example, "10-11" in the <hour> field means "10th and 11th hours"
+ ==,== (values) specifies multiple values. For example, "MON, WED, FRI" in <day-of-week> field means on the days "Monday, Wednesday and Friday"
+ ==/== (increments) specifies the incremental values. For example, a "5/15" in the <minute> field means at "5, 20, 35 and 50 minutes of an hour"
+ ==L== (last) has different meanings when used in various fields. For example, if it’s applied in the <day-of-month> field, it means last day of the month, i.e. 31st of January and so on as per the calendar month.
  - it can be used with an offset value, like =L-3=, which denotes the "third to last day of the calendar month"
  - in <day-of-week>, it specifies the "last day of a week"
  - it can also be used with another value in <day-of-week>, like ==6L==, which denotes the "last Saturday"
+ ==W== (weekday) determines the weekday (Monday to Friday) nearest to a given day of the month. For example, if we specify "10W" in the <day-of-month> field, it means the "weekday near to 10th of that month". So if "10th" is a Saturday, the job will be triggered on "9th" and if "10th" is a Sunday, it will trigger on "11th". If we specify "1W" in <day-of-month> and if "1st" is Saturday, the job will be triggered on "3rd" which is Monday, it will not jump back to the previous month
+ ==#== specifies the "N-th" occurrence of a weekday of the month (<day-of-week>), for example, "third Friday of the month" can be indicated as "5#3"
**** Examples
+ at 12:00 p.m. (noon) every day :: ==0 12 * * ?==
+ every 15 minutes every day :: ==0/15 0 * * ?==
+ using increments to run the job every odd minute :: ==1/2 0 * * ?==
+ every five minutes starting at 1 p.m. and ending at 1:55 p.m. (cron job reference by default is per hour) and then starting at 6 p.m. and ending at 6:55 p.m., every day :: ==0/5 13,18 * * ?==
+ every minute starting at 1 p.m. and ending at 1:05 p.m. (not default here, but rather range), every day :: ==0-5 13 * * ?==
+ at 1:15 p.m. and 1:45 p.m. every Tuesday in the month of June :: ==15,45 13 ? 6 Tue==
+ at 9:30 a.m. every Monday, Tuesday, Wednesday, Thursday and Friday :: ==30 9 ? * MON-FRI==
+ at 6 p.m. on the third to last day of every month :: ==0 18 L-3 * ?==
+ at 10:30 a.m. on the last Thursday of every month :: ==30 10 ? * 4L==
+ at 10 a.m. on the third Monday of every month :: ==0 10 ? * 1#3==
+ at 12 midnight on every 5th day, starting from the 10th until the end of the month :: ==0 0 10/5 * ?==
**** Cron Special Strings
In addition to the fields specified in the cron expression, there’s also support for some special, predefined values that we can use instead of the fields :
+ ==@reboot== :: run once at the startup
+ ==@yearly== or ==@annualy== :: run once a year
+ ==@monthly== :: run once a month
+ ==@weekly== :: run once a week
+ ==@daily== or ==@midnight== :: run once a day
+ ==@hourly== :: run hourly
* cut
* git
[2024-05-31 Fri 22:58]
** git worktrees (work with different "branches" at the time)
- allows switching between different works in progress (WIP) and branches/commits/etc.
  + instead of usual check out, for which can't have different branches at once, worktree allows checking out on different branches at once. Each working tree is checked out in different folders
  + worktree is somehow similar to cloning the same repository in different folders, but with worktree all work-trees are still linked to the same clone
*** git worktree basic commands (some)
[2024-05-31 Fri 23:34]
- notes ::
  + as always with git, there are plenty of ways of doing the same thing
- create worktree from current branch :: and have two different branches of the same clone at once
  + ~git worktree add [path-to-new-worktree] [target-branch-to-check-out-to]~
    - eg: ~git worktree add ../new-worktree current-or-other-branch-or-commit~
    - ~new-worktree~ will be a new folder with current WIP snapshot, and parallel to the current repo (~../~)
    - ~current-or-other-branch-or-commit~ will be the target commit/branch after creating the new worktree
    - ~new-worktree~ will not have any ~.git/~ within, but instead a ~.git~ file
      + ~.git~ points to the original ~.git/~ directory in its parent clone => all git commands will work with it, as well as its parent
  + as for git switch, can even use remote branch to switch to after adding worktree: eg:  ~git worktree add ../new-worktree a-remote/a-remote-branch -b name-of-local-from-remote-branch~
    - means 
- remove worktree ::
  + ~git worktree remove [path-to-worktree-from-within-parent]~
    - if still has change, unless use ==--force==, git will block the worktree deletion
- main con of worktree :: can't check out on the same branch for more than one worktree at the time, get an error
** git subtrees and git submodules
[2024-06-04 Tue 21:58]
*** git submodules vs git subtrees
[2024-06-04 Tue 21:59]
- subtrees and submodules allow nesting subprojects within projects, useful for example when subprojects are dependencies of "super" projects
- submodules are pointers to commits, usually in another repositories
- subtrees are directories in the "main" repositories
- when use one over another ? ::
  + project has dependency updated in another repo? :: => submodules
  + project has dependency maintained in same repo? :: => subtrees
  + want to push a subdirectory, not the whole directory, to a branch on the same repo? :: => subtrees, but can work with worktrees ([[https://nicolevanderhoeven.com/blog/20210302-presentation-slides-as-code/][checkout this]])
*** git submodules
[2024-06-04 Tue 21:36]
- allows tracking version history of external submodules into parent repo
- with submodules it's possible to lock the version in its parent (no git submodule update)
- the same way a git repo has a .git folder, when it has submodules, there is a .gitmodules file added to it
- link submodule to current repo (of course current has to be a valid repo) : ~git submodule add [link-to-existing-submodule-repo]~
- it's possible to have nested submodules, and in that case to update from the "super" parent, use: ~git submodule update –init –recursive~
- clone repo with submodules, eg:
  #+begin_example
  git clone /url/to/repo/with/submodules
  # git submodule init: copies mapping from .gitmodules file into the local ./.git/config file
  git submodule init
  git submodule update
  #+end_example
  
*** git subtrees
[2024-06-04 Tue 21:56]
* screen (terminal multiplexer)
:PROPERTIES:
:CUSTOM_ID: screen_configs
:END:
[2024-06-22 Sat 19:28]
- Notes ::
  + terminal multiplexer :: tool used to multiplex several pseudoterminal-based login sessions inside a single terminal display/terminal emulator window/PC/workstation system console/remote login session, or to detach and reattach sessions from a terminal
    - some examples: tmux, GNU screen, terminator
  + some main features ::
    - open multiple windows within the screen session
    - disconnect and reconnect Screen without losing the current session
- screen control sequence prefix :: default is: ~C-a~ as in ~C-a [command]~
  + sometimes default control sequence ~C-a~ is used somewhere else (eg: emacs), so it can be changed by setting the command into ==~/.screenrc==
    - eg: to change the default screen prefix to ~C-x~: ~$echo "escape ^Xx" >> ~/.screenrc~
  + since prefix can be changed, from now forward, it will be referred to as ~[screen-prefix]~
  + ~M-1~ as prefix ::
    + source :: https://stackoverflow.com/questions/1543427/gnu-screen-changing-the-default-escape-command-key-to-alt-x
    + ~/.screenrc ::
      #+begin_example
      # trick to use M-1 as the gnu screen prefix (source: https://stackoverflow.com/questions/1543427/gnu-screen-changing-the-default-escape-command-key-to-alt-x)
      # set prefix to "C-@" (or ~C-2~), and also set its "alias": ~M-1~
      escape ^@a
        
      # use screen auxiliary register to save C-a into register S
      register S ^@
        
      # M-1 produces ^A and acts as an escape key
      bindkey "^[1" process S
      #+end_example
- help :: ~[screen-prefix] ?~
- sessions ::
  + launch a session :: ~$screen~
    - start named session :: ~screen -S nameofsession~
  + (de)attach screen session ::
    - disconnect from screen session and keep it running on the host (detach: d) :: ~[screen-prefix] d~
    - resume (-r) connection to session :: ~$screen -r~
    - resume connection to a session that was not disconnected properly due to unstable network for example :: ~$screen -r -d~ (disconnect first from host: -d, and then resume to reconnect)
    - resume session, or create new one if none pending :: ~screen -d -RR~
    - start detach session and run command on it (option -m) :: ~screen -d -m ping 10.10.10.10~
  + kill session ::
    - ~screen -X -S nameofsession quit~ :: -X: send command to alive session, -S: define session name
    - ~screen -X 269 quit~ :: auto-completes if unique
  + rename session ::
    - ~screen -S OLDSESSIONNAME -X sessionname NEWSESSIONNAME~
    - ~[screen-prefix] :sessionname NEW-NAME~ :: in-screen session
- screen windows ::
  + start new window (independent window with its own shell, in same region/) :: ~[screen-prefix] c~
  + move to next/previous window :: ~[screen-prefix] n~ or ~[screen-prefix] <space>~ / ~[screen-prefix] p~
  + move to windows/screen by number :: ~[screen-prefix] [n]~
  + rename current windows :: ~[screen-prefix] A~
  + list all windows and select with cursor :: ~[screen-prefix] "~
  + list all running sessions (screen offline mode) :: ~screen -ls~
  + kill current window :: ~[screen-prefix] K~, or ~exit~, or ~C-d~
- split screens ::
  + horizontal split :: ~[screen-prefix] S~
  + vertical split :: ~[screen-prefix] |~
  + switch between screens :: ~[screen-prefix] <Tab>~
  + close current split/region :: ~[screen-prefix] x~
  + remove all screen region but the current one (doesn't kill windows) :: ~[screen-prefix] Q~
- lookup if in screen session ::
  + ~echo $STY~ :: custom screen environment variable
  + ~echo $TERM~ :: linux terminal environment variable
  + ~[screen-prefix] x~ :: screen will print time, if does then current is a screen session
- copy paste mode ::
  1. enter copy mode :: ~[screen-prefix] [~, or ~[screen-prefix] <Esc>~
  2. select region :: ~<Space>~ + move to end of region to copy + ~<Space>~
  3. paste copy buffer to cursor :: ~[screen-prefix] ]~
  4. leave copy mode :: ~q~
- advanced ::
  + logging ::
    - start logging current session :: ~[screen-prefix] H~
      + generates a file named ~screenlog.n~, where n is the screen session number
      + can also enable logging in detached mode with -L: ~screen -L~
        - customise target file ~screen -L -Logfile /path/to/logfile.txt~
    - screenshot ::
      + make screenshot of what's currently on the screen :: ~[screen-prefix] h~
        - generates a file named ~hardcopy.n~, n being the current window number
          + by default, each time a screenshot is taken it overwrites the previous content of ~hardcopy.n~
            - to append instead of the overwrite, add to configuration file (==screenrc==): ~hardcopy_append on~
            - and to customise target directory for screenshots: ~hardcopydir /your/dir/~
  + lock screen session ::
    - lock current screen session :: ~[screen-prefix] x~ (locks screen with user's password)
- examples ::
  + monitor the outputs of a detached session :: 
    - ~screen -dmS test -L~ :: start screen session named 'test' (-S defines session name), but start it in detached mode (-dm), and enable logging (-L)
      + can then attach to session with ~screen -r test~, but here want to do it from another session
    - ~screen -S test -X colon "logfile flush 0^M"~ :: enable real-time logging:
      + send command to an alive session (-X)
      + signal the session that a command is being sent (~colon~)
      + set the screen session to fix the log interval time in seconds (~logfile flush~), and value 0 for real-time logging (default is 10secs)
      + execute (enter) sent command with ~C-M~ (~^M~)
    - ~tail -Fn 0 screenlog.0~ :: monitor outputs from detached screen
      + ~-F~ ::  keep watching screen log file for changes
      + ~-n 0~ :: only watch, don’t output older lines
      + ~screenlog.0~ :: .0 since the detached screen is supposed to be the first
    - ~tail -Fn 0 screenlog.0 | ccze | grep -i error~ :: watch for errors using ccze (logs colorizer)
* other nice utilities
- stow ::
  + usecase :: setting dotfiles located in folder ==~/mydotfiles/==
    - without stow: one would need to ==cp ~/mydotfiles/.zshrc ~/.zshrc==, or set-up manually an "automatic" symlinks generation (with bash scripts for example)
    - with stow: every file/folder in the stowed folder has a symlink (not explicitly set) created for an identical file/folder in ==home==
    - careful :: path is preserved: ==~/mydotfiles/randomfolder/.zshrc== will stow ==~/randomfolder/.zshrc==, and not ==~/.zshrc==
      + => organise dotfiles in the exact same way you would organise them in the home folder
  + use ::
    - basic use: stow [target_folder_path]/ :: ~cd target_folder_path && stow .~
    - notes ::
      + stow default ignore list :: .git, .gitignore, README.*, etc., or will use content of ==.stow-global-ignore== (see [[https://www.gnu.org/software/stow/manual/html_node/Types-And-Syntax-Of-Ignore-Lists.html][more details]])
    - clean-up all generated symlinks :: ~cd target_folder_path && stow -D .~
- showkey ::
  + useful for keybindings when whants to get text version of keys from keyboard
  + find mappings :: ~showkey -a~, and then type the key sequence to know its value
- lsof ::
  + lsof :: "List Open Files"
    - does what its name, but its potential isn't limited to just that, since in Linux everything is a file (kind of),
    - lsof utility is a robust interface for information inside the ==/proc== virtual filesystem for example
    - examples ::
      + what process are using a specific directory :: (here ==/run==)
        #+begin_example
        lsof /run
        COMMAND    PID            USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
        mate-scre 1640 myfirstdebianpc   15w  FIFO   0,22      0t0 1596 /run/systemd/inhibit/5.ref
        mate-powe 1654 myfirstdebianpc   13w  FIFO   0,22      0t0 1581 /run/systemd/inhibit/3.ref
        #+end_example
      + what files a user is using :: ~lsof -u <user-name>~
      + what files a particular process has opened :: ~lsof -p <pid-number>~
      + what process is using a specific TCP port :: ~lsof -i TCP:<port-number>~ (22 for example)
      + generate report on all network processes associated with a particular interface :: ~lsof -i TCP@<IP-address>~ (eg: ~lsof -i TCP@127.0.0.1~)
    - columns output of lsof :: (some)
      + FD :: File descriptor definition
        - when a file is opened, the OS creates an entry to represent that file and store the information about it. So if there are 100 files opened then there will be 100 entries somewhere in the kernel. These entries are represented by integers (...100, 101, 102....) known as "file descriptor". FD is an integer that uniquely represents an opened file for the process
        - similarly, when one open's a network socket, it is also represented by an integer and is called Socket Descriptor
      + DEVICE :: device number or, in the case of a block device, character or other
      + SIZE/OFF :: dimension of the file or offset (the suffix 0t is the offset)
      + NODE :: node description of the local file; could be the number of the local file, TCP, UDP, or STR (stream)
      + NAME :: the name of the mount point where the file resides
  + format :: ~lsof -p <process id>~
- read ::
  + transform string to array of strings (split) :: ~echo "one two three" | read -ra array~, or ~read -ra array <<< "one two three"~ => alo: ["one" "two" "three"]
    - option -a to specify the array to redirect the split into (may vary with the shell, eg: -A with zsh)
- time [command] :: times how long a command takes, eg: ~time (find -iname '*.org' | xargs -I {} basename {})~
  + has even formatting options, eg : ~time -f '%E %W %r' [command]~
    - to get a better format for zsh as for bash, set variable TIMEFMT to ~TIMEFMT=$'real\t%E\nuser\t%U\nsys\t%S'~
  + for longer commands, can either use braces (~time { ls; pwd; whoami; }~), or use a subshell (~time (ls; pwd)~)
- top ::
  + 
- tree [path-to-folder-to-visualise] :: to visualise directories structures
  + Some options ::
    - -a :: include hidden files
    - -d :: list folders only
  + HOWEVER :: can do basic folders' list formatting with sed scripting, examples ::
    - with grep and sed :: ~ls -R | grep ":$" | sed -e '"'"'s/:$//'"'"' -e '"'"'s/[^-][^\/]*\//--/g'"'"' -e '"'"'s/^/   /'"'"' -e '"'"'s/-/|/'"'"~
    - with grep and perl :: ~ls -aR | grep ":$" | perl -pe 's/:$//;s/[^-][^\/]*\//    /g;s/^    (\S)/└── \1/;s/(^    |    (?= ))/│   /g;s/    (\S)/└── \1/'~
    
